# Uderia Platform - Tutorial Master Plan
## Comprehensive Tutorial Structure & Narration Guide

**Platform:** Uderia (www.uderia.com)  
**Tutorial Platform:** Tight.studio  
**Target Duration per Section:** 2-5 minutes  
**Last Updated:** December 7, 2025

---

## üìë Table of Contents

### Quick Navigation
- [Tutorial Philosophy](#-tutorial-philosophy)
- [Profile Personas Overview](#-profile-personas-used-throughout-tutorials)
- [Module 1: Introduction & Quick Start](#module-1-introduction--quick-start)
  - [1.1 Welcome to Uderia - The Platform Promise](#11-welcome-to-uderia---the-platform-promise)
  - [1.2 UI Orientation & Core Components](#12-ui-orientation--core-components)
  - [1.3 First Conversation - Revenue Analysis](#13-first-conversation---revenue-analysis)
- [Module 2: The Two-in-One Paradigm](#module-2-the-two-in-one-paradigm)
  - [2.1 Conversational Interface ‚Üí REST API Transformation](#21-conversational-interface--rest-api-transformation)
  - [2.2 Profile System - "God Mode" Orchestration](#22-profile-system---god-mode-orchestration)
  - [2.3 REST API Deep Dive](#23-rest-api-deep-dive)
  - [2.4 Airflow Integration for Batch Processing](#24-airflow-integration-for-batch-processing)
- [Module 3: Transparent Execution](#module-3-transparent-execution)
  - [3.1 Live Status Updates & Execution Transparency](#31-live-status-updates--execution-transparency)
  - [3.2 MCP Capabilities Discovery](#32-mcp-capabilities-discovery)
  - [3.3 Context Window Management](#33-context-window-management)
  - [3.4 Tool Customization & Preferences](#34-tool-customization--preferences)
  - [3.5 Execution Monitoring & Debugging](#35-execution-monitoring--debugging)
- [Module 4: Strategic Intelligence](#module-4-strategic-intelligence)
  - [4.1 Strategic Planning - The Hidden Orchestration Layer](#41-strategic-planning---the-hidden-orchestration-layer)
  - [4.2 RAG Self-Learning from Champion Cases](#42-rag-self-learning-from-champion-cases)
  - [4.3 Optimization & Learning Loops](#43-optimization--learning-loops)
  - [4.4 Cost Tracking & Token Analytics](#44-cost-tracking--token-analytics)
- [Module 5: Sovereignty](#module-5-sovereignty)
  - [5.1 Multi-Provider LLM Architecture](#51-multi-provider-llm-architecture)
  - [5.2 Hybrid Intelligence - Cloud + Local Models](#52-hybrid-intelligence---cloud--local-models)
  - [5.3 Security & Multi-User Management](#53-security--multi-user-management)
- [Module 6: Collaborative](#module-6-collaborative)
  - [6.1 Marketplace Overview - The Network Effect](#61-marketplace-overview---the-network-effect)
  - [6.2 Planner Repositories - Reusable Execution Patterns](#62-planner-repositories---reusable-execution-patterns)
  - [6.3 Knowledge Repositories - Shared Expertise](#63-knowledge-repositories---shared-expertise)
  - [6.4 Community Ecosystem & Ratings](#64-community-ecosystem--ratings)
- [Module 7: Financial Governance](#module-7-financial-governance)
  - [7.1 Consumption Profiles & Rate Limiting](#71-consumption-profiles--rate-limiting)
  - [7.2 Cost Analytics & Optimization Dashboard](#72-cost-analytics--optimization-dashboard)
  - [7.3 Budget Alerts & Forecasting](#73-budget-alerts--forecasting)
- [Module 8: Administration](#module-8-administration)
  - [8.1 User & Access Management](#81-user--access-management)
  - [8.2 Long-Lived Access Tokens](#82-long-lived-access-tokens)
  - [8.3 System Configuration & Monitoring](#83-system-configuration--monitoring)
- [Module 9: Advanced Topics](#module-9-advanced-topics)
  - [9.1 Flowise Integration - Low-Code Workflows](#91-flowise-integration-low-code-workflows)
  - [9.2 Docker Deployment - Production Setup](#92-docker-deployment-production-setup)
  - [9.3 Developer Mode & Custom Integrations](#93-developer-mode--custom-integrations)
  - [9.4 Installing Your Own Uderia Platform](#94-installing-your-own-uderia-platform)
- [Module 10: User Journey Scenarios](#module-10-user-journey-scenarios)
  - [10.1 Business Analyst Journey - @ANALYST Profile](#101-business-analyst-journey---analyst-profile)
  - [10.2 Database Administrator Journey - @DBA Profile](#102-database-administrator-journey---dba-profile)
  - [10.3 Data Quality Analyst Journey - @QUALITY Profile](#103-data-quality-analyst-journey---quality-profile)
  - [10.4 Compliance Officer Journey - @LOCAL Profile](#104-compliance-officer-journey---local-profile)
  - [10.5 "God Mode" Power User Journey - Multi-Profile Orchestration](#105-god-mode-power-user-journey---multi-profile-orchestration)
- [Profile Ecosystem Reference](#-profile-ecosystem-reference)
- [Demo Environment Reference](#-demo-environment-reference)

---

## üìã Tutorial Philosophy

This tutorial series showcases Uderia's unique value proposition: **Cloud-Level Reasoning with Zero-Trust Privacy**. Each module aligns with one of Uderia's six core principles, ensuring comprehensive coverage of features, benefits, and differentiation.

**Key Messaging Themes:**
- From Days to Seconds (Actionable)
- From Guesswork to Clarity (Transparent)
- From $$$ to ¬¢¬¢¬¢ (Efficient)
- From Data Exposure to Data Sovereignty (Sovereignty)
- From Hidden Costs to Total Visibility (Financial Governance)
- From Isolated Expertise to Collective Intelligence (Collaborative)

---

## üé≠ Profile Personas Used Throughout Tutorials

A core differentiator of Uderia is the ability to **orchestrate a team of specialized AI personas** through profile switching. Throughout these tutorials, we demonstrate six key profiles that represent different user roles and use cases. Think of profiles not as configuration files, but as complete expert personas‚Äîeach with their own LLM, tools, knowledge, and learned patterns.

**The Profile Lineup:**

1. **@ANALYST** - Business Intelligence Persona
   - **Role:** Business Analyst, Product Manager
   - **LLM:** Gemini 1.5 Flash (fast, cost-effective)
   - **Specialty:** Business insights, visualizations, executive reporting
   - **Knowledge:** Business metrics, KPIs, fitness_db business context
   - **Appears in:** Modules 1.3, 2.2, 4.1, 6.2, 10.1

2. **@DBA** - Database Administrator Persona
   - **Role:** Database Admin, Data Engineer
   - **LLM:** Claude Sonnet 3.5 (reliable SQL optimization)
   - **Specialty:** Query optimization, schema analysis, performance tuning
   - **Knowledge:** SQL best practices, Teradata optimization, schema docs
   - **Appears in:** Modules 2.2, 3.2, 4.1, 8.2, 10.2

3. **@QUALITY** - Data Quality Engineer Persona
   - **Role:** Data Steward, Quality Analyst
   - **LLM:** Claude Sonnet 3.5 (thorough validation)
   - **Specialty:** Data validation, anomaly detection, compliance audits
   - **Knowledge:** Quality standards, constraints, validation rules
   - **Appears in:** Modules 2.2, 3.1, 10.3

4. **@LOCAL** - Privacy-First Persona
   - **Role:** Compliance Officer, Privacy-Conscious User
   - **LLM:** Ollama Llama 3.1 70B (on-premises, zero cloud)
   - **Specialty:** Sovereign data analysis with complete privacy
   - **Knowledge:** Same as @ANALYST but locally stored/executed
   - **Appears in:** Modules 2.2, 5.2, 5.3, 10.4

5. **@PROD** - Production Workhorse Persona
   - **Role:** Default automation profile for APIs and scheduled jobs
   - **LLM:** Claude Sonnet 3.5 (balanced reliability)
   - **Specialty:** REST API calls, Airflow DAGs, production automation
   - **Knowledge:** Cross-domain general purpose
   - **Appears in:** Modules 2.1, 2.3, 2.4, 9.1, 10.5

6. **@COST** - Budget-Conscious Persona
   - **Role:** Cost optimization, high-volume queries
   - **LLM:** Gemini 1.5 Flash (cheapest option)
   - **Specialty:** Simple lookups, batch jobs, development/testing
   - **Knowledge:** Minimal (reduces overhead)
   - **Appears in:** Modules 4.4, 7.2, 10.5 (optional)

**The "God Mode" Concept:**

Rather than being limited to one agent's perspective, Uderia users can **orchestrate an entire team of expert personas**, switching between them as naturally as consulting different colleagues. This allows one person to perform work that traditionally requires multiple specialists:

- Need SQL optimization? ‚Üí Switch to **@DBA**
- Need business insights? ‚Üí Switch to **@ANALYST**
- Need data validation? ‚Üí Switch to **@QUALITY**
- Need privacy compliance? ‚Üí Switch to **@LOCAL**
- Need to operationalize? ‚Üí Switch to **@PROD**

Each profile has its own knowledge collections and learned execution patterns (planner repositories), making profile switching equivalent to accessing specialized domain expertise instantly.

**Note:** Detailed specifications for each profile are provided in the "Profile Ecosystem Reference" section before Module 10. They are introduced narratively in Module 2.2 and demonstrated throughout subsequent modules.

---

## Module 1: Introduction & Quick Start
**Goal:** Hook viewers with immediate value demonstration and quick wins

**Demo Environment Setup:**
- **Database:** fitness_db (Teradata)
- **Schema:** 5 tables (Customers, Sales, SaleDetails, Products, ServiceTickets)
- **Business Context:** Fitness equipment retail and service tracking
- **MCP Tools:** base_tools, quality_tools for business user scenarios
- **Profiles:** Standard profile with Teradata MCP server configured

### 1.1 Welcome to Uderia - The Platform Promise
**Duration:** 2-3 minutes  
**Learning Objectives:**
- Understand Uderia's unique positioning in the AI landscape
- Recognize the six core principles
- See the fundamental "before vs after" transformation

**Key Topics to Cover:**
- [ ] The problem: Traditional AI tools force trade-offs (cloud intelligence vs data privacy)
- [ ] The Uderia solution: Best of both worlds
- [ ] Six core principles overview (visual diagram)
- [ ] Platform overview screenshot tour

**Narration Script:**
```
Welcome to Uderia‚Äîthe platform that finally ends the impossible choice between 
cloud intelligence and data privacy.

For too long, organizations have been forced into a corner: either send your 
sensitive data to powerful cloud models and sacrifice sovereignty, or run local 
models and compromise on capability. It's been one or the other‚Äînever both.

Uderia shatters this false dichotomy.

Imagine having the reasoning power of Claude, GPT-4, or Gemini‚Äîwhile keeping your 
most sensitive data completely private on your own infrastructure. That's Uderia's 
hybrid intelligence architecture: cloud-level planning meets local execution.

But we go far beyond just hybrid deployment. Uderia is built on six revolutionary 
principles that transform how you work with enterprise data:

First‚ÄîACTIONABLE: Your conversational discovery IS your production-ready API. No 
handoffs. No rebuilding. From insight to automation in seconds, not weeks.

Second‚ÄîTRANSPARENT: No more AI black boxes. Watch the agent plan, execute, and 
self-correct in real-time. Every decision, every tool call, fully visible.

Third‚ÄîEFFICIENT: Our Fusion Optimizer uses strategic planning, RAG-powered 
self-learning, and proactive optimization to slash costs from dollars to cents.

Fourth‚ÄîSOVEREIGNTY: Your data, your rules, your environment. Whether you choose 
cloud hyperscalers, AWS Bedrock, or fully private Ollama models‚Äîyou're in control.

Fifth‚ÄîFINANCIAL GOVERNANCE: Track every token, every penny, in real-time. Complete 
visibility into LLM spending with comprehensive analytics and cost optimization.

And sixth‚ÄîCOLLABORATIVE: Transform individual expertise into collective intelligence 
through our Intelligence Marketplace. Share patterns, subscribe to community wisdom, 
and amplify capabilities together.

This isn't just another data chat tool. Uderia is a complete AI orchestration platform 
that delivers enterprise results with optimized speed and minimal token cost.

Over the next few tutorials, you'll see exactly how Uderia transforms every aspect 
of data operations‚Äîfrom your first conversation to production deployment at scale.

Let's dive in.
```

**Screen Capture Plan:**
- [ ] Website hero section with tagline "Cloud-Level Reasoning. Zero-Trust Privacy."
- [ ] Six principles icons/cards appearing sequentially as mentioned
- [ ] Hybrid architecture diagram (cloud planner + local executor)
- [ ] Quick feature montage (API, Live Status Panel, Marketplace, Cost Dashboard)
- [ ] Live platform login screen teaser

**Supporting Materials:**
- README.md introduction
- Website value proposition sections
- Platform architecture diagram

---

### 1.2 UI Orientation & Core Components
**Duration:** 2-3 minutes  
**Learning Objectives:**
- Successfully log in or register
- Understand the UI layout at first glance
- Identify key navigation elements

**Key Topics to Cover:**
- [ ] Registration/Login process
- [ ] Platform UI overview (navigation bar, main panels)
- [ ] Quick orientation: Conversations, Executions, Intelligence, Marketplace, Admin
- [ ] Profile indicator and status elements

**Narration Script:**
```
Let's get you started with Uderia. If you're following along on the live platform 
at uderia.com, you'll see the login screen. New users can register in seconds‚Äî
just an email, username, and password. Enterprise deployments integrate with your 
existing SSO.

Once logged in, you're greeted by an interface designed around one goal: getting 
you from question to insight as quickly as possible.

Let's orient ourselves.

**NAVIGATION:** Five key sections across the top.

**Conversations**‚ÄîYour interactive workspace. Natural language queries. Real-time 
agent collaboration. This is where you'll spend most of your time.

**Executions**‚ÄîMission control. Monitor tasks. Track long-running queries. Review 
execution history across all automation workflows.

**Intelligence**‚ÄîWhere continuous improvement lives. Manage RAG collections. Review 
champion cases. Track efficiency gains as the platform learns from your successes.

**Marketplace**‚ÄîThe community hub. Discover execution patterns. Subscribe to expert 
knowledge. Contribute to collective intelligence. Network effects in action.

**Administration**‚ÄîYour governance center. User management. Cost analytics. Access 
tokens. System configuration. Financial oversight.

**PROFILE INDICATOR:** Top right corner.

Shows your active LLM profile‚Äîwhich language model and MCP tools you're using. Switch 
between specialized personas with one click. Think of it as choosing which expert 
consultant you're working with.

**THREE-PANEL WORKSPACE:**

**Left: Resource Panel**‚ÄîYour agent's capability catalog. Available MCP tools. Prompt 
templates. Data sources. Dynamically adapts based on your active profile.

**Center: Conversation Panel**‚ÄîPrimary interaction space. Queries. Results. Visualizations. 
Where the conversation happens.

**Right: Live Status Panel**‚ÄîThe transparency engine. Uderia's game-changer. Watch 
strategic plans unfold in real-time. See tactical decisions. Inspect raw data at every 
step. No black boxes. No guesswork. Complete visibility.

Everything you need. Within reach. Clean. Organized. Built for speed.

Now let's put it to work.
```

**Screen Capture Plan:**
- [ ] Login screen with registration option highlighted
- [ ] Successful login transition to dashboard
- [ ] Navigation bar with each section highlighted as mentioned
- [ ] Profile indicator zoom-in (top right corner)
- [ ] Three-panel layout with annotations: Resource (left), Conversation (center), Live Status (right)
- [ ] Quick hover over each panel to show dynamic content
- [ ] Smooth transition to next section (query input ready)

**Supporting Materials:**
- User Guide documentation
- UI screenshots

---

### 1.3 Your First Conversation - From Question to Insight
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Execute a simple query successfully
- Understand query ‚Üí insight ‚Üí action flow
- See immediate value in action

**Key Topics to Cover:**
- [ ] Typing a natural language query
- [ ] Watching the Live Status Panel (strategic plan appears)
- [ ] Tool execution visualization
- [ ] Receiving structured results
- [ ] Understanding the response format

**Narration Script:**
```
Let's ask your first question. No SQL. No technical syntax. Just ask naturally, like 
you would a colleague.

We're working with a fitness equipment retail database. Type: *"What are the top 5 
products by revenue this month?"*

Hit enter. Watch what happens.

**LIVE STATUS PANEL ACTIVATES:**

Strategic Plan appears instantly. The agent isn't blindly executing‚Äîit's thinking:

```
Phase 1: Identify relevant tables (Sales, Products, SaleDetails)
Phase 2: Calculate revenue by product for current month
Phase 3: Sort descending, return top 5
```

This is Uderia's transparency. No loading spinner. No wondering. You're watching a 
strategic blueprint unfold in real-time.

**TACTICAL EXECUTION:**

The agent selects its tool: `base_readQuery` (SQL execution)

Watch the SQL generate:

```sql
SELECT p.ProductName, p.Brand, 
       SUM(sd.Quantity * sd.UnitPrice) as Revenue
FROM Products p
JOIN SaleDetails sd ON p.ProductID = sd.ProductID
JOIN Sales s ON sd.SaleID = s.SaleID
WHERE EXTRACT(MONTH FROM s.SaleDate) = CURRENT_MONTH
GROUP BY p.ProductName, p.Brand
ORDER BY Revenue DESC
LIMIT 5;
```

Every parameter visible. Zero guesswork.

**RAW DATA RETURNS:**

Click into Live Status. Inspect the exact JSON response. Complete visibility.

**INTELLIGENT RENDERING:**

The agent doesn't dump data. It formats results beautifully:

```
1. Treadmill Pro 3000 (FitTech) - $45,230
2. Elliptical Elite (CardioMax) - $38,900
3. Weight Bench Deluxe (IronGrip) - $32,150
4. Rowing Machine Pro (AquaFit) - $28,500
5. Spin Bike Ultra (CycleMax) - $24,100
```

Clean. Formatted. Actionable.

**FOLLOW-UP:**

Type: *"Show me customer demographics for buyers of that top treadmill."*

Notice what just happened. The agent remembers "that top treadmill" = Treadmill Pro 3000. 
It knows the table structure. Context retained.

New strategic plan:

```
Phase 1: Find customers who purchased ProductID 1047 (Treadmill Pro 3000)
Phase 2: Retrieve demographics from Customers table
Phase 3: Aggregate by location, age, visualize
```

Joining Sales ‚Üí Customers automatically. You didn't specify a single table name.

**THE POWER:**

Question ‚Üí Insight: Seconds.
Insight ‚Üí Follow-up: Seamless.
Context: Retained.

No Python notebooks. No SQL editors. No context switching.

Just you, your questions, and an agent that shows its work.

**THE KICKER:**

Every query you just ran? Already available via REST API. Your conversational discovery 
is already operationalized. We'll explore this in Module 2.

**ONE MORE THING:**

See that profile indicator (top right)? You're using @ANALYST right now‚Äîbusiness 
analyst profile.

But imagine switching to:
- **@DBA** - SQL optimization expert
- **@QUALITY** - Data validation specialist  
- **@LOCAL** - Privacy-first, zero cloud exposure

One person orchestrating multiple expert personas. Switch like consulting different 
colleagues.

This is "god mode." We'll explore it in depth soon.

**WELCOME TO UDERIA:**

Intelligence that's actionable, transparent, and built for speed.

You're not just chatting with AI. You're orchestrating specialized expertise.

Let's keep going.
```

**Screen Capture Plan:**
- [ ] Query typed into input: "What are the top 5 products by revenue this quarter?"
- [ ] Live Status Panel: Strategic plan appears with phases
- [ ] Live Status Panel: Tactical section shows tool selection and SQL generation
- [ ] Live Status Panel: Raw data response visible (expandable JSON)
- [ ] Conversation Panel: Results render as formatted table with actual data
- [ ] Follow-up query typed: "Why did Product X decline compared to last quarter?"
- [ ] New strategic plan appears showing comparative analysis
- [ ] Results with trend analysis and potential chart visualization
- [ ] Quick flash of REST API documentation showing same query structure
- [ ] End with satisfied user looking at insights

**Supporting Materials:**
- fitness_db schema documentation (5 tables: Customers, Sales, SaleDetails, Products, ServiceTickets)
- MCP base_tools configuration (Teradata connector)
- Sample queries: revenue analysis, customer demographics, product trends
- Expected output samples with realistic fitness retail data
- MCP quality_tools for data validation demonstrations

---

## Module 2: ACTIONABLE - From Discovery to Production
**Goal:** Demonstrate seamless operationalization from conversation to automation

### 2.1 The Two-in-One Philosophy: Conversation ‚Üí API
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Understand the unique "discovery is production" paradigm
- See how a conversational query becomes an API call
- Grasp the value of eliminating handoffs

**Key Topics to Cover:**
- [ ] Traditional workflow: Analyst ‚Üí Data Engineer ‚Üí API Developer (days/weeks)
- [ ] Uderia workflow: One conversation ‚Üí Immediate API (seconds)
- [ ] REST API endpoint overview
- [ ] Session management concept
- [ ] Bearer token authentication basics

**Narration Script:**
```
Let's talk about what separates Uderia from every other data tool.

**THE OLD WORLD:**

Traditional workflow for ONE analytics question:

```
Business Analyst asks: "What are our top revenue products?"
  ‚Üì (2 days)
Data Analyst writes SQL in notebook
  ‚Üì (3 days)  
Data Engineer rebuilds it in pipeline
  ‚Üì (5 days)
API Developer wraps endpoint
  ‚Üì (2 days)
DevOps deploys to production

Total: ~2 weeks, 4 handoffs, context loss
```

This friction kills innovation.

**THE UDERIA WAY:**

We just asked: *"What are the top 5 products by revenue this month?"*

Got answer in seconds. That was conversational discovery.

**Here's the revolution:**

That EXACT query is already available via REST API.

- No rebuilding
- No translation
- No handoffs
- No deployment

**WATCH:**

Postman ‚Üí `/v1/query/submit` endpoint

```json
POST https://tda.uderia.com/api/v1/query/submit
Authorization: Bearer YOUR_TOKEN

{
  "query": "What are the top 5 products by revenue this month?",
  "session_id": "your-session-id"
}
```

Same question. Same strategic plan. Same tool execution. Same results.

**Or cURL:**

```bash
curl -X POST https://tda.uderia.com/api/v1/query/submit \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"query": "What are the top 5 products by revenue this month?", "session_id": "abc123"}'
```

**THE PARADIGM:**

Discovery = Production.

Conversation = Automation.

No engineering team. No deployment cycle. Zero friction.

**THE MULTIPLIER:**

Now imagine:

1. @DBA profile ‚Üí Query optimization
2. @ANALYST profile ‚Üí Business visualization  
3. @QUALITY profile ‚Üí Data validation

All three conversations ‚Üí All three APIs ‚Üí Instantly.

You're not automating one query. You're operationalizing an entire team of expert personas.

**THE IMPACT:**

Old world:
- Weeks to production
- Multiple specialists required
- Context lost in handoffs

Uderia:
- Seconds to production
- One person orchestrating profiles
- Context preserved

**From insight to automation: One conversation.**

This is actionable intelligence.

Let's see how it works.
```

**Screen Capture Plan:**
- [ ] Traditional workflow diagram: Analyst ‚Üí Engineer ‚Üí API Dev ‚Üí DevOps (days/weeks)
- [ ] Uderia workflow: One conversation ‚Üí Immediate API (seconds)
- [ ] Live query execution showing `base_readQuery` tool call
- [ ] Switch to Postman: same query via `/v1/query/submit` endpoint
- [ ] Show cURL command with actual request/response
- [ ] Side-by-side comparison: UI results vs API results (identical)
- [ ] REST API documentation page

**Supporting Materials:**
- REST API documentation (query endpoints)
- Postman collection with pre-configured requests
- cURL command examples
- Workflow comparison diagram
- MCP tools reference (base_readQuery)

---

### 2.2 Profile System: Complete AI Personas
**Duration:** 5-7 minutes  
**Learning Objectives:**
- Understand profiles as complete AI personas (LLM + Tools + Knowledge + Patterns)
- See how RAG Knowledge Collections integrate into profiles
- Learn how Planner Repositories provide learned execution patterns
- Master profile switching as persona switching
- Recognize the power of bundled intelligence

**Key Topics to Cover:**
- [ ] What is a profile? (Four-component persona architecture)
- [ ] Component 1: LLM Provider configuration
- [ ] Component 2: MCP Tools/Prompts (capabilities)
- [ ] Component 3: RAG Knowledge Collections (domain expertise)
- [ ] Component 4: Planner Repositories (execution patterns)
- [ ] Profile tags and switching (@DBA, @ANALYST, @LOCAL)
- [ ] Creating profiles as specialized personas
- [ ] Marketplace integration (subscribing to shared knowledge/planners)

**Narration Script:**
```
Now let's talk about profiles‚Äîbut not as simple configuration files. Profiles are 
something far more powerful: they're complete AI personas with memory, tools, knowledge, 
and learned behaviors.

When you switch profiles in Uderia, you're not just changing which LLM you're using. 
You're switching to a completely different intelligent agent with its own expertise, 
capabilities, and experience.

Let me show you what I mean.

**THE FOUR PILLARS OF A PROFILE:**

A profile bundles four critical components into a cohesive persona:

**1. LLM Provider** - Which language model powers the reasoning
Think: Claude Opus for complex analysis, Gemini Flash for cost efficiency, Ollama 
for complete privacy.

**2. MCP Tools & Prompts** - Which capabilities are available
Think: Database tools (base_readQuery, base_tableList), Quality tools (qlty_validateData), 
Admin tools (dba_databaseVersion), Platform tools (TDA_FinalReport, TDA_Charting).

**3. RAG Knowledge Collections** - What domain expertise the agent has access to
Think: SQL optimization guides, database documentation, data quality standards, business 
logic rules, company-specific knowledge.

**4. Planner Repositories** - Which execution patterns the agent has mastered
Think: Champion cases from successful queries‚Äîproven strategies for revenue analysis, 
customer segmentation, performance optimization, all learned from past successes.

Together, these four components create a complete persona. Let me show you with a 
real example.

**PERSONA EXAMPLE: @DBA Profile**

Navigate to Setup ‚Üí Profiles. Let's look at the "Database Administrator" profile with 
tag @DBA.

**Component 1: LLM Provider**
- Model: Claude Sonnet 3.5 (reliable, balanced performance)
- Cost tier: Mid-range (~$3 per 1M input tokens)

**Component 2: MCP Tools**
- Teradata MCP Server enabled
- Tools available:
  - base_databaseList (list all databases)
  - base_tableList (list tables in a database)
  - base_tableDDL (get table structure)
  - base_readQuery (execute SQL)
  - dba_databaseVersion (system version info)
  - qlty_profileColumn (data profiling)
  - qlty_validateData (quality checks)

**Component 3: RAG Knowledge Collections**
Let's click into the "Knowledge" tab for this profile.

You see three collections attached:
1. "Teradata SQL Optimization Guide" (23 documents)
   - Contains best practices for query performance
   - Index usage strategies
   - Join optimization techniques
   
2. "fitness_db Schema Documentation" (5 documents)
   - Table relationships
   - Business logic rules
   - Data quality constraints
   
3. "Database Administration Runbooks" (12 documents)
   - Troubleshooting procedures
   - Maintenance scripts
   - Performance tuning guides

When you ask this profile a question, it doesn't just have access to database tools‚Äî
it has access to your organization's accumulated DBA knowledge. 

Ask: "What's the best way to optimize a query joining Sales and SaleDetails for large 
date ranges?"

The agent retrieves relevant content from your SQL Optimization Guide: "For large 
date ranges, use partition elimination with explicit date predicates. Consider 
creating a covering index on SaleDate + frequently joined columns."

This isn't generic LLM knowledge. This is YOUR organization's expertise, embedded 
into the profile.

**Component 4: Planner Repositories**
Now click the "Planners" tab.

You see the "DBA_Champion_Cases" repository with 47 successful execution patterns:
- "Monthly Sales Aggregation" (champion case, 94/100 efficiency)
- "Customer Purchase History Lookup" (champion case, 91/100 efficiency)
- "Product Revenue Analysis with Trends" (champion case, 96/100 efficiency)

Each case contains:
- The original query that succeeded
- The strategic plan that was used
- The exact tool execution sequence (e.g., base_readQuery with optimized SQL)
- Token usage and cost data
- Efficiency metrics

When you ask the @DBA profile a new question, it searches these repositories for 
similar past successes. If it finds one, it uses that proven pattern as a few-shot 
example to guide the new execution.

Ask: "Show me top revenue products by category this quarter."

The Live Status Panel shows:
```
üîç RAG-RETRIEVED CONTEXT:

Similar patterns found in DBA_Champion_Cases:
1. "Product Revenue Analysis with Trends" (similarity: 0.92)
   - Strategy: Single aggregation with RANK() window function
   - Tools: base_readQuery ‚Üí TDA_FinalReport
   - SQL pattern: SUM(Quantity * UnitPrice) GROUP BY with PARTITION BY
   - Efficiency: 96/100 (champion case)
```

The agent learned from past success. It's using the proven champion pattern, not 
reinventing the approach.

This is profile-bound learning. The @DBA persona gets smarter every time a DBA uses it.

**PERSONA COMPARISON: @ANALYST Profile**

Now switch to the "Business Analyst" profile (@ANALYST). Watch what changes.

**LLM Provider:** Gemini 1.5 Flash (fast, cheap‚Äî$0.075 per 1M input tokens)

**MCP Tools:** Same Teradata tools, but ALSO:
- TDA_Charting (visualization generation)
- TDA_LLMTask (natural language analysis)

**RAG Knowledge:** Completely different collections:
1. "Business Metrics Definitions" (15 documents)
   - KPI calculation formulas
   - Reporting standards
   - Business terminology
   
2. "fitness_db Business Context" (8 documents)
   - Product categories explained
   - Customer segmentation rules
   - Sales cycle patterns

**Planner Repositories:** Different patterns:
- "Monthly_Executive_Reports" (23 cases focused on summarization)
- "Customer_Segmentation_Analysis" (17 cases with visualization)
- "Trend_Analysis_Patterns" (31 cases with time-series queries)

Same platform. Same database. Completely different persona.

Ask the @ANALYST profile: "Show me top revenue products by category this quarter."

The strategic plan is different:
- Phase 1: Query data with business-friendly column aliases
- Phase 2: Calculate percentage contribution by category
- Phase 3: Generate bar chart visualization
- Phase 4: Summarize insights in executive language

It uses TDA_Charting to create a visual. It frames results in business terms, not 
technical output. Because this persona has different knowledge, different patterns, 
and different priorities.

**PROFILE SWITCHING = PERSONA SWITCHING:**

This is the profound insight: switching profiles isn't about changing infrastructure. 
It's about switching intelligent personas.

Need database optimization? Switch to @DBA.
- Tools: Database admin tools
- Knowledge: SQL optimization, schema docs
- Patterns: Performance-tuned queries
- Behavior: Technical, optimization-focused

Need business insights? Switch to @ANALYST.
- Tools: Charting, summarization
- Knowledge: Business metrics, KPI definitions
- Patterns: Visualization-heavy reports
- Behavior: Executive-friendly, insight-focused

Need complete privacy? Switch to @LOCAL.
- LLM: Ollama (on-premises, zero cloud)
- Tools: Same capabilities, local execution
- Knowledge: Same collections, private retrieval
- Patterns: Same strategies, sovereign infrastructure

One platform. Multiple specialized personas. Each with their own expertise.

**CREATING A NEW PERSONA:**

Let's create a custom profile from scratch.

Setup ‚Üí Profiles ‚Üí New Profile

Name: "Data Quality Auditor"
Tag: @QUALITY

**Step 1: Choose LLM**
Provider: Claude Sonnet 3.5 (reliable for analysis)

**Step 2: Select MCP Server**
Teradata MCP ‚Üí This gives us database access

**Step 3: Attach Knowledge Collections**
Click "Add Knowledge Collection"
- Select: "Data Quality Standards" (your org's validation rules)
- Select: "fitness_db Constraints Documentation" (expected value ranges)

**Step 4: Attach Planner Repositories**
Click "Add Planner Repository"
- Select: "Quality_Validation_Patterns" (champion cases for data validation queries)

**Step 5: Configure Tool Preferences**
Enable specific tools:
- qlty_profileColumn (profiling)
- qlty_validateData (validation)
- base_readQuery (data sampling)

Save profile.

Now @QUALITY is a complete persona: 
- Reasoning: Claude Sonnet
- Capabilities: Quality tools + database access
- Expertise: Your org's quality standards
- Experience: Proven validation patterns

Ask @QUALITY: "Validate the Customers table for data quality issues."

The agent:
1. Retrieves validation rules from your "Data Quality Standards" collection
2. Uses proven patterns from "Quality_Validation_Patterns" repository
3. Executes qlty_profileColumn and qlty_validateData tools
4. Reports findings against YOUR organization's standards

This isn't generic data profiling. This is a specialized quality auditor persona with 
your company's rules and proven validation workflows.

**MARKETPLACE INTEGRATION:**

Here's where it gets powerful for organizations.

Go to Marketplace ‚Üí Knowledge Repositories.

Browse community-shared collections:
- "Teradata SQL Best Practices" (4,200 subscribers)
- "Retail Analytics Patterns" (1,800 subscribers)
- "Customer Churn Analysis Techniques" (950 subscribers)

Find one relevant: "Retail Analytics Patterns" 

Click "Subscribe" ‚Üí Select your @ANALYST profile ‚Üí Confirm.

That collection is now attached to your @ANALYST persona. Instant expertise expansion.

Or browse Marketplace ‚Üí Planner Repositories:
- "E-commerce Revenue Optimization" (3,100 subscribers, 127 champion cases)
- "Customer Lifetime Value Analysis" (2,400 subscribers, 89 cases)

Subscribe "E-commerce Revenue Optimization" to your @ANALYST profile.

Now your @ANALYST persona has 127 additional proven patterns from the community's 
best executions.

Your profiles don't just grow from your own usage‚Äîthey grow from collective intelligence.

**ORCHESTRATING A TEAM OF EXPERTS:**

Here's the ultimate power move. You're not working with ONE agent. You're orchestrating 
a TEAM of specialized experts‚Äîand you can switch between them instantly.

Watch this workflow:

**Scenario: Complex Business Intelligence Project**

You need to analyze Q4 revenue performance, but the workflow requires multiple types 
of expertise. In a traditional organization, this would involve:
- Database Admin to optimize queries
- Data Quality Engineer to validate data
- Business Analyst to generate insights
- Executive to review and approve

That's four people. Four handoffs. Days of coordination.

With Uderia? You ARE all four experts. Watch.

**Step 1: Start with @DBA - Database Optimization**

Type: "@DBA What's the most efficient way to query Q4 revenue across all product 
categories?"

The @DBA persona:
- Uses SQL optimization knowledge from your Teradata guides
- Applies proven champion patterns for revenue aggregation
- Generates optimal SQL with proper indexing strategy
- Returns: Optimized query structure

Result: "Use partitioned query with covering index on SaleDate, ProductType. Estimated 
execution time: 1.2s for 2.3M rows."

**Step 2: Switch to @QUALITY - Data Validation**

Type: "@QUALITY Validate the data quality for Q4 sales data before analysis."

The @QUALITY persona:
- Uses your organization's data quality standards
- Executes qlty_profileColumn on Sales table
- Applies validation rules from your quality knowledge base
- Returns: Data quality report

Result: "Q4 data validated: 99.7% completeness, 2 outliers flagged in SaleDetails 
(likely data entry errors), recommend excluding SaleID 45023, 45089."

**Step 3: Switch to @ANALYST - Business Intelligence**

Type: "@ANALYST Generate Q4 revenue analysis by category with trends and visualizations."

The @ANALYST persona:
- Uses business metrics definitions for proper KPI calculation
- Applies executive reporting patterns from champion cases
- Generates TDA_Charting visualizations
- Frames insights in business language

Result: Beautiful charts + "Cardio equipment down 18% YoY due to market saturation. 
Accessories up 34% driven by home fitness trend. Recommend diversification strategy."

**Step 4: Switch to @LOCAL - Sensitive Deep Dive**

Type: "@LOCAL Show me individual customer purchase patterns for our top 10 customers."

The @LOCAL persona:
- Uses Ollama (local model, zero cloud exposure)
- Same MCP tools, but execution stays on-premises
- Customer PII never leaves your infrastructure
- Returns: Detailed customer analysis with complete privacy

Result: Customer-level insights without compromising sovereignty.

**Four personas. Four specializations. One seamless workflow. Under 5 minutes.**

This is what we call "God Mode"‚Äîyou're not limited to one agent's perspective. You 
orchestrate an entire team of specialized experts, each with their own knowledge base 
and proven execution patterns, switching between them as naturally as asking a colleague 
for their input.

No coordination overhead. No context lost in handoffs. No delays.

Just instant access to specialized intelligence, perfectly suited for each phase of work.

**THE PROFILE PARADIGM SHIFT:**

Forget "configuration management." Profiles are intelligent personas:

- @DBA knows SQL optimization and has proven DBA workflows
- @ANALYST speaks business language and has visualization patterns  
- @QUALITY enforces your org's standards and has validation playbooks
- @LOCAL preserves sovereignty while maintaining same capabilities

Switch profiles. Switch personas. Orchestrate experts. Work in god mode.

And every profile gets smarter:
- RAG captures successful queries as champion cases
- Knowledge collections grow with new documentation
- Marketplace subscriptions add community wisdom
- Planner repositories expand with every execution

This is profile-bound continuous improvement. Your personas evolve.

**COLLABORATIVE INTELLIGENCE AT SCALE:**

In traditional organizations, expertise is siloed:
- The DBA knows database optimization but not business context
- The analyst knows metrics but not data quality rules
- The quality engineer knows validation but not SQL tuning

In Uderia, YOU have access to ALL specialized personas, each with complete domain 
expertise, and you can orchestrate them together in a single workflow.

This isn't just faster. This is a fundamentally different way of working.

One person. Multiple expert personas. Instant switching. Zero friction.

From individual contributor to orchestrator of specialized intelligence.

From "I need to ask the DBA" to "Let me switch to @DBA persona."

From days of coordination to seconds of profile switching.

This is Uderia's profile system. Not just settings‚Äîcomplete AI personas with memory, 
tools, knowledge, and experience.

Infrastructure as personas. Intelligence as a package. Expertise as a profile.

And YOU as the orchestrator. God mode activated.
```

**Screen Capture Plan:**
- [ ] Setup ‚Üí Profiles landing page showing multiple personas
- [ ] @DBA Profile detail view - Four tabs visible: Config, Tools, Knowledge, Planners
- [ ] Config tab: Claude Sonnet 3.5 selected
- [ ] Tools tab: List of MCP tools (base_*, dba_*, qlty_*)
- [ ] Knowledge tab: Three collections attached with document counts
  - Teradata SQL Optimization Guide (23 docs)
  - fitness_db Schema Documentation (5 docs)
  - Database Administration Runbooks (12 docs)
- [ ] Click into one collection: show document list with titles
- [ ] Planners tab: DBA_Champion_Cases repository (47 cases)
- [ ] Case detail view: "Product Revenue Analysis" champion case
  - Original query
  - Strategic plan
  - Tool execution trace with base_readQuery
  - Efficiency score: 96/100
- [ ] Query with @DBA: "Optimize query joining Sales and SaleDetails"
- [ ] Live Status Panel: RAG context shows retrieved SQL optimization knowledge
- [ ] Switch to @ANALYST profile - watch UI update
- [ ] @ANALYST Tools tab: Additional TDA_Charting, TDA_LLMTask
- [ ] @ANALYST Knowledge tab: Different collections (Business Metrics, Business Context)
- [ ] @ANALYST Planners tab: Different repositories (Executive Reports, Segmentation)
- [ ] Same query with @ANALYST: Different strategic plan (includes visualization phase)
- [ ] Results include chart generation
- [ ] Create new profile: "Data Quality Auditor" @QUALITY
- [ ] Step through four-component setup
- [ ] Marketplace ‚Üí Knowledge Repositories browser
- [ ] "Retail Analytics Patterns" collection with subscriber count
- [ ] Subscribe to @ANALYST profile
- [ ] Marketplace ‚Üí Planner Repositories browser
- [ ] "E-commerce Revenue Optimization" with 127 cases
- [ ] Subscribe to @ANALYST profile
- [ ] @ANALYST profile now shows expanded Knowledge + Planners counts
- [ ] Side-by-side comparison: @DBA vs @ANALYST vs @QUALITY personas

**Supporting Materials:**
- Profile architecture diagram (four-component model)
- RAG Knowledge Collections documentation
- Planner Repositories guide
- MCP server configuration guide
- Marketplace subscription workflow
- Profile tag reference (@DBA, @ANALYST, @QUALITY, @LOCAL, @PROD, @COST, @OPUS)
- Classification mode comparison (Light vs Full)
- Example persona configurations (DBA, Analyst, Quality Auditor, Data Scientist)
- Champion case efficiency scoring methodology
- Knowledge collection upload guide
- Planner repository case capture process

---

### 2.3 REST API Power: Operationalize Everything
**Duration:** 4-5 minutes  
**Learning Objectives:**
- Master REST API capabilities
- Understand async task pattern
- Learn endpoint categories

**Key Topics to Cover:**
- [ ] API endpoint categories (session, query, task, config, RAG, analytics)
- [ ] Async submit + poll pattern
- [ ] Session management (create, reuse, delete)
- [ ] Long-lived access tokens
- [ ] Profile override via API

**Narration Script:**
```
Now let's see the REST API that makes everything programmable.

**SIX ENDPOINT CATEGORIES:**

1. **Session Management** - Create/delete conversation contexts
2. **Query Execution** - Submit/poll/retrieve  
3. **Task Management** - Cancel, check status
4. **Configuration** - Switch profiles, control MCP
5. **RAG Operations** - Manage knowledge collections
6. **Analytics** - Track costs, tokens, efficiency

**AUTHENTICATION:**

Admin Panel ‚Üí Generate long-lived access token (90 days or permanent).

Copy once. Never shown again. Store securely.

**WORKFLOW:**

**Step 1: Create Session**

```bash
POST /api/v1/sessions/create
{
  "session_name": "Revenue Analysis",
  "profile_override": "@PROD"
}

‚Üí {"session_id": "abc-123"}
```

Isolated conversation context. All queries share memory.

**Step 2: Submit Query (Async)**

```bash
POST /api/v1/query/submit
{
  "query": "What are top 5 products by revenue this month?",
  "session_id": "abc-123"
}

‚Üí {"task_id": "task_789"}
```

No immediate results. Agent works in background.

**Step 3: Poll Status**

```bash
GET /api/v1/query/status/task_789

‚Üí {"status": "running", "progress": "Phase 1: Database query"}

(poll again)

‚Üí {"status": "completed", "result_ready": true}
```

**Step 4: Retrieve Results**

```bash
GET /api/v1/query/result/task_789

‚Üí {
  "query": "...",
  "answer": "...",
  "tool_calls": [
    {"tool": "base_readQuery", "args": {...}, "result": [...]}
  ],
  "tokens": {"input": 1250, "output": 320},
  "cost": 0.0042
}
```

Everything you'd see in UI‚Äîas JSON.

Answer + Tool executions + Token counts + Costs.

Complete transparency via API.

**Step 5: Follow-Up (Context Preserved)**

```bash
POST /api/v1/query/submit
{
  "query": "Show customer demographics for that top product",
  "session_id": "abc-123"
}
```

Agent remembers previous query. Context maintained across API calls.

**PROFILE OVERRIDE:**

```bash
POST /api/v1/query/submit
{
  "query": "Complex analytical question",
  "session_id": "abc-123",
  "profile_override": "@OPUS"
}
```

This query runs on Claude Opus. Next query returns to default profile.

**TASK CANCELLATION:**

```bash
POST /api/v1/tasks/cancel/task_789
```

Gracefully stops long-running execution.

**PRODUCTION USE CASES:**

- Batch processing (Airflow)
- Chatbot integrations (Flowise)
- Custom dashboards (live insights)
- CI/CD validation queries
- Scheduled overnight reports

Your conversational intelligence‚Äîfully programmable.

Production-grade AI orchestration.
```

**Screen Capture Plan:**
- [ ] API documentation landing page with six categories
- [ ] Admin panel: Generate long-lived access token
- [ ] Postman collection: Session creation request/response
- [ ] Postman: Query submission (async) ‚Üí task_id returned
- [ ] Postman: Status polling (running ‚Üí completed)
- [ ] Postman: Result retrieval with full JSON response
- [ ] Show tool_calls array with `base_readQuery` execution details
- [ ] Follow-up query demonstrating context preservation
- [ ] Profile override example (@OPUS)
- [ ] Task cancellation request

**Supporting Materials:**
- Complete REST API documentation
- Postman collection (pre-configured)
- Authentication guide (JWT vs access tokens)
- Async pattern best practices
- Error handling reference
- Rate limiting documentation

**Supporting Materials:**
- REST API complete guide
- Postman collection
- Python/JavaScript SDK examples

---

### 2.4 Airflow Integration: Production Automation
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Understand batch automation use case
- Learn DAG structure for Uderia integration
- See production-ready patterns

**Key Topics to Cover:**
- [ ] Apache Airflow integration overview
- [ ] Example DAG walkthrough
- [ ] Session reuse pattern
- [ ] Profile override for specialized workloads
- [ ] Async polling pattern for long-running queries

**Narration Script:**
```
Let's put the REST API into production with Apache Airflow‚Äîindustry-standard workflow orchestration.

Uderia ships with production-ready DAG examples. Deploy immediately.

**AIRFLOW UI:** "TDA Execute Questions" DAG - Runs analytical queries daily at 6 AM.

**CODE STRUCTURE:** `tda_00_execute_questions.py`

**1. SESSION REUSE PATTERN**

Create ONE session for all queries:

```python
def create_tda_session(**context):
    response = requests.post(
        f"{TDA_BASE_URL}/api/v1/sessions/create",
        headers={"Authorization": f"Bearer {TDA_ACCESS_TOKEN}"},
        json={"session_name": f"Airflow Batch {context['ds']}"}
    )
    session_id = response.json()["session_id"]
    context['ti'].xcom_push(key='tda_session_id', value=session_id)
```

Session ID stored in XCom. All tasks share context. Query 2 references Query 1‚Äîconversational follow-ups in batch mode.

**2. ASYNC SUBMIT + POLL**

```python
def submit_query(query_text, session_id):
    # Submit
    response = requests.post(
        f"{TDA_BASE_URL}/api/v1/query/submit",
        json={
            "query": query_text,
            "session_id": session_id,
            "profile_override": "@PROD"
        }
    )
    task_id = response.json()["task_id"]
    
    # Poll until complete
    while True:
        status = requests.get(
            f"{TDA_BASE_URL}/api/v1/query/status/{task_id}"
        ).json()
        
        if status["status"] == "completed":
            break
        elif status["status"] == "failed":
            raise Exception(f"Query failed: {status['error']}")
        
        time.sleep(5)
    
    # Retrieve results
    return requests.get(
        f"{TDA_BASE_URL}/api/v1/query/result/{task_id}"
    ).json()
```

No timeouts. No blocking. Graceful long-running query handling.

**3. PROFILE OVERRIDE PER TASK**

Morning batch: `@COST` (Gemini Flash, cheap)
Executive reports: `@OPUS` (Claude Opus, accurate)

Same infrastructure. Different profiles per workload.

**4. ERROR HANDLING + RETRIES**

```python
task = PythonOperator(
    task_id='query_revenue_analysis',
    python_callable=submit_query,
    op_kwargs={'query_text': 'What are monthly revenue trends?'},
    retries=3,
    retry_delay=timedelta(minutes=2)
)
```

Airflow auto-retries failures. MCP hiccup? LLM timeout? Handled.

**5. SCHEDULING**

```python
dag = DAG(
    'tda_execute_questions',
    schedule_interval='0 6 * * *',  # Daily 6 AM
    catchup=False
)
```

Cron scheduling. Manual trigger. API trigger. Upstream data trigger. Full flexibility.

**6. RESULTS DISTRIBUTION**

```python
def store_results(result, **context):
    s3.put_object(
        Bucket='analytics-reports',
        Key=f"daily_revenue_{context['ds']}.json",
        Body=json.dumps(result)
    )
```

Uderia: Intelligence. Airflow: Distribution.

**WATCH IT RUN:**

```
Task 1: Create session ‚úì (XCom: session_id)
Task 2: Query "Top revenue products" ‚úì (base_readQuery ‚Üí fitness_db)
Task 3: Query "Customer demographics" ‚úì (context from Task 2)
Task 4: Store to S3 ‚úì
```

Four tasks. One conversational workflow. Production-grade.

**THE SHIFT:**

Old: Notebook explorations ‚Üí Manual engineering ‚Üí Separate deployment

Uderia: Conversation ‚Üí API ‚Üí Airflow orchestration

Same agent. Same queries. Production reliability.

Conversational AI meets enterprise workflows.
```

**Screen Capture Plan:**
- [ ] Airflow UI: DAG graph view showing task dependencies
- [ ] Code editor: `tda_00_execute_questions.py` walkthrough
- [ ] Session creation task code
- [ ] Async submit + poll code pattern
- [ ] Profile override configuration
- [ ] Error handling and retry configuration
- [ ] Schedule interval setting (cron)
- [ ] Manual DAG trigger
- [ ] Live task execution: green checks appearing
- [ ] Task logs showing API calls and responses
- [ ] XCom variables showing session_id propagation
- [ ] Results storage task completing

**Supporting Materials:**
- docs/Airflow complete documentation
- Example DAG files (tda_00_execute_questions.py)
- Airflow setup guide
- Best practices document
- Error handling patterns
- Production deployment checklist

**Supporting Materials:**
- docs/Airflow documentation
- Example DAG files
- Production deployment guide

---

## Module 3: TRANSPARENT - Eliminate the Black Box
**Goal:** Build trust through complete visibility into agent reasoning

### 3.1 Live Status Panel: Watch the Agent Think
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Understand real-time agent reasoning visibility
- Learn to read strategic plans
- Recognize tactical decisions and tool selections

**Key Topics to Cover:**
- [ ] Live Status Panel components (Strategic Plan, Tactical Execution, Raw Data)
- [ ] Streaming updates via Server-Sent Events
- [ ] Phase-by-phase plan breakdown
- [ ] Tool selection rationale display
- [ ] Self-correction events

**Narration Script:**
```
Welcome to the feature that changes AI trust‚Äîthe Live Status Panel.

**THE BLACK BOX PROBLEM:**

Most AI tools:
- You ask ‚Üí You wait ‚Üí You get answer
- What happened between? Unknown.
- Accurate data? Hallucination? Guesswork.

Uderia eliminates guesswork. Radical transparency.

**SCENARIO: Customer Support Director**

Complaints piling up. Need data.

Ask: *"Compare service ticket resolution times across product categories and identify quality issues."*

Complex query:
- Join ServiceTickets + Products
- Calculate time deltas
- Group by category  
- Infer quality patterns

Hit enter. Watch right panel.

**STRATEGIC PLAN APPEARS:**

```
Phase 1: Identify relevant tables (ServiceTickets, Products)
Phase 2: Calculate average resolution times by ProductType
Phase 3: Identify high-ticket products with slow resolution
Phase 4: Generate comparative analysis report
```

Not a loading spinner. A strategic blueprint.

Agent shows reasoning BEFORE executing. Validate approach before burning tokens.

**TACTICAL EXECUTION:**

**Phase 1:**

```
Tool: base_tableList
Arguments: {"database": "fitness_db"}
Rationale: "Verify ServiceTickets and Products tables exist"
```

Exact tool. Exact arguments. Exact reason. Zero ambiguity.

**Raw Data (click to expand):**

```json
{
  "tables": ["Customers", "Products", "Sales", "SaleDetails", "ServiceTickets"],
  "database": "fitness_db"
}
```

Complete visibility into agent's data sources.

**Phase 2:**

```
Tool: base_readQuery
SQL: 
  SELECT p.ProductType, 
         AVG(CAST(st.ResolutionDate - st.TicketDate AS INT)) as AvgResolutionDays
  FROM fitness_db.ServiceTickets st 
  JOIN fitness_db.Products p ON st.ProductID = p.ProductID
  WHERE st.ResolutionDate IS NOT NULL
  GROUP BY p.ProductType
  ORDER BY AvgResolutionDays DESC
Rationale: "Calculate average resolution time by product type"
```

Exact SQL visible. Spot logic issues immediately. No blind trust.

**Raw Results:**

```json
[
  {"ProductType": "Cardio", "AvgResolutionDays": 8.3},
  {"ProductType": "Strength", "AvgResolutionDays": 5.1},
  {"ProductType": "Accessories", "AvgResolutionDays": 2.7}
]
```

**Phase 3:**

```
Tool: TDA_LLMTask
Task: "Analyze which categories show quality issues based on ticket volume + resolution time"
Data: [resolution data + ticket counts]
Rationale: "Need LLM reasoning for pattern identification beyond metrics"
```

Delegated analytical reasoning. Decision made explicitly. Not hidden.

**SELF-CORRECTION (The Trust Builder):**

Agent tries wrong table:

```
Tool: base_readQuery
SQL: ...FROM ProductReturns...
‚ùå Error: "Table fitness_db.ProductReturns does not exist"
```

**RECOVERY:**

Live Status Panel shows:

```
üîÑ Error detected. Replanning.
Strategy: Use ServiceTickets as quality proxy (ProductReturns unavailable)
New approach: Query ServiceTickets grouped by ProductID for complaint patterns
```

```
Tool: base_readQuery (corrected)
SQL: ...FROM ServiceTickets GROUP BY ProductID...
‚úÖ Success
```

**WHAT JUST HAPPENED:**

Agent failed ‚Üí Recognized failure ‚Üí Replanned ‚Üí Succeeded

All visible. All transparent.

**CONTRAST:**

Traditional AI: Generic error or hallucinated answer
Uderia: Think ‚Üí Execute ‚Üí Detect ‚Üí Self-correct (all visible)

**THE IMPACT:**

Live Status Panel = Radical visibility

Every decision. Every tool call. Every data source. Every recovery.

No black boxes. No guesswork.

Complete trust through complete transparency.
```

**Screen Capture Plan:**
- [ ] Complex query input: "Compare service ticket resolution times..."
- [ ] Strategic Plan appears with 4 phases listed
- [ ] Tactical section: Phase 1 executing with `base_tableList` tool
- [ ] Raw Data section: Click to expand JSON response
- [ ] Tactical section: Phase 2 executing with `base_readQuery` showing full SQL
- [ ] Raw Data: Query results JSON display
- [ ] Tactical section: Phase 3 with `TDA_LLMTask` delegation
- [ ] Error scenario: Wrong table name ‚Üí Error message appears
- [ ] Recovery strategy displayed in Live Status
- [ ] Corrected query execution succeeds
- [ ] Final results rendering in Conversation Panel

**Supporting Materials:**
- Live Status Panel documentation
- Complex query examples
- Self-correction scenarios
- Error handling guide
- Tool execution logs

**Supporting Materials:**
- Live Status Panel documentation
- Example complex queries
- Self-correction examples

---

### 3.2 Capabilities Discovery: Know What's Possible
**Duration:** 2-3 minutes  
**Learning Objectives:**
- Explore available MCP tools
- Understand prompt library organization
- Learn resource enumeration

**Key Topics to Cover:**
- [ ] Capabilities Panel overview
- [ ] MCP Tools tab (automatic discovery)
- [ ] Prompts tab (categorized library)
- [ ] Resources tab (data source visibility)
- [ ] Dynamic updates on configuration changes

**Narration Script:**
```
Trust requires knowing what the agent can do.

Capabilities Panel = Complete visibility into tools, prompts, resources.

**LEFT PANEL:** Three tabs

**1. TOOLS TAB**

Complete catalog of MCP tools (current profile).

Organized by category:

**Database Operations:**
- `base_databaseList` - List all databases
- `base_tableList` - List tables in database
- `base_tableDDL` - Get table DDL structure
- `base_readQuery` - Execute SELECT queries

**Data Quality:**
- `qlty_dataProfile` - Profile data quality metrics
- `qlty_findAnomalies` - Detect statistical anomalies
- `qlty_validateConstraints` - Check constraint violations

**System Administration:**
- `dba_databaseVersion` - Get system version
- `dba_systemHealth` - Check resource utilization
- `dba_userActivity` - Monitor user sessions

Each tool shows:
1. Exact name agent uses
2. Clear description
3. Arguments (required vs optional - hover for details)

Not documentation. Real-time inventory of current capabilities.

**2. PROMPTS TAB**

Pre-built complex workflows. Invoke like specialized team members.

**Business Intelligence:**

- `base_databaseBusinessDesc` - Business context for database
  - Args: database_name (required), detail_level (optional)
  - Returns: Purpose, entities, use cases

- `base_tableBusinessDesc` - Business meaning of table
  - Args: database_name, table_name (required)
  - Returns: Purpose, key columns, business rules

**Data Quality:**

- `qlty_databaseQuality` - Comprehensive quality assessment
  - Args: database_name (required), include_profiling (optional)
  - Returns: Scores, issues, recommendations

**Administration:**

- `dba_databaseHealthAssessment` - Full system health report
  - Args: include_recommendations (optional)
  - Returns: Resource usage, performance, alerts

- `dba_userActivityAnalysis` - User behavior patterns
  - Args: time_range (optional, default: 24h)
  - Returns: Active users, query volumes, resource usage

Instead of complex multi-step queries, invoke prompts that know the workflow.

**3. RESOURCES TAB**

Data sources + knowledge repositories:

- Database connections (e.g., fitness_db on Teradata)
- API endpoints (if configured)
- Knowledge collections (uploaded docs)
- External tools (if integrated)

**DYNAMIC CAPABILITY UPDATES:**

Add new capability:

Setup ‚Üí MCP Servers ‚Üí Add New Server

Example: PostgreSQL for cross-database analysis

```
Name: "Analytics PostgreSQL"
Host: analytics-db.company.com
Port: 5432
Path: /mcp
```

Save. Add to profile.

Capabilities Panel refreshes (auto or manual).

New tools appear:
- `postgres_queryExecute`
- `postgres_schemaIntrospect`
- `postgres_tableSample`

Agent immediately knows. No restart. Dynamic discovery.

Now ask: *"Compare revenue trends in Teradata production vs PostgreSQL analytics models."*

Agent sees both:
- `base_readQuery` (Teradata)
- `postgres_queryExecute` (PostgreSQL)

Orchestrates cross-database analysis automatically.

**CAPABILITY FILTERING:**

Disable expensive tool:

Setup ‚Üí Capabilities Management ‚Üí Tools
Find `base_tableDDL` ‚Üí Toggle OFF

Capabilities Panel: Tool turns gray ("Disabled")

Ask: *"Show me Products table structure."*

Agent uses alternatives:
- `qlty_dataProfile`
- INFORMATION_SCHEMA query

`base_tableDDL` unavailable ‚Üí Agent adapts.

Re-enable ‚Üí Back in action.

**THE POWER:**

Dynamic capability management. Real-time control. No code.

Capabilities Panel = Window into agent potential.

Every tool. Every prompt. Every resource. Visible. Managed.

Know your agent. Trust your agent. Control your agent.
```

**Screen Capture Plan:**
- [ ] Capabilities Panel (left side) with three tabs
- [ ] Tools tab: Expand "Database Operations" category showing base_* tools
- [ ] Tool hover showing arguments and descriptions
- [ ] Expand "Data Quality" category showing qlty_* tools
- [ ] Expand "System Administration" showing dba_* tools
- [ ] Switch to Prompts tab
- [ ] Expand "Business Intelligence" showing base_databaseBusinessDesc, base_tableBusinessDesc
- [ ] Prompt detail view showing arguments (required vs optional)
- [ ] Expand "Data Quality" showing qlty_databaseQuality
- [ ] Expand "Administration" showing dba_* prompts
- [ ] Switch to Resources tab showing database connections
- [ ] Setup ‚Üí MCP Servers ‚Üí Add New Server (PostgreSQL example)
- [ ] Capabilities Panel refresh showing new postgres_* tools
- [ ] Capability filtering: Disable base_tableDDL
- [ ] Tool turns gray in Capabilities Panel
- [ ] Query execution using alternative tool
- [ ] Re-enable tool demonstration

**Supporting Materials:**
- MCP Tools complete reference
- Prompt library documentation
- Dynamic capability management guide
- MCP server configuration examples
- Tool filtering best practices

**Supporting Materials:**
- MCP Tools documentation
- Prompt library reference
- Capability discovery guide

---

### 3.3 Context Management: Control Agent Memory
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Master turn activation/deactivation
- Understand context purge vs selective memory
- Learn query replay functionality

**Key Topics to Cover:**
- [ ] Turn-level context control (checkboxes)
- [ ] Full Context vs Turn Summaries modes
- [ ] Context purge for memory reset
- [ ] Query replay for alternative approaches
- [ ] Context indicator real-time status

**Narration Script:**
```
Most underappreciated AI feature: Context management‚Äîcontrolling what the agent remembers.

Uderia: Surgical precision over agent memory.

**TURN HISTORY PANEL:**

Session History (below conversation). Every query-response = "turn" with checkbox.

**Current conversation:**

```
Turn 1: ‚úì "What are the top 5 products by revenue?"
Turn 2: ‚úì "Show customer demographics for those products"
Turn 3: ‚úì "Which states have the highest sales?"
Turn 4: ‚úì "What's the average ticket resolution time?"
```

All checked = All active in context.

**SELECTIVE MEMORY:**

Uncheck Turn 1.

Context indicator updates: "3 turns active"

Ask: *"What are the top selling products this week?"*

Agent treats as fresh question. Turn 1 deactivated = Not referenced.

Turns 2-4 still available if needed.

**WHY THIS MATTERS:**

Exploration mistake:
- Asked about "Product X"
- Meant "Product Y"

Traditional AI: Carries mistake through entire conversation

Uderia: Uncheck wrong turn ‚Üí Never happened ‚Üí Clean continuation

**CONTEXT MODES:**

Top bar toggle: "Full Context" vs "Turn Summaries"

**Full Context (default):**
- Every active turn's complete text ‚Üí LLM
- High token usage
- Maximum accuracy

**Turn Summaries:**
- Only summaries sent
- Lower token cost  
- Slightly less detail

Switch to Turn Summaries. Ask: *"What patterns do you see across our conversation?"*

Agent receives:

```
- Turn 1 summary: "Top products by revenue"
- Turn 2 summary: "Customer demographics request"
- Turn 3 summary: "Geographic sales distribution"
- Turn 4 summary: "Service ticket resolution times"
```

Reasoning maintained. 70% fewer tokens.

Long exploratory sessions (dozens of questions): Turn Summaries = Significant cost savings.

**CONTEXT PURGE:**

Fresh start WITHOUT creating new session (preserves history + tool results).

Click "Purge Context" ‚Üí Confirm

All checkboxes uncheck. Context indicator: "0 turns active"

Agent has zero conversational memory. Every query = Brand new.

History still visible. Reactivate turns selectively.

**QUERY REPLAY:**

Turn 2 gave interesting results. Want different angle?

Three-dot menu (Turn 2) ‚Üí "Replay Query"

Original question appears: *"Show customer demographics for those products."*

Modify it: *"Show customer demographics AND purchasing patterns for those products."*

Hit enter.

Agent executes with Turn 2 context (Turn 1 still active) + enhanced question.

Becomes Turn 5.

Alternative path explored. Original results preserved.

**CONTEXT INDICATOR:**

Top right - real-time status:

```
"4/5 turns active (2,340 tokens)"
```

Tells you:
- 4 of 5 turns providing context
- Current context size: 2,340 tokens
- Exact token count sent to LLM

**ADVANCED USE CASE:**

Complex analysis: 20 turns

```
Turns 1-5: Data exploration (messy)
Turns 6-15: Deep analysis (focused)
Turns 16-20: Final synthesis (polished)
```

Goal: Clean summary without exploratory noise.

Strategy:
- Deactivate Turns 1-5
- Keep Turns 6-20 active

Ask: *"Summarize our key findings."*

Agent generates summary from:
- Deep analysis (6-15)
- Final synthesis (16-20)

Exploratory phase excluded. Context clean.

**THE POWER:**

Surgical control over agent memory.

Context = Tool (not limitation)

No other platform: This precision.

Uderia: Context management = First-class feature.

Real-world workflows: What agent remembers = As important as what it can do.

Memory precision + Cost control + Exploratory freedom.

All in your hands.
```

**Screen Capture Plan:**
- [ ] Session History panel showing 4 turns with checkboxes (all checked)
- [ ] Context indicator showing "4 turns active"
- [ ] Uncheck Turn 1 ‚Üí Context indicator updates to "3 turns active"
- [ ] New query asked with Turn 1 deactivated
- [ ] Context mode toggle: Switch from "Full Context" to "Turn Summaries"
- [ ] Token count comparison (Full Context: 4,200 tokens vs Turn Summaries: 1,300 tokens)
- [ ] Context Purge button ‚Üí Confirmation dialog
- [ ] All checkboxes unchecked after purge
- [ ] Context indicator showing "0 turns active"
- [ ] Turn 2 three-dot menu ‚Üí "Replay Query"
- [ ] Modified query in input box
- [ ] New Turn 5 executes with enhanced question
- [ ] Complex scenario: 20 turns displayed, selectively activate Turns 6-20
- [ ] Summary query generates clean results

**Supporting Materials:**
- Context management best practices
- Token optimization guide
- Turn Summaries vs Full Context comparison
- Advanced exploration workflows
- Cost optimization through context control

**Supporting Materials:**
- Context management guide
- Advanced context strategies
- Use case examples

---

### 3.4 System Customization: Shape Agent Behavior
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Edit system prompts for custom behavior
- Test baseline LLM without tools
- Manage capability enable/disable

**Key Topics to Cover:**
- [ ] System Prompt Editor access
- [ ] Per-model instruction customization
- [ ] Save and reset capabilities
- [ ] Direct Model Chat for baseline testing
- [ ] Dynamic capability management (tools/prompts)

**Narration Script:**
```
Transparency = Seeing what agent does.
Customization = Controlling HOW it behaves.

Uderia: Deep customization. No proprietary walls.

**SYSTEM PROMPT EDITOR:**

Setup ‚Üí Advanced ‚Üí System Prompts

Actual system instructions. Guide agent behavior.

Organized by provider:

```
- MASTER_SYSTEM_PROMPT (Universal)
- GOOGLE_MASTER_SYSTEM_PROMPT (Gemini optimizations)
- OLLAMA_MASTER_SYSTEM_PROMPT (Local model adaptations)
```

Click "Edit" on MASTER_SYSTEM_PROMPT:

```
You are an enterprise data intelligence agent with tools and knowledge.

Your role:
1. Decompose queries into strategic phases
2. Select optimal tools per phase
3. Execute with precision, self-correct on errors
4. Provide structured, actionable insights

Planning:
- Break complex requests into clear phases
- Identify required tools before execution
- Consider data dependencies
- Anticipate errors

[...more...]
```

This is the agent's brain. Editable.

**CUSTOMIZE REASONING:**

Add verbosity:

```
When selecting tools, always explain:
- Why this tool is best
- What alternatives you considered
- What you expect returned
```

Save. Ask: *"What are the top revenue products?"*

**Before:**
```
Tool selected: base_readQuery
```

**After:**
```
Tool selected: base_readQuery
Rationale: Direct SQL execution optimal for revenue aggregation.
Alternatives: base_tableList (insufficient‚Äîstructure only), 
              qlty_dataProfile (unnecessary‚Äînot quality query)
Expected: Table with ProductName, Revenue (sorted desc)
```

You customized agent communication by editing system prompt.

**PROVIDER-SPECIFIC PROMPTS:**

Gemini ‚â† Claude ‚â† GPT-4. Optimizations differ.

GOOGLE_MASTER_SYSTEM_PROMPT:

```
Function calling: Structure arguments as clear JSON
Gemini strength: Parallel tool execution
Identify opportunities for simultaneous tool calls
```

Provider-specific tuning = Optimal performance per model.

**SAVE AND RESET:**

```
Save to Overrides: Stores in prompt_overrides/ (persists across updates)
Reset to Default: Revert to baseline
```

**DIRECT MODEL CHAT:**

Test raw LLM. No tools. No agent logic.

Setup ‚Üí Advanced ‚Üí Direct Model Chat

Ask: *"What are the top revenue products?"*

**LLM Response:**
```
"I don't have database access. Provide data or run query."
```

Baseline = No tooling.

Switch back to agent mode. Same question:

**Agent Response:**
```
Actual data from fitness_db via base_readQuery:
1. Treadmill Pro 3000 - $45,230
2. Elliptical Elite - $38,900
...
```

Comparison shows orchestration layer value:
- Tool access
- Context management
- Execution planning

**DYNAMIC CAPABILITY MANAGEMENT:**

Setup ‚Üí Capabilities ‚Üí Tools

```
‚úì base_databaseList (Enabled)
‚úì base_tableList (Enabled)
‚úì base_tableDDL (Enabled)
‚úì base_readQuery (Enabled)
```

Experiment: Disable `base_readQuery`

Toggle OFF ‚Üí Confirm

```
‚úó base_readQuery (Disabled)
```

Ask: *"Top 5 products by revenue?"*

**Live Status Panel:**
```
‚ùå ERROR: base_readQuery unavailable
üîÑ Replanning: Use base_tableList + TDA_LLMTask for schema insights
‚ö†Ô∏è Query fails or produces workaround
```

Re-enable ‚Üí Toggle ON

Same question ‚Üí ‚úÖ Succeeds instantly

**USE CASES:**
- Test agent without specific tools
- Restrict expensive tools (cost control)
- Disable problematic tools (debugging)
- Phase in new tools gradually

No restart. No code edits. Dynamic.

**PROMPT MANAGEMENT:**

Setup ‚Üí Capabilities ‚Üí Prompts

Disable `base_databaseBusinessDesc`

Ask: *"Business overview of fitness_db?"*

Agent falls back: `base_tableList` + `base_tableDDL` (manual investigation)

Re-enable ‚Üí Efficient pre-built workflow returns

**THE SHIFT:**

Most AI platforms: Proprietary secrets. Take what they give.

Uderia: Full keys.
- Edit system prompts
- Test raw models
- Enable/disable capabilities dynamically

Not just transparency. CONTROL.

Your agent. Your rules. Your customization.

Trust through visibility. Power through customization.
```

**Screen Capture Plan:**
- [ ] Setup ‚Üí Advanced ‚Üí System Prompts menu
- [ ] System Prompt Editor showing MASTER_SYSTEM_PROMPT
- [ ] Edit mode: Adding custom reasoning verbosity instructions
- [ ] Save changes
- [ ] Query execution showing enhanced tactical reasoning output
- [ ] Provider-specific prompts: GOOGLE_MASTER_SYSTEM_PROMPT example
- [ ] "Save to Overrides" and "Reset to Default" buttons
- [ ] Setup ‚Üí Advanced ‚Üí Direct Model Chat
- [ ] Direct chat query (no tools) ‚Üí Generic LLM response
- [ ] Switch back to agent mode ‚Üí Same query with actual data results
- [ ] Setup ‚Üí Capabilities ‚Üí Tools management
- [ ] Toggle base_readQuery to Disabled
- [ ] Query execution failing/workaround due to disabled tool
- [ ] Live Status Panel showing "base_readQuery unavailable" message
- [ ] Re-enable base_readQuery
- [ ] Query succeeds with proper tool
- [ ] Setup ‚Üí Capabilities ‚Üí Prompts management
- [ ] Disable base_databaseBusinessDesc prompt
- [ ] Query using fallback approach
- [ ] Re-enable prompt

**Supporting Materials:**
- System prompt customization guide
- Provider-specific optimization examples
- Direct Model Chat use cases
- Capability management best practices
- Tool filtering strategies
- Prompt override documentation

**Supporting Materials:**
- System prompt examples
- Customization best practices
- Developer mode guide

---

### 3.5 Execution Monitoring: Track Every Task
**Duration:** 2-3 minutes  
**Learning Objectives:**
- Monitor cross-source workload tracking
- Review execution logs and tool history
- Learn task control operations

**Key Topics to Cover:**
- [ ] Executions View dashboard
- [ ] Real-time task list (running, completed, failed)
- [ ] Detailed execution logs
- [ ] Tool invocation history with arguments
- [ ] Task control (cancel, retry)

**Narration Script:**
```
Transparency doesn't stop at current conversation.

Uderia: Complete view of ALL agent activity‚Äîpast, present, running.

Welcome to **Executions View** = Mission Control.

Main Navigation ‚Üí "Executions"

**EXECUTIONS DASHBOARD:**

Mission control for all agent tasks:
- UI conversations
- REST API calls
- Airflow DAGs
- Flowise workflows

Three sections:

**1. ACTIVE TASKS (Real-Time)**

Currently running:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Task: task_7f3a2b  Status: RUNNING  Progress: 65% ‚îÇ
‚îÇ Query: "Analyze quarterly revenue trends"         ‚îÇ
‚îÇ Session: Revenue Analysis Q4  Started: 2m ago     ‚îÇ
‚îÇ Phase: Data aggregation (Phase 2 of 3)            ‚îÇ
‚îÇ Tools: base_tableList, base_readQuery             ‚îÇ
‚îÇ [View Live Status] [Cancel]                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Click "View Live Status" ‚Üí Modal opens

Watch execution real-time (even if submitted via API, not UI):

```
- Strategic plan: Phase 1 ‚úì  Phase 2 ‚ñ∂  Phase 3 ‚è∏
- Current: base_readQuery (complex aggregation SQL)
- Tokens: 3,200 in / 1,800 out (live)
- Cost: $0.023 (estimated)
```

**2. COMPLETED TASKS (Historical Log)**

Execution history:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚úì Task: task_6a8d1c  COMPLETED  Duration: 12s ‚îÇ
‚îÇ Query: "Top 5 products by revenue?"           ‚îÇ
‚îÇ Session: Morning Batch  Finished: 5m ago      ‚îÇ
‚îÇ Tools: base_readQuery, TDA_FinalReport        ‚îÇ
‚îÇ Tokens: 1,250 in / 340 out  Cost: $0.008      ‚îÇ
‚îÇ [View Execution Log] [Replay Query]           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Click "View Execution Log" ‚Üí Forensic detail:

```
Task: task_6a8d1c
Query: "Top 5 products by revenue?"
Session: Morning Batch (abc-123)
Profile: @PROD (Claude Sonnet 3.5 + Teradata MCP)
Started: 09:15:23 UTC
Completed: 09:15:35 UTC
Duration: 12.4s

--- STRATEGIC PLAN ---
Phase 1: Revenue aggregation query
Phase 2: Final report formatting

--- EXECUTION TRACE ---

[09:15:23] Planning
  LLM: Strategic Planner (Claude Sonnet 3.5)
  Tokens: 1,100 in / 85 out
  Cost: $0.0035
  Result: 2-phase plan

[09:15:25] Phase 1
  Tool: base_readQuery
  SQL: SELECT p.ProductName, SUM(sd.Quantity * sd.UnitPrice) as Revenue
       FROM Products p JOIN SaleDetails sd ...
  Time: 3.2s
  Result: 5 rows
  Preview: [{"ProductName": "Treadmill Pro 3000", "Revenue": 45230}, ...]

[09:15:29] Phase 2
  Tool: TDA_FinalReport
  Format: table_with_summary
  LLM: Report Formatter
  Tokens: 150 in / 255 out
  Cost: $0.0045
  Result: Formatted HTML

[09:15:35] ‚úì Completed

TOTAL: 1,250 in / 340 out | $0.008
```

Forensic detail:
- Exact timing
- Every tool + arguments
- Every LLM call + tokens
- SQL executed
- Data returned
- Cost breakdown

Something wrong? Know exactly where and why.

**3. FAILED TASKS (Error Investigation)**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚úó Task: task_2c5e9f  FAILED  Duration: 8s    ‚îÇ
‚îÇ Query: "Compare sales across regions"         ‚îÇ
‚îÇ Session: Regional Analysis  Failed: 1h ago    ‚îÇ
‚îÇ Error: Table 'fitness_db.Regions' not found   ‚îÇ
‚îÇ [View Error Log] [Retry with Corrections]     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Click "View Error Log":

```
[10:42:15] Phase 1
  Tool: base_readQuery
  SQL: SELECT * FROM fitness_db.Regions ...
  ‚ùå ERROR: Table does not exist
  üîÑ Recovery: Replan using Customers.State
  ‚ùå Recovery FAILED: Insufficient data

[10:42:23] Task Failed
  Reason: Unable to complete with available tables
  Suggestion: Create Regions table OR use City/State from Customers
```

Agent tried self-correction. Recovery failed. You know exactly what to fix.

Click "Retry with Corrections" ‚Üí Modify query: *"Compare sales by state using customer location."*

**TASK CANCELLATION:**

Runaway query?

Active Tasks ‚Üí Find task ‚Üí "Cancel Task" ‚Üí Confirm

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚äó Task: task_9b3f1a  CANCELLED  Duration: 45s ‚îÇ
‚îÇ Query: "Full DB quality scan"                  ‚îÇ
‚îÇ Session: System Audit  Cancelled: Now         ‚îÇ
‚îÇ Reason: Manual cancellation                   ‚îÇ
‚îÇ Progress: 30% (Phase 1 of 4 complete)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Graceful shutdown. Resources freed. Partial results saved.

**CROSS-SOURCE VISIBILITY:**

ALL sources in one place:

```
- UI conversations
- REST API calls (Postman, cURL, apps)
- Airflow DAGs (batch jobs)
- Flowise workflows (chatbots)
```

Unified execution log.

**FILTER BY:**
- Session ID
- Profile  
- Status (running/completed/failed/cancelled)
- Date range
- User

**OPERATIONAL INTELLIGENCE:**

Not just debugging. Operational insights:

```
Sort by cost     ‚Üí Most expensive queries?
Filter by failed ‚Üí Which sessions failing most?
Filter by pattern‚Üí Avg execution time for revenue queries?
Aggregate usage  ‚Üí Which MCP tools used most?
```

Agent activity ‚Üí Operational data.

**THE POWER:**

Every task. Every tool. Every token. Every error.

Complete transparency across entire AI operations.

Production-grade monitoring. Trust through visibility. Control through insight.
```

**Screen Capture Plan:**
- [ ] Executions View navigation (main menu)
- [ ] Dashboard with three sections: Active, Completed, Failed
- [ ] Active task card showing progress bar and current phase
- [ ] Click "View Live Status" ‚Üí Modal with real-time execution
- [ ] Strategic plan showing Phase 1 ‚úì, Phase 2 Running, Phase 3 Pending
- [ ] Tool execution details (base_readQuery in progress)
- [ ] Token count and cost tracking (live updates)
- [ ] Completed tasks list with multiple entries
- [ ] Click "View Execution Log" on completed task
- [ ] Detailed execution trace with timestamps
- [ ] Tool invocations with full arguments (SQL queries visible)
- [ ] Token and cost breakdown per phase
- [ ] Failed task card with error message
- [ ] Click "View Error Log" ‚Üí Detailed failure analysis
- [ ] Recovery attempt details
- [ ] "Retry with Corrections" button
- [ ] Active task cancellation: Click "Cancel Task" ‚Üí Confirmation
- [ ] Task status changes to CANCELLED
- [ ] Filter options: Session, Profile, Status, Date range
- [ ] Cross-source task list: UI, API, Airflow labels
- [ ] Sort by cost (most expensive queries at top)

**Supporting Materials:**
- Execution monitoring guide
- Task lifecycle documentation
- Error investigation workflows
- Operational analytics examples
- Cross-source tracking reference
- Performance optimization guide

**Supporting Materials:**
- Execution monitoring guide
- Task lifecycle documentation
- Error debugging examples

---

## Module 4: EFFICIENT - The Fusion Optimizer
**Goal:** Showcase the revolutionary optimization engine and self-learning capabilities

### 4.1 Strategic & Tactical Planning Architecture
**Duration:** 4-5 minutes  
**Learning Objectives:**
- Understand multi-layered planning process
- See strategic meta-plans in action
- Grasp recursive delegation concept

**Key Topics to Cover:**
- [ ] Strategic Planner (high-level meta-plan)
- [ ] Tactical Execution (best next action)
- [ ] Recursive delegation for complex problems
- [ ] Phase-by-phase breakdown visualization
- [ ] Plan vs execution distinction

**Narration Script:**
```
Welcome to Uderia's heart‚Äîthe **Fusion Optimizer**.

LLM wrapper ‚Üí Enterprise AI orchestration engine. This is the transform.

**MOST AI TOOLS:**
```
User asks ‚Üí Model responds ‚Üí Repeat

No planning. No optimization. No intelligence in execution.
```

**FUSION OPTIMIZER:**
```
Strategic planning + Tactical execution + Continuous optimization

Multi-layered architecture. Enterprise-grade intelligence.
```

**SCENARIO: Q4 Board Meeting Prep**

CEO wants: Why revenue missed + What actions to take.

**Traditional setup requires:**
```
1. BI Analyst     ‚Üí Pull sales data
2. Data Engineer  ‚Üí Correlate service data
3. Business Analyst ‚Üí Synthesize insights
4. Executive      ‚Üí Frame recommendations

Multiple specialists. Days of coordination.
```

**Uderia:**
```
One intelligent workflow. All perspectives orchestrated.
```

Profile: @ANALYST (business knowledge + reporting patterns + visualization tools)

Ask: *"Analyze fitness equipment business: identify declining product categories, investigate service patterns, recommend improvements."*

Watch Live Status Panel.

**STRATEGIC PLANNING LAYER:**

Agent doesn't just execute. First: Generates meta-plan (strategic blueprint):

```
STRATEGIC META-PLAN:

Phase 1: Identify declining product categories
  Requires: Sales, SaleDetails tables
  Output: Categories with negative growth
  Dependencies: None (execute immediately)

Phase 2: Analyze service ticket patterns
  Requires: ServiceTickets + Phase 1 results
  Output: Ticket volume, resolution times, issues
  Dependencies: Phase 1

Phase 3: Correlate sales decline + service issues
  Requires: Phase 1 + Phase 2 results
  Output: Root cause analysis
  Dependencies: Phases 1, 2

Phase 4: Generate actionable recommendations
  Requires: All analysis
  Output: Prioritized strategies
  Dependencies: All phases
```

Strategic thinking. Entire workflow mapped BEFORE executing single tool.

**TACTICAL EXECUTION LAYER:**

Phase 1 execution begins.

Strategic planner ‚Üí Tactical executor: "Complete Phase 1"

Tactical layer asks: "Best next action?"

**Tactical Decision:**
```
Goal: Identify declining categories
Action: SQL query comparing Q4 vs Q3 sales by ProductType
Tool: base_readQuery
Rationale: Single query captures current + previous quarter,
           calculates growth %, identifies decline efficiently
```

**SQL Generated:**
```sql
SELECT p.ProductType,
       SUM(CASE WHEN QUARTER(s.SaleDate) = CURRENT_QUARTER 
           THEN sd.Quantity * sd.UnitPrice ELSE 0 END) as CurrentQ,
       SUM(CASE WHEN QUARTER(s.SaleDate) = CURRENT_QUARTER-1
           THEN sd.Quantity * sd.UnitPrice ELSE 0 END) as PrevQ,
       ((CurrentQ - PrevQ) / PrevQ * 100) as GrowthPct
FROM Products p
JOIN SaleDetails sd ON p.ProductID = sd.ProductID
JOIN Sales s ON sd.SaleID = s.SaleID
WHERE s.SaleDate >= '2024-07-01'
GROUP BY p.ProductType
ORDER BY GrowthPct ASC;
```

Tactical precision. Optimal tool + approach for this step.

**Results:**
```json
[
  {"ProductType": "Cardio", "CurrentQ": 125000, "PrevQ": 180000, "GrowthPct": -30.5},
  {"ProductType": "Accessories", "CurrentQ": 45000, "PrevQ": 52000, "GrowthPct": -13.5},
  {"ProductType": "Strength", "CurrentQ": 200000, "PrevQ": 190000, "GrowthPct": 5.3}
]
```

‚úÖ Phase 1 complete. Cardio: -30.5% | Accessories: -13.5%

**RECURSIVE DELEGATION:**

Phase 2: "Analyze service ticket patterns for declining products"

Complex phase requires:
```
1. Get tickets for Cardio + Accessories
2. Calculate avg resolution times
3. Categorize issue types
4. Identify patterns
```

Strategic planner doesn't handle monolithically.

Delegates to NEW subordinate planner instance.

**Recursive sub-plan appears:**
```
SUB-PLAN for Phase 2:
  2.1: Retrieve product IDs for Cardio and Accessories categories
  2.2: Query ServiceTickets for those product IDs
  2.3: Calculate resolution time statistics
  2.4: Extract and categorize issue descriptions
```

This sub-planner operates independently, executes its sub-phases, and returns consolidated 
results to the parent planner.

The parent doesn't micromanage. It delegated a complex sub-task to a specialized instance, 
which broke it down further and solved it.

**This is recursive intelligence.**

Sub-Plan 2.1 executes:
```
Tool: base_readQuery
SQL: SELECT ProductID FROM fitness_db.Products WHERE ProductType IN ('Cardio', 'Accessories')
Result: 47 product IDs identified
```

Sub-Plan 2.2 executes:
```
Tool: base_readQuery  
SQL: SELECT ProductID, COUNT(*) as TicketCount, 
            AVG(CAST(ResolutionDate - TicketDate AS INT)) as AvgResolutionDays,
            IssueDescription
     FROM fitness_db.ServiceTickets
     WHERE ProductID IN (list of 47 IDs)
     GROUP BY ProductID
Result: Service patterns retrieved
```

Sub-Plan 2.3 executes:
```
Tool: TDA_LLMTask
Task: "Analyze resolution time distribution and identify outliers"
Result: Cardio products average 8.3 days to resolve vs 2.7 for Accessories
```

Sub-Plan 2.4 executes:
```
Tool: TDA_LLMTask
Task: "Categorize issue descriptions into common themes"
Result: Top issues for Cardio: Motor failures (45%), Belt wear (30%), Electronic issues (25%)
```

Sub-plan complete. Results return to parent Phase 2. Phase 2 marks complete.

**HIERARCHICAL PROBLEM SOLVING:**

Phase 3 begins: "Correlate sales decline with service issues."

This phase uses results from BOTH Phase 1 and Phase 2. The strategic planner has those 
results in context.

Tactical execution:
```
Tool: TDA_LLMTask
Task: "Analyze correlation between 30% Cardio sales decline and 8.3-day average 
       service resolution time with 45% motor failure rate. Determine causation likelihood."
Result: Strong correlation. Customer reviews and repeat purchase rates show service 
        experience directly impacts sales. Motor failures driving negative sentiment.
```

Phase 3 complete.

Phase 4 begins: "Generate actionable recommendations."

```
Tool: TDA_FinalReport
Arguments: All prior phase results
Task: Synthesize insights into prioritized recommendations
Result: 
  1. URGENT: Address motor quality issues in Cardio line (root cause)
  2. HIGH: Reduce service resolution time target to <5 days (customer experience)
  3. MEDIUM: Implement proactive outreach for products >6 months old (prevention)
```

Final answer delivered to the user.

**WHAT HAPPENED:**

Complex question decomposed into:
```
‚úì 4-phase strategic meta-plan
‚úì Tactical tool selections per phase
‚úì Recursive sub-plan for Phase 2
‚úì Data gathering ‚Üí Analysis ‚Üí Correlation ‚Üí Synthesis
‚úì Actionable recommendations
```

All autonomous. All visible. All optimized.

**THE DISTINCTION:**

```
Chatbot: User asks ‚Üí Model responds
Uderia:  Strategic planning ‚Üí Tactical execution ‚Üí Recursive delegation
```

Enterprise AI orchestration engine.

**FUSION OPTIMIZER =**
```
Strategic planning + Tactical execution
Recursive delegation + Hierarchical problem-solving
```

**WHY IT MATTERS:**

Uderia: Enterprise complexity ‚úì
Other tools: Simple Q&A only

Intelligence in architecture. Not just model.
```

**Screen Capture Plan:**
- [ ] Complex multi-part query input
- [ ] Strategic Meta-Plan appears with 4 phases and dependencies mapped
- [ ] Phase 1 tactical execution: base_readQuery with sales comparison SQL
- [ ] Phase 1 results: Declining product categories identified
- [ ] Phase 2 begins: "Analyze service patterns" 
- [ ] RECURSIVE delegation indicator appears
- [ ] Sub-plan appears: 2.1, 2.2, 2.3, 2.4 sub-phases
- [ ] Sub-plan execution: Multiple base_readQuery and TDA_LLMTask calls
- [ ] Sub-plan results consolidated and returned to parent
- [ ] Phase 2 complete checkmark
- [ ] Phase 3 execution: Correlation analysis using Phase 1 + Phase 2 data
- [ ] Phase 4 execution: TDA_FinalReport with synthesized recommendations
- [ ] Final formatted report in Conversation Panel
- [ ] Execution summary: 4 phases, 8 tool calls, 1 recursive delegation

**Supporting Materials:**
- Fusion Optimizer architecture diagram
- Strategic vs Tactical planning comparison
- Recursive delegation examples
- Complex query patterns
- Phase dependency visualization

**Supporting Materials:**
- Fusion Optimizer architecture diagram
- Complex query examples
- Planning algorithm documentation

---

### 4.2 RAG Self-Learning System: Continuous Improvement
**Duration:** 4-5 minutes  
**Learning Objectives:**
- Understand automatic case capture
- Learn efficiency analysis and champion selection
- See few-shot learning injection in action

**Key Topics to Cover:**
- [ ] Automatic case archiving after successful sessions
- [ ] Efficiency scoring (token reduction, fast paths)
- [ ] Champion strategy selection
- [ ] Few-shot learning injection at planning time
- [ ] Asynchronous processing (zero latency)

**Narration Script:**
```
Fusion Optimizer: Powerful.

What makes it revolutionary: **Learns from every success.**

Welcome to self-improving RAG system.

**MOST AI AGENTS:**
```
Execute in isolation
Don't remember what worked
Don't learn from success
Reinvent wheel every time
```

**UDERIA RAG:**
```
Every successful interaction ‚Üí Organizational knowledge
Agent gets smarter with every query
```

**AUTOMATIC CASE CAPTURE:**

Intelligence ‚Üí RAG Collections

"Planner Repository" = Knowledge base of successful execution patterns

Every completed session:
- Analyzed automatically
- Archived as reusable pattern

Click case: "Revenue Analysis - Top Products by Category"

```
CASE: Revenue Analysis - Top Products by Category

Query: "Top 5 products by revenue in each category?"

Strategic Plan:
  Phase 1: Revenue aggregation with category grouping
  Phase 2: Format results

Tool Trace:
  1. base_readQuery (1.8s)
     SQL: SELECT ProductType, ProductName, Brand,
                 SUM(Quantity * UnitPrice) as Revenue,
                 RANK() OVER (PARTITION BY ProductType 
                              ORDER BY Revenue DESC) as Rank
          FROM Products p
          JOIN SaleDetails sd ON p.ProductID = sd.ProductID
          WHERE Rank <= 5
          GROUP BY ProductType, ProductName, Brand
     Result: 15 rows (5 per category)
  
  2. TDA_FinalReport
     Format: Grouped table
     Result: Clean output

Tokens:
  Planning: 1,200 in / 95 out
  Execution: 850 in / 120 out
  Total: 2,265

Cost: $0.0112

Efficiency Score: 94/100
  ‚úì Optimal tool selection
  ‚úì Minimal tokens
  ‚úì Single-pass
  ‚úì No errors/retries

Champion Case: YES üèÜ (lowest tokens for pattern)
```

Entire workflow ‚Üí Reusable pattern.

**EFFICIENCY ANALYSIS:**

Score: 94/100. RAG analyzes every case:

```
Token Efficiency:    Minimal tokens?
Execution Efficiency: Single-pass?
Tool Selection:      Optimal choices?
Strategic Planning:  Direct path?
```

Cases ranked. Most efficient = **Champion Case üèÜ**

List shows champion badges.

**CHAMPION STRATEGY SELECTION:**

Let's say we have three cases for "top revenue queries":

1. Revenue by Product (Champion üèÜ) - 2,265 tokens, 1.8s, score 94
2. Revenue by Category - 3,100 tokens, 3.2s, score 78  
3. Revenue with Customer Analysis - 5,800 tokens, 7.5s, score 65

Case 1 is the champion‚Äîmost efficient for straightforward revenue queries.

When a new similar query comes in, the RAG system will inject Case 1 as a few-shot 
example to guide the strategic planner.

**FEW-SHOT LEARNING INJECTION:**

New similar query: *"Show me top revenue products by brand this month."*

Live Status Panel ‚Üí Strategic planning shows:

**üîç RAG-RETRIEVED CONTEXT:**

```
Similar Past Cases:

1. Revenue Analysis - Top Products by Category (similarity: 0.89)
   Strategy: Single aggregation + grouping
   Tools: base_readQuery ‚Üí TDA_FinalReport
   Tokens: 2,265 | Cost: $0.0112 | Duration: 1.8s
   Key: RANK() window function for top-N per group

2. Monthly Revenue Trends (similarity: 0.76)
   Strategy: Time-based filtering + SUM
   Tools: base_readQuery ‚Üí TDA_FinalReport
   Tokens: 1,980 | Cost: $0.0095 | Duration: 1.5s
   Key: EXTRACT(MONTH) for current month filter
```

Strategic planner learns from champions:
```
Case 1 ‚Üí Use RANK() for top-N per group
Case 2 ‚Üí Use EXTRACT(MONTH) for monthly filter
Combine both patterns
```

Strategic plan generated:
```
Phase 1: Execute revenue query by brand for current month using optimized SQL pattern
  Tool: base_readQuery
  SQL: SELECT p.Brand, p.ProductName,
              SUM(sd.Quantity * sd.UnitPrice) as Revenue,
              RANK() OVER (PARTITION BY p.Brand ORDER BY Revenue DESC) as Rank
       FROM fitness_db.Products p
       JOIN fitness_db.SaleDetails sd ON p.ProductID = sd.ProductID  
       JOIN fitness_db.Sales s ON sd.SaleID = s.SaleID
       WHERE EXTRACT(MONTH FROM s.SaleDate) = EXTRACT(MONTH FROM CURRENT_DATE)
         AND Rank <= 5
       GROUP BY p.Brand, p.ProductName

Phase 2: Format results
  Tool: TDA_FinalReport
```

Look at that‚Äîthe agent learned the RANK() pattern from past success. It didn't waste 
time exploring alternative approaches. It went straight to the proven champion strategy.

Execution completes.

Token usage: 2,180 total (85 fewer than the first time we solved this pattern)
Cost: $0.0106 (saved $0.0006)
Duration: 1.6s (0.2s faster)

**CONTINUOUS IMPROVEMENT:**

New execution analyzed:
```
Succeeded?              YES
More efficient?         YES (fewer tokens, faster)
Become new champion?    YES
```

RAG auto-promotes to champion.

Next similar query ‚Üí Benefits from even-more-optimized approach.

**THE EVOLUTION:**
```
Agent learning: Continuous
Getting faster: Automatic
Getting cheaper: Automatic
```

**ASYNCHRONOUS PROCESSING:**

Does RAG slow responses?

**No. Watch timeline:**

```
[00:00.000] User submits query
[00:00.050] RAG retrieval starts (parallel)
[00:00.100] Strategic planner invoked
[00:00.350] RAG context injected ‚úì
[00:00.400] Strategic plan finalized
[00:00.450] Tactical execution begins
[00:02.100] Results ‚Üí User
------- USER HAS ANSWER -------
[00:02.150] Async: Case archiving (background)
[00:03.500] Async: Efficiency analysis (background)
[00:03.600] Async: Case stored (background)
```

RAG retrieval: During planning (<300ms)
Case archiving: After user gets answer

Zero user-facing latency.

**PER-USER COST SAVINGS ATTRIBUTION:**

Go to Intelligence ‚Üí Efficiency Metrics.

You'll see a dashboard:

```
YOUR RAG EFFICIENCY GAINS:

Total Queries: 247
RAG-Guided Queries: 183 (74%)

Token Savings:
  Without RAG (projected): 892,000 tokens
  With RAG (actual): 645,000 tokens  
  Savings: 247,000 tokens (28% reduction)

Cost Savings:
  Projected cost: $52.30
  Actual cost: $37.60
  Savings: $14.70 (28% reduction)

Time Savings:
  Average execution time without RAG: 4.2s
  Average execution time with RAG: 2.8s
  Per-query savings: 1.4s (33% faster)
```

The RAG system isn't just making the agent smarter‚Äîit's making it dramatically more 
cost-effective. And these savings compound as your repository grows.

**KNOWLEDGE TRANSFER & PERSONA-BOUND LEARNING:**

Here's the organizational benefit‚Äîand it gets even more powerful with profiles.

When you use the @DBA profile to optimize a complex join query, that champion case 
gets stored in the DBA persona's planner repository. The next time ANYONE switches to 
@DBA, they inherit that optimized pattern. Your database expertise becomes the DBA 
persona's expertise.

When your colleague uses @ANALYST to create an executive dashboard, that visualization 
pattern gets captured. Now the @ANALYST persona has learned a new reporting style. 
Every user of that profile benefits.

When the quality team uses @QUALITY to validate data, those validation workflows become 
part of the Quality persona's playbook.

This is collective intelligence at work‚Äînot just across users, but across specialized 
personas. Each profile evolves independently:
- @DBA gets better at SQL optimization
- @ANALYST gets better at business insights  
- @QUALITY gets better at data validation
- @LOCAL maintains same patterns with sovereign execution

You're not just improving one agent. You're building a library of specialized expert 
personas, each learning from every user's successes in their domain.

Switch profiles. Switch to the persona that learned from your team's best work. 
Orchestrate expertise that compounds over time.

**CROSS-TIER KNOWLEDGE TRANSFER: Train on Premium, Execute on Budget**

Here's the ultimate cost optimization strategy: use expensive intelligence to teach, 
then use cheap execution to repeat.

Let's demonstrate with a complex analytical challenge that would normally require a 
premium model.

**Phase 1: Learning with @OPUS (Claude Opus - Premium)**

Switch to @OPUS profile. This is the most expensive, most capable model‚Äî$15 per 1M 
input tokens. But we're going to use it strategically.

Ask a genuinely complex question:

"Analyze the relationship between customer demographics (age, location), product purchase 
patterns, service ticket frequency, and revenue contribution to identify high-value 
customer segments and predict churn risk."

This requires sophisticated reasoning:
- Multi-table joins (Customers, Sales, SaleDetails, Products, ServiceTickets)
- Statistical analysis (correlation, clustering)
- Pattern recognition (churn indicators)
- Segmentation logic (defining high-value cohorts)

Watch @OPUS work. The Live Status Panel shows:

```
Profile: @OPUS (Claude Opus)
Cost Tier: Premium ($15/$75 per 1M tokens)

Strategic Plan (6 phases):
  Phase 1: Aggregate customer purchase history
  Phase 2: Join service ticket patterns
  Phase 3: Calculate customer lifetime value
  Phase 4: Perform demographic clustering
  Phase 5: Identify churn risk indicators
  Phase 6: Generate segmentation report

Tool Executions:
  - base_readQuery (3 complex multi-table joins)
  - TDA_LLMTask (statistical analysis)
  - TDA_LLMTask (clustering algorithm)
  - TDA_FinalReport (executive summary)

Execution:
  Tokens: 12,400 (input) + 2,300 (output) = 14,700 total
  Cost: $0.3585
  Duration: 8.7s
  Result: HIGH QUALITY - Sophisticated segmentation with 5 distinct customer cohorts
```

Expensive? Yes. But brilliant. Opus identified patterns a simpler model would miss.

**Champion Case Captured:**

The RAG system analyzes this execution:

```
Case: "Customer Segmentation with Churn Prediction"
Profile: @OPUS
Efficiency Score: 91/100
  - Complex reasoning: ‚úì (required premium model)
  - Optimal tool selection: ‚úì
  - Minimal retries: ‚úì
  - High-quality output: ‚úì

Champion Case: YES
Pattern Captured:
  - Strategic approach: Multi-phase with clustering
  - Tool sequence: Aggregation ‚Üí Join ‚Üí Calculate ‚Üí Analyze ‚Üí Report
  - SQL patterns: Complex CTEs with window functions
  - Analysis techniques: Demographic clustering, churn indicators

Stored in: DBA_Champion_Cases + ANALYST_Champion_Cases
(This pattern is valuable for both database optimization and business analysis)
```

We just paid $0.36 to solve this problem brilliantly. But that investment is about to 
pay massive dividends.

**Phase 2: Execution with @COST (Gemini Flash - Budget)**

Now switch to @COST profile. This is the cheapest model available‚Äî$0.075 per 1M input 
tokens. That's 200x cheaper than Opus for input, 125x cheaper for output.

Ask a similar question (not identical, but same pattern):

"Identify high-value customer segments for our Strength equipment line and predict 
which segments are at churn risk."

Watch the magic happen. The Live Status Panel shows:

```
Profile: @COST (Gemini 1.5 Flash)
Cost Tier: Budget ($0.075/$0.30 per 1M tokens)

üîç RAG-RETRIEVED CONTEXT:

Champion Case Found: "Customer Segmentation with Churn Prediction"
  - Similarity: 0.94 (excellent match)
  - Source Profile: @OPUS
  - Original Cost: $0.3585
  - Strategy: Multi-phase clustering with churn analysis
  - Tools: base_readQuery (aggregations) ‚Üí TDA_LLMTask (clustering) ‚Üí TDA_FinalReport
  - SQL Pattern: CTEs with window functions for lifetime value calculation
  - Analysis Pattern: Demographic clustering ‚Üí Service pattern correlation ‚Üí Churn scoring

Applying learned strategy...

Strategic Plan (6 phases - INHERITED FROM OPUS):
  Phase 1: Aggregate customer purchase history (Strength products filter)
  Phase 2: Join service ticket patterns
  Phase 3: Calculate customer lifetime value
  Phase 4: Perform demographic clustering
  Phase 5: Identify churn risk indicators
  Phase 6: Generate segmentation report

Tool Executions (SAME SEQUENCE AS OPUS):
  - base_readQuery (using Opus-optimized SQL patterns)
  - TDA_LLMTask (following Opus analysis structure)
  - TDA_LLMTask (applying Opus clustering approach)
  - TDA_FinalReport (matching Opus report format)

Execution:
  Tokens: 8,200 (input) + 1,850 (output) = 10,050 total
  Cost: $0.0123
  Duration: 5.1s
  Result: HIGH QUALITY - Sophisticated segmentation with 4 distinct customer cohorts
```

Look at what just happened:

**Cost Comparison:**
- @OPUS: $0.3585 (to learn the pattern)
- @COST: $0.0123 (to execute the learned pattern)
- **Savings: $0.3462 per query (96.5% cost reduction)**

**Quality Comparison:**
- @OPUS: 5 customer cohorts, sophisticated clustering
- @COST: 4 customer cohorts, nearly identical insights
- **Quality preservation: ~95% of Opus quality at 3.5% of Opus cost**

**The ROI Calculation:**

If you run this type of analysis monthly:
- Without RAG: 12 queries √ó $0.3585 = $4.30/year (all @OPUS)
- With RAG Transfer: 1 query √ó $0.3585 + 11 queries √ó $0.0123 = $0.4938/year
- **Annual Savings: $3.80 (88.5% cost reduction)**

If you run it weekly:
- Without RAG: 52 queries √ó $0.3585 = $18.64/year
- With RAG Transfer: 1 query √ó $0.3585 + 51 queries √ó $0.0123 = $0.9858/year  
- **Annual Savings: $17.65 (94.7% cost reduction)**

**The Strategy:**

1. Use premium models (@OPUS, @SONNET) to solve complex problems ONCE
2. RAG captures the champion strategy automatically
3. Use budget models (@COST, @GEMINI_FLASH) for subsequent similar queries
4. Budget models inherit premium intelligence via few-shot learning
5. Pay premium prices for intelligence, budget prices for execution

**Cross-Profile Learning at Scale:**

This works across ANY profile combination:

- **@OPUS ‚Üí @ANALYST:** Learn complex business analysis, execute cheaply for routine reports
- **@SONNET ‚Üí @DBA:** Learn SQL optimization, execute with local Ollama for free
- **@OPUS ‚Üí @QUALITY:** Learn sophisticated anomaly detection, execute routinely at low cost
- **Premium Cloud ‚Üí @LOCAL:** Learn in cloud, execute sovereignly on-premises

The expensive model teaches. The cheap model learns. Your organization gets premium 
intelligence at budget prices.

**Organizational Knowledge Compounding:**

As your team uses premium profiles occasionally for hard problems, the RAG repositories 
fill with high-quality champion cases. Over time, your budget profiles become incredibly 
capable‚Äînot because the underlying LLM improved, but because they're standing on the 
shoulders of premium model intelligence.

This is cross-tier knowledge transfer. This is how you optimize costs without sacrificing 
quality.

Train with genius. Execute with efficiency. Compound organizational intelligence.

**THE TRANSFORMATION:**

```
Individual success      ‚Üí Organizational knowledge
One-time execution      ‚Üí Reusable patterns
Static AI               ‚Üí Self-improving intelligence

Train with genius       ‚Üí Execute with efficiency
Premium model teaches   ‚Üí Budget model learns
Expensive intelligence  ‚Üí Cheap execution
```

**RAG-POWERED FUSION OPTIMIZER:**

Agent that gets smarter every time you use it.

Continuous improvement. Organizational learning. Cross-tier knowledge transfer.

This is intelligence that compounds.
```

**Screen Capture Plan:**
- [ ] Intelligence ‚Üí RAG Collections navigation
- [ ] Planner Repository view with list of cases
- [ ] Case detail: "Revenue Analysis - Top Products by Category"
- [ ] Strategic plan display
- [ ] Tool execution trace with SQL queries
- [ ] Token usage breakdown (planning + execution)
- [ ] Efficiency Score: 94/100 with breakdown
- [ ] Champion badge (üèÜ) indicator
- [ ] List view showing three cases with champion ranking
- [ ] New query: "Top revenue products by brand this month"
- [ ] Live Status Panel: "RAG-RETRIEVED CONTEXT" section appears
- [ ] Similar cases displayed with similarity scores
- [ ] Strategic plan using learned patterns (RANK + EXTRACT)
- [ ] Execution completes with improved metrics
- [ ] New case promoted to champion status
- [ ] Timeline visualization: RAG retrieval parallel to planning
- [ ] Intelligence ‚Üí Efficiency Metrics dashboard
- [ ] Token savings: 247,000 saved (28% reduction)
- [ ] Cost savings: $14.70 saved
- [ ] Time savings: 1.4s per query faster
- [ ] **CROSS-TIER DEMO:** Profile switch to @OPUS
- [ ] Complex segmentation query execution with @OPUS
- [ ] Token usage: 14,700 tokens, Cost: $0.3585
- [ ] Champion case capture with "Source Profile: @OPUS" tag
- [ ] Profile switch to @COST (Gemini Flash)
- [ ] Similar segmentation query
- [ ] Live Status: RAG retrieval shows @OPUS champion case
- [ ] "Applying learned strategy from @OPUS..."
- [ ] Execution with @COST: 10,050 tokens, Cost: $0.0123
- [ ] Side-by-side comparison: $0.3585 (Opus) vs $0.0123 (Flash)
- [ ] Savings calculation: 96.5% cost reduction
- [ ] ROI calculator: Annual savings projection
- [ ] Cross-profile learning matrix showing all combinations

**Supporting Materials:**
- RAG system architecture documentation
- Efficiency scoring methodology
- Champion case selection algorithm
- Few-shot learning examples
- Asynchronous processing flow diagram
- Cost savings calculator
- Cross-tier learning transfer diagram
- Premium vs Budget model pricing comparison
- ROI calculation methodology

---

### 4.3 Proactive Optimization: Fast Paths & Hydration
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Learn plan hydration mechanism
- Understand tactical fast path
- See specialized orchestrators in action

**Key Topics to Cover:**
- [ ] Plan hydration (reusing previous turn data)
- [ ] Tactical fast path (skip LLM for simple operations)
- [ ] Specialized orchestrators (date range, comparative analysis)
- [ ] Context distillation for large datasets
- [ ] Optimization impact on speed and cost

**Narration Script:**
```
Fusion Optimizer: Plans + Learns + **Hunts for optimization opportunities**

Proactive optimization. Before execution. During execution.

Make Uderia fast. Make Uderia cost-effective.

**TECHNIQUE 1: PLAN HYDRATION**

Query: *"Top 5 products by revenue this month?"*

Execution:
```
Strategic Plan: Single phase
Tool: base_readQuery
Result: 5 products + revenue
Tokens: 1,800
Duration: 2.1s
```

Results displayed.

Immediate follow-up: *"What about the top 10?"*

Live Status Panel ‚Üí Watch carefully:

```
üîß PLAN HYDRATION DETECTED

New: "Top 10?"
Analysis: Expanded version of previous query

Previous results cached:
  Query: Top 5 products by revenue
  Tool: base_readQuery
  Results: 5 products
  Age: 3s (fresh)

Strategy: HYDRATE with cached data
  ‚úì Skip Phase 1 planning (reuse strategy)
  ‚úì Skip redundant call (modify LIMIT 5 ‚Üí 10)
  ‚úì Execute optimized query

Savings: ~900 tokens, ~$0.005, ~1.2s
```

Agent recognizes iterative refinement. Reuses context vs starting over.

**Result:**
```
Tokens: 950 (850 fewer)
Duration: 0.9s (1.2s faster)
Cost: $0.0045 ($0.005 saved)
```

**Common triggers:**
- "What about X instead?" (parameter change)
- "Show me more" (pagination)
- "Add Y" (incremental)
- "Break down by Z" (granularity)

**TECHNIQUE 2: TACTICAL FAST PATH**

Query: *"What's the current date?"*

Live Status Panel:

No strategic planning. No LLM call.

```
‚ö° TACTICAL FAST PATH

Query: "Current date?"
Complexity: TRIVIAL (single deterministic tool)

Strategy: BYPASS strategic planning
  ‚úó Skip planner LLM
  ‚úì Direct tool invocation
  Tool: TDA_CurrentDate
  Args: {}

Savings: ~1,500 tokens, ~$0.008, ~0.8s
```

Instant recognition: Trivial query. Single tool. No args. No planning.

**Direct execution:**
```
Tool: TDA_CurrentDate
Result: "2025-12-07"
Time: 0.2s
```

No strategic plan. No tactical LLM. Pure efficiency.

**Triggers:**
- Single deterministic tool
- Arguments inferrable
- No multi-step logic

Eliminate ~1,500 tokens per fast-path query. Massive savings for conversational UIs.

**TECHNIQUE 3: SPECIALIZED ORCHESTRATORS**

Query: *"Show daily sales for past week."*

The agent detects a pattern:

```
üéØ OPTIMIZATION: DATE RANGE ORCHESTRATOR

Query: "daily sales for the past week"
Pattern detected: Time-series iteration over date range

Standard approach:
  - Plan each day as separate phase (7 phases)
  - Execute 7 individual queries
  - Tokens: ~12,000 | Duration: ~15s

Optimized approach:
  - Specialized Date Range Orchestrator
  - Single multi-day aggregation query
  - Post-processing for daily breakdown
  - Tokens: ~2,800 | Duration: ~3.5s

Applying optimization...
```

The orchestrator generates:
```sql
SELECT CAST(SaleDate AS DATE) as Day,
       SUM(TotalAmount) as DailySales
FROM fitness_db.Sales
WHERE SaleDate >= CURRENT_DATE - 7
GROUP BY CAST(SaleDate AS DATE)
ORDER BY Day
```

One query. Seven days. Clean results.

Tokens: 2,800 (9,200 saved vs naive approach)
Duration: 3.2s (11.8s saved)

Other specialized orchestrators:

**Comparative Analysis Orchestrator:**
Query: "Compare Claude vs GPT-4 on this question: [complex query]"

Detects: Multi-model comparison pattern
Optimization: Executes query on both models in parallel, aggregates results
Savings: ~40% faster than sequential execution

**Multi-Table Join Orchestrator:**  
Query: "Join customers, sales, and products for full order analysis"

Detects: Complex multi-table join pattern
Optimization: Single optimized JOIN query instead of multiple tool calls
Savings: ~60% token reduction

**TECHNIQUE 4: CONTEXT DISTILLATION**

Ask: "List all products in the database."

The query returns 500 products. That's a lot of data.

Now ask a follow-up: "Which of those are Cardio equipment?"

Normally, the agent would send all 500 product records back to the LLM for analysis. 
That's ~50,000 tokens of context.

Watch the optimization:

```
üíß OPTIMIZATION: CONTEXT DISTILLATION

Previous tool result: 500 products (48,000 tokens raw)
New query requires: Subset filtering by ProductType='Cardio'

Strategy: DISTILL large dataset before LLM processing
  - Apply deterministic filter: ProductType='Cardio'
  - Reduced dataset: 87 products
  - Context tokens: 48,000 ‚Üí 4,200 (91% reduction)

Passing distilled context to LLM for response generation...
```

The agent performed a deterministic pre-filter, dramatically reducing context size 
before involving the LLM.

Tokens used: 5,800 total (42,200 saved)
Cost: $0.028 ($2.10 saved on this single query)

Context distillation applies when:
- Large tool results (>10,000 tokens)
- Follow-up query requires subset or aggregation
- Deterministic filtering possible without LLM reasoning

**TECHNIQUE 5: MULTI-TOOL PARALLEL EXECUTION**

Ask: "Give me the database version, table list, and current system date."

The agent detects three independent requests:

```
‚ö° OPTIMIZATION: PARALLEL TOOL EXECUTION

Query decomposition:
  1. Database version ‚Üí dba_databaseVersion (independent)
  2. Table list ‚Üí base_tableList (independent)
  3. Current date ‚Üí TDA_CurrentDate (independent)

No dependencies detected. Executing in parallel...

Parallel execution:
  [Thread 1] dba_databaseVersion ‚Üí 0.8s
  [Thread 2] base_tableList ‚Üí 1.2s
  [Thread 3] TDA_CurrentDate ‚Üí 0.1s
  
  Total duration: 1.2s (longest thread)
  
Sequential execution would take: 2.1s
Time saved: 0.9s (43% faster)
```

When multiple tools have no data dependencies, the Fusion Optimizer executes them 
concurrently.

**OPTIMIZATION IMPACT:**

Intelligence ‚Üí Optimization Metrics (30 days)

```
Plan Hydration:
  Applied: 89x | Tokens: 78K saved | Cost: $4.20 saved

Tactical Fast Path:
  Applied: 134x | Tokens: 195K saved | Cost: $10.40 saved

Specialized Orchestrators:
  Applied: 23x | Tokens: 142K saved | Cost: $7.60 saved

Context Distillation:
  Applied: 12x | Tokens: 380K saved | Cost: $18.90 saved

Parallel Execution:
  Applied: 34x | Time: 41s saved

--- TOTAL IMPACT ---
Tokens: 795,000 saved (32% of usage)
Cost: $41.10 saved (34% of spend)
Time: 556s saved (9.3 minutes)
```

Automatic. Transparent. Continuous.

No configuration. No enabling. Built into Fusion Optimizer core.

**THE SHIFT:**
```
Dollars  ‚Üí Cents
Slow     ‚Üí Fast
Wasteful ‚Üí Efficient
```

Proactive optimization. Works smarter, not harder.
```

**Screen Capture Plan:**
- [ ] Query: "Top 5 products by revenue"
- [ ] Results displayed with token/cost metrics
- [ ] Follow-up: "What about the top 10?"
- [ ] Live Status Panel: "üîß PLAN HYDRATION DETECTED" message
- [ ] Optimization strategy explanation displayed
- [ ] Hydrated execution with reduced tokens (950 vs 1,800)
- [ ] Query: "What's the current date?"
- [ ] Live Status Panel: "‚ö° TACTICAL FAST PATH" message
- [ ] Direct tool execution (no strategic planning)
- [ ] Result in 0.2s with token comparison
- [ ] Query: "Show daily sales for the past week"
- [ ] Live Status Panel: "üéØ DATE RANGE ORCHESTRATOR" detected
- [ ] Optimization comparison: 12,000 tokens naive vs 2,800 optimized
- [ ] Single aggregated SQL query executed
- [ ] Query: "List all products" ‚Üí 500 results
- [ ] Follow-up: "Which are Cardio equipment?"
- [ ] Live Status Panel: "üíß CONTEXT DISTILLATION" message
- [ ] Context reduction: 48,000 ‚Üí 4,200 tokens (91% reduction)
- [ ] Query: "Give me version, tables, and date"
- [ ] Live Status Panel: "‚ö° PARALLEL TOOL EXECUTION" message
- [ ] Three threads executing simultaneously
- [ ] Duration comparison: 1.2s parallel vs 2.1s sequential
- [ ] Intelligence ‚Üí Optimization Metrics dashboard
- [ ] 30-day summary with savings breakdown
- [ ] Total impact: 795,000 tokens saved, $41.10 saved

**Supporting Materials:**
- Optimization techniques documentation
- Plan hydration pattern recognition guide
- Fast path eligibility criteria
- Orchestrator catalog
- Context distillation algorithms
- Parallel execution dependency analyzer
- Performance benchmarks

**Supporting Materials:**
- Optimization techniques documentation
- Performance benchmarks
- Orchestrator examples

---

### 4.4 Cost Tracking: See Every Token, Every Cent
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Monitor per-turn token usage
- Understand cost-to-token mapping
- Learn cost optimization strategies

**Key Topics to Cover:**
- [ ] Per-turn token breakdown (input/output)
- [ ] Provider-specific pricing
- [ ] Historical token trends
- [ ] Optimization insights
- [ ] RAG efficiency savings attribution

**Narration Script:**
```
Optimizations powerful. But how know they're saving money?

Uderia: **Complete cost transparency. Every level.**

Token usage: Mystery ‚Üí Actionable data.

**TURN-LEVEL COST VISIBILITY:**

Session History panel. Any turn.

Below each query-response:

```
Turn 3: "Top revenue products?"

üìä TOKENS:
  Input:  1,850
  Output:   340
  Total:  2,190

üí∞ COST:
  Model: Claude Sonnet 3.5
  Input:  $0.0055 (1,850 √ó $0.003/1K)
  Output: $0.0051 (340 √ó $0.015/1K)
  Total:  $0.0106
  
‚ö° EFFICIENCY:
  RAG-guided: YES
  Optimization: Plan hydration
  Baseline: 3,200 tokens
  Saved: 1,010 tokens (32%)
  Cost saved: $0.0034
```

Exact consumption. Exact cost. Not estimates. Actual.

**PROVIDER PRICING:**

Uderia tracks all models accurately:

```
Claude (Anthropic):
  Opus:      $15/$75 per 1M
  Sonnet 3.5: $3/$15 per 1M
  Haiku:     $0.25/$1.25 per 1M

GPT-4 (OpenAI):
  GPT-4o:      $2.50/$10 per 1M
  GPT-4o-mini: $0.15/$0.60 per 1M

Gemini (Google):
  1.5 Pro (‚â§128K): $1.25/$5 per 1M
  1.5 Pro (>128K): $2.50/$10 per 1M
  1.5 Flash:      $0.075/$0.30 per 1M

AWS Bedrock:
  Various models + inference profiles
  Custom pricing tracked

Ollama (Local):
  $0.00 (local execution)
```

Every interaction: Actual model + Actual pricing tier.

**SESSION-LEVEL COST SUMMARY:**

At the top of every session, you see cumulative costs:

```
üìä SESSION SUMMARY: "Q4 Revenue Analysis"

Turns completed: 12
Total tokens: 28,450 (input: 19,200 | output: 9,250)
Total cost: $0.2134

Average per turn:
  Tokens: 2,371
  Cost: $0.0178

Most expensive turn: Turn 7 - "Complex multi-table analysis" ($0.0456)
Most efficient turn: Turn 3 - "Simple product lookup" ($0.0032)

Optimization impact:
  Baseline (no optimization): $0.3287
  Actual (with optimization): $0.2134
  Savings: $0.1153 (35%)
```

You can see exactly where costs are concentrated and where optimizations delivered value.

**HISTORICAL TOKEN TRENDS:**

Go to Intelligence ‚Üí Token Analytics.

You'll see charts showing usage over time:

**30-Day Token Usage:**
```
    Tokens (thousands)
    ‚Üë
50  ‚îÇ                                    ‚óè
40  ‚îÇ                    ‚óè           ‚óè
30  ‚îÇ          ‚óè    ‚óè         ‚óè
20  ‚îÇ    ‚óè                        ‚óè
10  ‚îÇ‚óè
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí
     Day 1    5    10   15   20   25   30
```

You can spot trends:
- Usage spike on Day 23 (what happened?)
- Declining baseline after Day 15 (optimizations kicking in?)
- Consistent daily average ~25K tokens

**Cost Trends:**
```
    Cost ($)
    ‚Üë
2.0 ‚îÇ                                    ‚óè
1.5 ‚îÇ                    ‚óè           ‚óè
1.0 ‚îÇ          ‚óè    ‚óè         ‚óè
0.5 ‚îÇ    ‚óè                        ‚óè
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí
     Day 1    5    10   15   20   25   30

Total 30-day spend: $34.20
Average daily: $1.14
Projected monthly: $35.04
```

This helps with budgeting and cost forecasting.

**PROVIDER COST COMPARISON:**

Now here's where it gets strategic. Go to Intelligence ‚Üí Provider Comparison.

You can run the same workload on different models and see actual cost differences:

```
WORKLOAD: "Standard Revenue Analysis" (10 typical queries)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Provider/Model          ‚îÇ Tokens   ‚îÇ Cost    ‚îÇ Duration ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Claude Opus             ‚îÇ 24,500   ‚îÇ $0.487  ‚îÇ 8.2s     ‚îÇ
‚îÇ Claude Sonnet 3.5       ‚îÇ 24,500   ‚îÇ $0.097  ‚îÇ 7.8s     ‚îÇ
‚îÇ GPT-4o                  ‚îÇ 26,100   ‚îÇ $0.131  ‚îÇ 9.1s     ‚îÇ
‚îÇ Gemini 1.5 Pro          ‚îÇ 25,800   ‚îÇ $0.077  ‚îÇ 6.9s     ‚îÇ
‚îÇ Gemini 1.5 Flash        ‚îÇ 25,800   ‚îÇ $0.010  ‚îÇ 4.2s     ‚îÇ
‚îÇ Ollama (Llama 3.1 70B)  ‚îÇ 28,200   ‚îÇ $0.000  ‚îÇ 12.3s    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Insights:
- Gemini Flash: Lowest cost, fastest (4.2s), great for batch jobs
- Claude Sonnet 3.5: Best balance of cost ($0.097) and quality
- Claude Opus: 5x more expensive than Sonnet for similar results
- Ollama: Zero cost but slowest (acceptable for non-urgent queries)

Recommendation: Use Gemini Flash for routine queries (@COST profile),
                Claude Sonnet for complex analysis (@PROD profile),
                Ollama for sensitive data (@LOCAL profile)
```

This data-driven comparison helps you optimize your profile strategy.

**RAG EFFICIENCY SAVINGS:**

Back to Intelligence ‚Üí RAG Efficiency.

The RAG system tracks how much it saves you:

```
RAG COST IMPACT ANALYSIS:

Total queries: 247
RAG-guided queries: 183 (74%)

Without RAG (projected baseline):
  Average tokens per query: 3,612
  Total tokens: 892,164
  Total cost: $52.30

With RAG (actual usage):
  Average tokens per query: 2,611  
  Total tokens: 645,013
  Total cost: $37.60

RAG SAVINGS:
  Tokens saved: 247,151 (28%)
  Cost saved: $14.70 (28%)
  
Cost per query:
  Without RAG: $0.212
  With RAG: $0.152
  Savings per query: $0.060

Projected annual savings (at current usage): $876
```

The RAG system pays for itself through continuous optimization.

**OPTIMIZATION INSIGHTS:**

Every cost display includes optimization context:

```
üí° COST OPTIMIZATION TIPS:

1. Turn 7 used Claude Opus ($0.0456) for a simple lookup.
   ‚Üí Could use Gemini Flash ($0.0018) for 96% cost reduction
   
2. 34% of queries are suitable for fast-path execution.
   ‚Üí Enable tactical fast path for ~$12/month savings
   
3. Session "Daily Reports" runs at peak hours with Sonnet.
   ‚Üí Switch to Gemini Flash off-peak for 90% cost reduction
   
4. 12 queries loaded >10K context unnecessarily.
   ‚Üí Enable context distillation for ~$8/month savings
```

Actionable recommendations based on your actual usage patterns.

**COST ALERTS:**

You can set spending alerts:

```
üö® COST ALERT CONFIGURATION:

‚úì Daily spending exceeds $5
‚úì Session cost exceeds $1  
‚úì Single query exceeds $0.50
‚úì Monthly projected spend exceeds $150

Alert method: Email + Dashboard notification
```

If a query unexpectedly balloons in cost, you'll know immediately.

**EXPORT AND REPORTING:**

All cost data is exportable:

```
EXPORT OPTIONS:
- CSV (for Excel analysis)
- JSON (for API integration)
- PDF (for stakeholder reports)

Filters:
- Date range
- User (multi-user environments)
- Session
- Model/Provider
- Cost threshold
```

Generate monthly reports for finance. Track ROI. Justify infrastructure costs.

**THE BOTTOM LINE:**

From individual tokens to organizational budgets, Uderia gives you complete visibility:

- Turn-level: See every token, every cent
- Session-level: Track cumulative costs per workflow
- Historical: Identify trends and forecast spending
- Comparative: Choose optimal models based on real data
- RAG-attributed: Measure continuous improvement ROI
- Optimization-aware: Understand what's saving money

No hidden costs. No surprises. Complete financial transparency.

Because in enterprise AI, trust requires knowing exactly what you're paying for.

Every token. Every penny. Every optimization. All visible. All actionable.

This is financial governance for the AI age.
```

**Screen Capture Plan:**
- [ ] Session History with turn-level cost displays
- [ ] Turn 3 expanded: Token breakdown (input/output)
- [ ] Cost breakdown with model-specific pricing
- [ ] Efficiency section showing RAG guidance and optimization savings
- [ ] Provider pricing reference table (Claude, GPT, Gemini, Bedrock, Ollama)
- [ ] Session summary at top: 12 turns, $0.2134 total
- [ ] Average per turn, most expensive turn, most efficient turn
- [ ] Optimization impact: $0.3287 baseline vs $0.2134 actual
- [ ] Intelligence ‚Üí Token Analytics dashboard
- [ ] 30-day token usage chart with trend line
- [ ] 30-day cost trend chart
- [ ] Projected monthly spend calculation
- [ ] Intelligence ‚Üí Provider Comparison
- [ ] Workload comparison table across 6 providers
- [ ] Cost range: $0.487 (Opus) to $0.000 (Ollama)
- [ ] Recommendation box with strategic guidance
- [ ] Intelligence ‚Üí RAG Efficiency dashboard
- [ ] RAG cost impact: $52.30 baseline vs $37.60 actual
- [ ] Savings: $14.70 (28%), projected annual: $876
- [ ] Optimization insights panel with 4 actionable tips
- [ ] Cost alert configuration interface
- [ ] Export options: CSV, JSON, PDF with filters

**Supporting Materials:**
- Cost tracking documentation
- Provider pricing reference (updated monthly)
- Cost optimization strategies guide
- ROI calculation methodology
- Budget forecasting tools
- Cost alert setup guide
- Financial reporting templates

**Supporting Materials:**
- Token tracking documentation
- Cost optimization guide
- Pricing model reference

---

## Module 5: SOVEREIGNTY - Your Data, Your Rules
**Goal:** Demonstrate ultimate flexibility in data exposure and infrastructure choice

### 5.1 Multi-Provider LLM Support: Freedom to Choose
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Explore supported LLM providers
- Learn provider switching mechanics
- Understand use cases for different providers

**Key Topics to Cover:**
- [ ] Cloud hyperscalers (Google, Anthropic, OpenAI, Azure)
- [ ] AWS Bedrock support
- [ ] Friendli.AI integration
- [ ] Ollama for local execution
- [ ] Dynamic provider switching
- [ ] Live model refresh

**Narration Script:**
```
Most AI platforms lock you into one provider. Choose Claude or GPT-4 or Gemini‚Äîyou're stuck.

Vendor lock-in. No flexibility. No negotiating leverage.

Uderia breaks this constraint. True multi-provider freedom.

**THE PROVIDER LANDSCAPE:**

Setup ‚Üí LLM Providers. Uderia's ecosystem:

```
PROVIDER ECOSYSTEM:

‚òÅÔ∏è Cloud Hyperscalers:
   ‚Ä¢ Anthropic (Claude Opus, Sonnet, Haiku)
   ‚Ä¢ OpenAI (GPT-4o, GPT-4o-mini)
   ‚Ä¢ Google (Gemini 1.5 Pro, Flash)
   ‚Ä¢ Azure OpenAI (Enterprise models)

üîß Managed Inference:
   ‚Ä¢ AWS Bedrock (foundation models + inference profiles)
   ‚Ä¢ Friendli.AI (optimized inference)

üè† Sovereign Execution:
   ‚Ä¢ Ollama (Llama 3.1, Mistral, Qwen - fully local)
```

Every provider. Every model. One platform. Your choice.

**ADDING A NEW PROVIDER:**

Add AWS Bedrock. Click "Add Provider" ‚Üí "AWS Bedrock."

```
BEDROCK CONFIGURATION:

üåç Region: us-east-1
üîë Access Key ID: [encrypted at rest]
üîê Secret Access Key: [SHA256 hashing + salt]
‚ö° Inference Profile: [optional cross-region routing]

Save ‚Üí Bedrock available
```

**MODEL SELECTION:**

Setup ‚Üí Profiles ‚Üí Edit @PROD.

LLM Provider dropdown:

```
AVAILABLE PROVIDERS:
‚Ä¢ Claude Sonnet 3.5 (Anthropic)
‚Ä¢ GPT-4o (OpenAI)
‚Ä¢ Gemini 1.5 Pro (Google)
‚Ä¢ Claude Sonnet 3.5 v2 (AWS Bedrock)
‚Ä¢ Llama 3.1 70B (Ollama - Local)
```

One dropdown. Six infrastructure options.

Select "Claude Sonnet 3.5 v2 (AWS Bedrock)" ‚Üí Save.
**DYNAMIC PROVIDER SWITCHING:**

Flexibility in action. Query with @PROD (AWS Bedrock):

```
QUERY: "Q4 revenue trends by product category?"

@PROD Execution:
‚îú‚îÄ Provider: AWS Bedrock
‚îú‚îÄ Model: Claude Sonnet 3.5 v2
‚îú‚îÄ Region: us-east-1
‚îî‚îÄ Cost: $0.0112
```

Switch to @ANALYST (Google Gemini Flash). Same query:

```
@ANALYST Execution:
‚îú‚îÄ Provider: Google AI
‚îú‚îÄ Model: Gemini 1.5 Flash
‚îî‚îÄ Cost: $0.0018 (6x cheaper)
```

Same question. Different provider. Different cost.

You're not locked in. You're orchestrating strategically.
Execution:
- Provider: Google AI
- Model: Gemini 1.5 Flash
- Cost: $0.0018

Same question. Same Uderia platform. Different provider. Different cost. Different 
infrastructure characteristics (Gemini is 6x cheaper, but less capable on complex reasoning).

You're not locked in. You're orchestrating providers strategically.

**USE CASES FOR DIFFERENT PROVIDERS:**

```
STRATEGIC PROVIDER SELECTION:

Anthropic Direct ‚Üí Latest Claude features, fastest updates
AWS Bedrock ‚Üí Enterprise compliance, cross-region routing
Google AI ‚Üí Cost optimization, fast inference, multimodal
OpenAI ‚Üí GPT-4 reasoning, specific requirements
Ollama Local ‚Üí Complete sovereignty, zero cloud cost
```

**LIVE MODEL REFRESH:**

Providers release new models constantly. Uderia supports live refresh.

Setup ‚Üí LLM Providers ‚Üí "Refresh Models."

Uderia queries the provider API. Updates dropdown immediately.

No platform restart. No config editing. Just click.

Anthropic released Claude Opus 4? Refresh ‚Üí Available.
OpenAI released GPT-5? Refresh ‚Üí Ready.
**COST ARBITRAGE:**

Strategic advantage: price competition between providers.

```
PRICING COMPARISON (per 1M tokens):

Model                        Input / Output
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Claude Sonnet 3.5 (Anthropic)  $3 / $15
Claude Sonnet 3.5 v2 (Bedrock) $3 / $15 (volume discounts)
Gemini 1.5 Pro (Google)        $1.25 / $5 (comparable quality)
```

Test identical queries across providers. Measure quality. Choose optimal cost-to-quality ratios.
**PROVIDER FAILOVER:**

Bonus: provider outage? Switch profiles and continue.

Anthropic API down? Switch @PROD (Anthropic) ‚Üí @BACKUP (AWS Bedrock Claude).

Same model. Different infrastructure. Zero downtime.

Multi-provider sovereignty. Not vendor lock-in. Not single-point-of-failure.

From "stuck with one vendor" to "orchestrating the entire AI ecosystem."

Your data. Your infrastructure. Your choice.

From "stuck with one vendor" to "orchestrating the entire AI ecosystem."

Your data. Your infrastructure. Your choice.
```

**Screen Capture Plan:**
- [ ] Setup ‚Üí LLM Providers landing page showing 6+ providers
- [ ] "Add Provider" button ‚Üí AWS Bedrock configuration form
- [ ] Credential entry (Access Key, Secret Key) with encryption indicator
- [ ] Save provider ‚Üí Success confirmation
- [ ] Setup ‚Üí Profiles ‚Üí Edit @PROD profile
- [ ] LLM Provider dropdown showing 6 options across providers
- [ ] Select "Claude Sonnet 3.5 v2 (AWS Bedrock)"
- [ ] Query execution showing "Provider: AWS Bedrock" in metadata
- [ ] Switch to @ANALYST profile (Gemini Flash)
- [ ] Same query execution showing "Provider: Google AI" in metadata
- [ ] Cost comparison: $0.0112 (Bedrock) vs $0.0018 (Gemini)
- [ ] "Refresh Models" button demonstration
- [ ] New models appearing in dropdown after refresh
- [ ] Provider comparison table with pricing across all options

**Supporting Materials:**
- Multi-provider documentation
- Provider setup guides per platform (AWS, Google, Anthropic, etc.)
- Credential encryption security documentation
- Cost comparison calculator
- Provider failover strategies
- Model refresh documentation

---

### 5.2 Hybrid Intelligence: Cloud Power + Local Privacy
**Duration:** 4-5 minutes  
**Learning Objectives:**
- Understand hybrid architecture
- See cloud planning + local execution
- Learn champion case injection for local models

**Key Topics to Cover:**
- [ ] Hybrid architecture concept (strategic planner vs executor)
- [ ] Cloud LLM for planning intelligence
- [ ] Local Ollama for private execution
- [ ] Champion case injection to boost local model performance
- [ ] Complete data sovereignty

**Narration Script:**
```
The impossible choice every organization faces: cloud intelligence or data sovereignty.

Pick one.

Uderia says: Why not both?

Hybrid Intelligence‚Äîcloud reasoning with zero-trust privacy.

**THE IMPOSSIBLE CHOICE:**

```
TRADITIONAL APPROACH:

Option A: Cloud Models (Claude, GPT-4)
‚îú‚îÄ Intelligence: ‚úì‚úì Brilliant
‚îú‚îÄ Data: ‚úó Sent to external APIs
‚îî‚îÄ Sovereignty: ‚úó Sacrificed

Option B: Local Models (Ollama)
‚îú‚îÄ Intelligence: ‚úó Mediocre
‚îú‚îÄ Data: ‚úì Kept private
‚îî‚îÄ Sovereignty: ‚úì Maintained
```

Binary choice. Intelligence OR privacy. Never both.

**UDERIA'S HYBRID ARCHITECTURE:**

Uderia splits the AI agent into two layers:

```
LAYER 1: Strategic Planner (Cloud)
‚îú‚îÄ Runs: Claude, GPT-4, Gemini
‚îú‚îÄ Receives: Question + tool names + schemas
‚îú‚îÄ Does NOT receive: Actual data, PII, results
‚îú‚îÄ Generates: Strategic plan, SQL templates
‚îî‚îÄ Sends: Intelligence back to your infrastructure

LAYER 2: Tactical Executor (Local/Sovereign)
‚îú‚îÄ Runs: Ollama on YOUR infrastructure
‚îú‚îÄ Receives: Strategic plan + MCP tools + actual data
‚îú‚îÄ Executes: SQL against YOUR database
‚îú‚îÄ Data: Never leaves your environment
‚îî‚îÄ Returns: Results to you (never to cloud)
```

Cloud does the thinking. Local does the data access.

**DEMONSTRATION: @LOCAL PROFILE WITH HYBRID MODE**

Switch to @LOCAL profile.

```
PROFILE CONFIGURATION:

Strategic Planner: Claude Sonnet 3.5 (Cloud - Anthropic)
Tactical Executor: Llama 3.1 70B (Local - Ollama)
MCP Tools: Local Teradata connector
Mode: Hybrid Intelligence
```
**SCENARIO: Analyzing Customer Churn with PII**

Ask: "Identify customers who stopped purchasing in Q4, show their names, emails, addresses, and analyze common patterns for churn risk."

This requires:
- Sensitive PII (names, emails, addresses)
- Multi-table joins (Customers, Sales)
- Complex pattern analysis

Live Status Panel shows:
- Sensitive PII (names, emails, addresses)
- Multi-table joins (Customers, Sales)
- Complex pattern analysis (churn indicators)

Watch what happens in the Live Status Panel:

```
üå©Ô∏è STRATEGIC PLANNING (Cloud - Claude Sonnet 3.5):

Query received: "Identify customers who stopped purchasing in Q4..."

Data sent to cloud: 
  - Query text: ‚úì
  - Available tools: [base_readQuery, base_tableList, TDA_LLMTask]
  - Schema info: Customers(CustomerID, FirstName, LastName, Email, Address...)
                Sales(SaleID, CustomerID, SaleDate...)
  - Actual customer data: ‚úó (NEVER SENT)

Strategic Plan Generated:
  Phase 1: Query customers with no Q4 purchases
    Tool: base_readQuery
    SQL Template: SELECT c.CustomerID, c.FirstName, c.LastName, c.Email, c.Address
                  FROM Customers c 
                  LEFT JOIN Sales s ON c.CustomerID = s.CustomerID 
                    AND EXTRACT(QUARTER FROM s.SaleDate) = 4
                  WHERE s.SaleID IS NULL
  
  Phase 2: Analyze patterns using TDA_LLMTask
    Input: Churned customer demographics
    Analysis: Identify common characteristics

Plan sent back to local environment.
```

```
CLOUD RECEIVED:
‚úì Your question (not sensitive)
‚úì Schema metadata (table/column names)
‚úì Tool names (generic capabilities)

CLOUD NEVER RECEIVED:
‚úó Customer names, emails, addresses
‚úó Actual sales data
‚úó Any PII or confidential information
```

Cloud generated brilliant strategic plan without ever touching your data.

```
üè† TACTICAL EXECUTION (Local - Ollama Llama 3.1 70B):

Strategic plan received from cloud.
Executing locally with full data access...

Phase 1 Execution:
  Tool: base_readQuery (LOCAL MCP connector)
  SQL: [Executing against local Teradata database]
  Results: 47 customers identified
  
  Sample data (STAYS LOCAL):
    - John Smith, john.smith@email.com, 123 Main St, Boston MA
    - Sarah Johnson, sarah.j@email.com, 456 Oak Ave, Seattle WA
    [...45 more customers with full PII]

Phase 2 Execution:
  Tool: TDA_LLMTask (LOCAL Ollama execution)
  Input: Churned customer demographics
  Analysis: 
    - 68% are located in urban areas
    - Average age: 34-42 demographic
    - Common trait: High service ticket volume (avg 3.2 tickets per customer)
    - Purchase pattern: Previously bought Cardio equipment
  
  Churn indicator: Strong correlation with unresolved service issues

Final Report Generated (LOCAL):
  Churn risk strongly tied to poor customer service experience.
  Recommendation: Proactive outreach to customers with >2 open service tickets.
```

```
DATA FLOW:
1. Your question ‚Üí Cloud (strategic planning)
2. Strategic plan ‚Üê Cloud (execution strategy)
3. Data access ‚Üí Local only (PII never leaves)
4. Analysis ‚Üí Local Ollama (cloud strategy)
5. Results ‚Üí You (privacy maintained)

THE RESULT:
‚úì Cloud-level strategic intelligence
‚úì Complete data sovereignty
‚úì Regulatory compliance (GDPR, HIPAA, SOC2)
‚úì Zero compromise on capability
```

**PERFORMANCE COMPARISON:**

```
APPROACH COMPARISON:

Approach 1: Fully Cloud (Traditional)
‚îú‚îÄ Quality: 95/100 (excellent)
‚îú‚îÄ Privacy: ‚úó (PII exposed to third party)
‚îú‚îÄ Compliance: ‚úó (violates data residency)
‚îî‚îÄ Cost: $0.045

Approach 2: Fully Local (Traditional)
‚îú‚îÄ Quality: 72/100 (struggles with planning)
‚îú‚îÄ Privacy: ‚úì (completely local)
‚îú‚îÄ Compliance: ‚úì (data never leaves)
‚îî‚îÄ Cost: $0.00 (local execution)

Approach 3: Uderia Hybrid Intelligence
‚îú‚îÄ Quality: 92/100 (cloud strategy + local execution)
‚îú‚îÄ Privacy: ‚úì (data never leaves premises)
‚îú‚îÄ Compliance: ‚úì (regulatory requirements met)
‚îî‚îÄ Cost: $0.012 (planning only, execution free)
```

92% of full-cloud quality with 100% data sovereignty. Best of both worlds.

**CHAMPION CASE INJECTION BOOST:**

Here's the multiplier effect. Remember RAG champion cases from Module 4.2?
**CHAMPION CASE INJECTION BOOST:**

Multiplier effect. RAG champion cases from Module 4.2?

Hybrid Intelligence with @LOCAL: local Ollama gets boosted by champion cases learned from cloud.

```
LEARNING SEQUENCE:
1. Cloud Claude generates brilliant plan
2. RAG captures as champion case
3. Next similar query: Cloud plan + RAG champion
4. Local Ollama receives: Strategy + past success
5. Result: Local performs at near-cloud quality
```

Local model isn't just executing. It's learning from cloud intelligence via RAG‚Äîwithout exposing your data.

**THE BREAKTHROUGH:**

```
COMPARISON MATRIX:

                    Intelligence  Privacy    Cost
Traditional Cloud      ‚úì‚úì          ‚úó        $$$
Traditional Local      ‚úó           ‚úì‚úì        $
Uderia Hybrid          ‚úì‚úì          ‚úì‚úì       $$
```

From impossible choice to having it all.

Cloud-level reasoning. Zero-trust privacy. No compromise.

Your data stays home. Your intelligence reaches the cloud. Hybrid sovereignty.
```

**Screen Capture Plan:**
- [ ] Hybrid architecture diagram: Cloud Planner + Local Executor with data flow arrows
- [ ] @LOCAL profile configuration showing dual LLM setup
- [ ] Strategic Planner: Claude Sonnet 3.5 (Cloud)
- [ ] Tactical Executor: Ollama Llama 3.1 (Local)
- [ ] Customer churn query input with PII requirements
- [ ] Live Status Panel: Strategic Planning section (Cloud)
- [ ] Data sent to cloud: Schema ‚úì, Actual data ‚úó
- [ ] Strategic plan with SQL template generated
- [ ] Live Status Panel: Tactical Execution section (Local)
- [ ] Local SQL execution with actual PII data
- [ ] Customer records with names, emails, addresses visible (LOCAL ONLY)
- [ ] Local Ollama analysis execution
- [ ] Results with churn patterns and recommendations
- [ ] Network traffic monitor showing: Outbound (plan request), Inbound (strategy), No data upload
- [ ] Performance comparison table: Cloud vs Local vs Hybrid
- [ ] Quality scores: 95 vs 72 vs 92
- [ ] Privacy indicators: ‚úó vs ‚úì vs ‚úì
- [ ] Cost comparison: $0.045 vs $0.00 vs $0.012
- [ ] Champion case boost diagram showing RAG injection to local model

**Supporting Materials:**
- Hybrid intelligence architecture documentation
- Data flow diagrams (with security annotations)
- Ollama setup guide for local execution
- Compliance certification documentation (GDPR, HIPAA, SOC2)
- Performance benchmark comparisons
- Champion case injection technical details

---

### 5.3 Security & Multi-User Isolation
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Understand JWT authentication
- Learn user isolation mechanisms
- Explore role-based access control

**Key Topics to Cover:**
- [ ] JWT-based authentication (24-hour expiry)
- [ ] User-specific session directories
- [ ] Database-level UUID isolation
- [ ] Role-based access (User, Developer, Admin)
- [ ] Encrypted credential storage
- [ ] Multi-user simultaneous support

**Narration Script:**
```
Data sovereignty isn't just where your data lives. It's who can access it.

Uderia: bank-grade security isolation for enterprise multi-user environments.

**JWT AUTHENTICATION: SHORT-LIVED TOKENS**

Login to Uderia? Not a permanent session cookie. JSON Web Token (JWT) with 24-hour expiration.

```
LOGIN SEQUENCE:
1. Enter username + password
2. Server validates (bcrypt-hashed, salted)
3. Server issues JWT (HS256 signed)
4. Token contains: user_id, username, role, expiration
5. Stored: httpOnly cookie (secure)

EVERY API REQUEST:
‚úì Signature authentic?
‚úì Not expired?
‚úì Valid account status?

After 24 hours ‚Üí Token expires ‚Üí Re-authenticate
```

Security by design. Stolen token? Worthless after 24 hours.

**USER-SPECIFIC SESSION ISOLATION:**

Multi-user isolation. File system level.

```
SESSION DIRECTORIES:

tda_sessions/
‚îú‚îÄ‚îÄ user_alice_82b9231b.../
‚îÇ   ‚îú‚îÄ‚îÄ session_001.json
‚îÇ   ‚îî‚îÄ‚îÄ session_002.json
‚îú‚îÄ‚îÄ user_bob_f3a1c5d7.../
‚îÇ   ‚îî‚îÄ‚îÄ session_001.json
‚îî‚îÄ‚îÄ user_carol_a9b8c7d6.../
    ‚îî‚îÄ‚îÄ session_001.json
```

UUID suffix? Security isolation.

**Alice cannot access Bob's sessions.** File system enforces user-specific paths.

```
API REQUEST: GET /api/sessions/user_bob_session_001
JWT TOKEN: user_id=alice
SERVER: 403 Forbidden (user_id mismatch)
```

Session data isolated: file system level AND application level.

**DATABASE-LEVEL UUID ISOLATION:**
**DATABASE-LEVEL UUID ISOLATION:**

Database layer: PostgreSQL with UUID-based user identification.

Not sequential integers. UUIDs. Non-enumerable.

```
BAD (Sequential IDs):
SELECT * FROM consumption_logs WHERE user_id = 1001;
Attacker: "Try 1002, 1003, 1004..." (enumeration attack)

GOOD (UUID):
SELECT * FROM consumption_logs 
WHERE user_id = '82b9231b-08b1-4b1e-bcdf-9762db21f90b';
Attacker: "Try... um... 82b9231b...90c?" (impossible)
```

Every table uses UUID foreign keys:

```
users(user_id UUID)
sessions(session_id UUID, user_id UUID)
consumption_logs(log_id UUID, user_id UUID)
rag_cases(case_id UUID, user_id UUID)
```

Even with SQL injection (doesn't exist‚Äîparameterized queries), enumerating users is cryptographically infeasible.
**ROLE-BASED ACCESS CONTROL (RBAC):**

**ROLE-BASED ACCESS CONTROL (RBAC):**

Three roles with escalating privileges:

```
ROLE: User (Standard)
‚îú‚îÄ Execute queries (UI + API)
‚îú‚îÄ Manage own sessions
‚îú‚îÄ Access own RAG collections
‚îî‚îÄ View own consumption logs

ROLE: Developer
‚îú‚îÄ All User permissions +
‚îú‚îÄ Developer tools
‚îú‚îÄ Custom MCP servers
‚îî‚îÄ Debug mode + advanced logging

ROLE: Admin
‚îú‚îÄ All Developer permissions +
‚îú‚îÄ User management (create, delete, suspend)
‚îú‚îÄ System configuration (LLM providers, MCP)
‚îú‚îÄ Global consumption analytics
‚îú‚îÄ Long-lived access tokens
‚îî‚îÄ Audit log access
```

Role enforcement at every API endpoint:

```python
@app.route('/api/admin/users', methods=['GET'])
@require_role('Admin')
def list_all_users():
    # Only executes if JWT has role='Admin'
    return get_all_users()
```

Access admin endpoints without admin role? **403 Forbidden.**

When you configure LLM providers (Anthropic API keys, AWS credentials, etc.), where 
do those secrets go?

NOT in plaintext. Not in environment variables (easily leaked). In encrypted database columns.

Storage sequence:
1. User enters API key: `sk-ant-api03-xxxxxxxxxxxx`
2. Server encrypts with AES-256: `U2FsdGVkX1+abcdef...` (cipher text)
3. Server stores cipher text in database
4. When needed: Server decrypts on-the-fly for API calls
5. Cipher text never leaves the server

Even if someone dumps the database, they get encrypted gibberish‚Äîuseless without the 
encryption key (stored separately in secure environment variables).

This is how Uderia handles sensitive credentials:
- LLM provider API keys: Encrypted
- AWS access keys: Encrypted
- Database connection strings: Encrypted
- OAuth tokens: Encrypted

**MULTI-USER SIMULTANEOUS SUPPORT:**

Shared multi-user platform. Alice, Bob, Carol‚Äîall logged in simultaneously. No interference.

How?

```
SESSION ISOLATION:
Alice: tda_sessions/user_alice_.../session_001.json
Bob: tda_sessions/user_bob_.../session_001.json
No overlap. No cross-contamination.

MEMORY ISOLATION:
‚Ä¢ Python processes handle requests independently
‚Ä¢ No shared state between users
‚Ä¢ Each API call validates JWT + user_id

DATABASE ISOLATION:
‚Ä¢ All queries filtered by user_id UUID
‚Ä¢ PostgreSQL row-level security policies
‚Ä¢ Application layer blocks cross-user queries
```

**AUDIT LOGGING:**

Compliance and security monitoring. Audit logs:

```
AUDIT LOG ENTRIES:

[10:32:15] alice (82b9...) | LOGIN | IP: 192.168.1.45 | Success
[10:33:01] alice (82b9...) | QUERY_EXECUTE | session_001 | $0.0045
[10:45:22] bob (f3a1...) | LOGIN | IP: 192.168.1.67 | Success
[10:46:10] bob (f3a1...) | ACCESS_ADMIN_PANEL | DENIED (role=User)
[11:02:33] admin (a9b8...) | USER_CREATE | Target: carol | Role: User
```

Every sensitive action logged. Admins review:

```
‚úì Failed login attempts (brute force detection)
‚úì Unauthorized access attempts (security monitoring)
‚úì User creation/deletion (accountability)
‚úì Credential changes (suspicious activity)
```
**THE SECURITY GUARANTEE:**

```
UDERIA MULTI-USER ARCHITECTURE:

Authentication ‚Üí JWT (24h expiration) + bcrypt hashing
Isolation ‚Üí User-specific paths + UUID database keys
Authorization ‚Üí RBAC (User, Developer, Admin)
Encryption ‚Üí AES-256 (credentials) + SHA256 (tokens)
Auditability ‚Üí Complete logging (compliance + forensics)
```

From "shared platform" to "bank-grade multi-tenant security."

Your data. Your users. Your control. Complete isolation. Total security.
From "shared platform" to "bank-grade multi-tenant security."

Your data. Your users. Your control. Complete isolation. Total security.
```

**Screen Capture Plan:**
- [ ] Login page with credential entry
- [ ] JWT token structure visualization (decoded payload showing user_id, role, exp)
- [ ] File system view: tda_sessions/ directory with multiple user folders
- [ ] User folder showing UUID suffix for isolation
- [ ] API request attempt: Alice trying to access Bob's session ‚Üí 403 Forbidden
- [ ] Database schema showing UUID columns (users, sessions, consumption_logs)
- [ ] UUID vs Sequential ID comparison diagram
- [ ] Admin panel: User Management interface
- [ ] Three users listed: alice (User), bob (Developer), admin (Admin)
- [ ] Role assignment dropdown
- [ ] Permissions matrix table showing role capabilities
- [ ] Developer trying to access admin endpoint ‚Üí 403 Forbidden
- [ ] Setup ‚Üí LLM Providers: API key entry
- [ ] Encryption indicator (üîí) next to saved credential
- [ ] Database view showing encrypted credential (cipher text)
- [ ] Multi-user simultaneous query execution (split screen)
- [ ] Alice running revenue query (her session)
- [ ] Bob running customer query (his session)
- [ ] No interference between executions
- [ ] Admin ‚Üí Audit Logs viewer
- [ ] Log entries showing: Logins, Query executions, Access denials
- [ ] Failed access attempt highlighted for security review

**Supporting Materials:**
- Security architecture documentation
- JWT authentication flow diagram
- Multi-user isolation technical specifications
- RBAC permissions matrix
- Credential encryption technical details (AES-256)
- Audit logging guide
- Compliance certifications (SOC2, ISO27001 readiness)
- Multi-tenant security best practices

---

## Module 6: COLLABORATIVE - Intelligence Marketplace
**Goal:** Transform individual expertise into collective organizational knowledge

### 6.1 Marketplace Overview: Collective Intelligence
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Understand marketplace value proposition
- Learn dual repository architecture (Planner vs Knowledge)
- See community ecosystem vision

**Key Topics to Cover:**
- [ ] From isolated expertise to collective intelligence
- [ ] Planner Repositories (üìã execution patterns)
- [ ] Knowledge Repositories (üìÑ domain knowledge)
- [ ] Visual separation and organization
- [ ] Network effects and cost reduction

**Narration Script:**
```
Every organization faces the same problem.

Your data analyst discovers a brilliant query pattern for customer churn analysis. Three days of iteration. $45 in LLM tokens. Perfect results.

Next week, another analyst asks the same question. Starts from zero. Another three days. Another $45.

Reinventing the wheel.

Uderia's Marketplace breaks this cycle. Collective Intelligence‚Äîorganizational knowledge becomes reusable assets.

**THE MARKETPLACE VISION:**

Marketplace icon ‚Üí Uderia's dual repository architecture:

```
TWO TYPES OF INTELLIGENCE:

üìã PLANNER REPOSITORIES (Left Tab):
   ‚Ä¢ Execution patterns ("how to do it")
   ‚Ä¢ RAG champion cases from successful queries
   ‚Ä¢ Strategic plans, tool sequences, optimizations
   ‚Ä¢ Think: "Recipes for accomplishing tasks"
   
   Examples:
   ‚îú‚îÄ Customer Churn Analysis Patterns (23 cases)
   ‚îú‚îÄ Revenue Forecasting Templates (17 cases)
   ‚îî‚îÄ Database Performance Playbooks (31 cases)

üìÑ KNOWLEDGE REPOSITORIES (Right Tab):
   ‚Ä¢ Domain expertise ("what you need to know")
   ‚Ä¢ Uploaded documents: Specs, guides, documentation
   ‚Ä¢ Chunked and embedded for RAG retrieval
   ‚Ä¢ Think: "Reference library your AI can read"
   
   Examples:
   ‚îú‚îÄ GDPR Compliance Guidelines (47 chunks)
   ‚îú‚îÄ SQL Optimization Best Practices (89 chunks)
   ‚îî‚îÄ Teradata System Documentation (312 chunks)
```
**VISUAL ORGANIZATION:**

Collection cards show:

```
COLLECTION CARD:

Customer Churn Analysis Patterns
üìã Planner | Created by @ANALYST

23 items | 847 uses | 4.8‚≠ê

"Pre-validated patterns for identifying at-risk customers 
using purchase history and service ticket correlation."

Tags: #churn #customer-analytics #predictive
```

Sort by: Most Popular | Top Rated | Recently Added | Most Relevant
**THE VALUE PROPOSITION:**

Quantifying collective intelligence:

```
WITHOUT MARKETPLACE (Traditional):
Analyst A ‚Üí Develops churn query ‚Üí 3 days, $45
Analyst B ‚Üí Same need, starts from scratch ‚Üí 3 days, $45
Analyst C ‚Üí Same need, starts from scratch ‚Üí 3 days, $45
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: 9 days, $135, 3x redundant work

WITH MARKETPLACE (Uderia):
Analyst A ‚Üí Develops query ‚Üí 3 days, $45 ‚Üí Publishes
Analyst B ‚Üí Searches ‚Üí Finds pattern ‚Üí Deploys ‚Üí 5 min, $0.12
Analyst C ‚Üí Same search ‚Üí Same deploy ‚Üí 5 min, $0.12
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: 3 days + 10 min, $45.24 (67% savings), zero redundancy
```

Analyst A's effort ‚Üí Organizational asset. B and C benefit immediately.ce
- Analyst B: Searches marketplace ‚Üí Finds "Customer Churn Patterns" ‚Üí Deploys to profile ‚Üí 5 minutes, $0.12 cost
- Analyst C: Same search ‚Üí Same deployment ‚Üí 5 minutes, $0.12 cost

**Total:** 3 days + 10 minutes, $45.24 cost (67% savings), zero redundancy

Analyst A's effort becomes organizational asset. Analysts B and C benefit immediately.

**NETWORK EFFECTS:**

The more people contribute, the more valuable the marketplace:

```
MARKETPLACE GROWTH:

After 10 analysts:
‚îú‚îÄ 10 planner collections (200+ champion cases)
‚îú‚îÄ 10 knowledge collections (500+ document chunks)
‚îî‚îÄ Every new analyst ‚Üí Instant collective expertise

After 100 users:
‚îú‚îÄ 100+ collections (every common business question)
‚îú‚îÄ Thousands of validated patterns
‚îú‚îÄ New analysts productive on day one
‚îî‚îÄ Zero ramp-up time for standard analyses
```

Value grows exponentially. Classic network effect.

**COST REDUCTION AT SCALE:**

Marketplace impact projection:

```
ORGANIZATION: 50 analysts

WITHOUT MARKETPLACE:
‚Ä¢ Each analyst: 10 common queries/quarter
‚Ä¢ Avg cost per query: $30 (LLM tokens + time)
‚Ä¢ Total quarterly: 50 √ó 10 √ó $30 = $15,000
‚Ä¢ Redundancy rate: ~70% (already solved elsewhere)

WITH MARKETPLACE:
‚Ä¢ 30% developed from scratch: $4,500
‚Ä¢ 70% reused from marketplace: $210 (deploy only)
‚Ä¢ Total quarterly: $4,710
‚Ä¢ Savings: $10,290 (69% reduction)
```

**DISCOVERY & SEARCH:**

Finding the right collection? Smart search.

Search: "How do I identify customers likely to churn?"

```
SEARCH RESULTS:

1. Customer Churn Analysis Patterns
   üìã Planner | 23 cases | 4.8‚≠ê
   "Churn prediction using purchase gaps"

2. Customer Retention Best Practices
   üìÑ Knowledge | 67 chunks | 4.6‚≠ê
   "Domain knowledge from CRM strategy documents"

3. Service Ticket Risk Indicators
   üìã Planner | 14 cases | 4.7‚≠ê
**PROFILES AS MARKETPLACE CONSUMERS:**

@ANALYST profile from Module 2.2:

```
@ANALYST PROFILE:
‚îú‚îÄ LLM: Gemini Flash (cost-optimized)
‚îú‚îÄ Tools: TDA_Charting (visualizations)
‚îú‚îÄ Deployed Planners:
‚îÇ  ‚îú‚îÄ Customer Churn Patterns
‚îÇ  ‚îî‚îÄ Revenue Forecasting Templates
‚îî‚îÄ Subscribed Knowledge:
   ‚îú‚îÄ Sales Strategy Playbooks
   ‚îî‚îÄ Product Documentation
```

Profiles aren't just LLM configs. They're marketplaces of deployed intelligence.

Activate @ANALYST ‚Üí LLM + deployed patterns + subscribed knowledge.

One profile. Dozens of collections. Hundreds of expert patterns. Instant capability.ce.

When you activate @ANALYST, you're activating:
- The LLM (Gemini Flash)
**THE COLLABORATIVE SHIFT:**

```
TRADITIONAL AI:
‚úó Each user isolated
‚úó Every query starts from zero
‚úó Knowledge dies when user leaves
‚úó No organizational learning

UDERIA MARKETPLACE:
‚úì Community of contributors
‚úì Every query builds on past success
‚úì Knowledge persists as organizational asset
‚úì Continuous collective improvement
```

From "everyone for themselves" to "standing on each other's shoulders."

From isolated expertise to collective intelligence. From reinventing wheels to deploying proven patterns.

The Marketplace isn't a feature. It's a fundamental shift in how organizations learn.

From isolated expertise to collective intelligence. From reinventing wheels to deploying 
proven patterns. From costly redundancy to efficient reuse.

The Marketplace isn't a feature. It's a fundamental shift in how organizations learn.
```

**Screen Capture Plan:**
- [ ] Marketplace landing page with dual-tab interface
- [ ] Planner Repositories tab (left) with collection cards
- [ ] Knowledge Repositories tab (right) with collection cards
- [ ] Collection card detail view showing:
  - Name, type badge (üìã or üìÑ), author
  - Stats (23 items | 847 uses | 4.8‚≠ê)
  - Description and tags
- [ ] Sort options dropdown (Popular, Top Rated, Recent, Relevant)
- [ ] Value proposition comparison chart:
  - Without Marketplace: 9 days, $135
  - With Marketplace: 3.17 days, $45.24 (67% savings)
- [ ] Network effects visualization: Value growth curve as user count increases
- [ ] Cost reduction projection table for 50-analyst organization
- [ ] Search bar with query: "identify customers likely to churn"
- [ ] Search results showing 3 relevant collections with semantic matching
- [ ] @ANALYST profile view showing deployed collections:
  - Planner: Customer Churn Patterns, Revenue Forecasting
  - Knowledge: Sales Strategy, Product Docs
- [ ] Community statistics panel:
  - Total collections: 127
  - Total contributors: 43
  - Total deployments: 1,284
  - Avg cost savings per reuse: $28.50

**Supporting Materials:**
- Marketplace architecture documentation
- Value proposition case studies
- Network effects analysis
- ROI calculator for marketplace adoption
- Discovery and search algorithm documentation

---

### 6.2 Planner Repositories: Share Execution Patterns
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Create templates from successful cases
- Browse and discover planner collections
- Deploy patterns to your repository

**Key Topics to Cover:**
- [ ] "Create Template" from RAG case
- [ ] Template metadata (name, description, tags)
- [ ] Browsing planner collections
- [ ] "Deploy to My Repository" workflow
- [ ] Pattern reuse and cost savings

**Narration Script:**
```
You just executed the perfect query.

20 minutes of refinement. Three tool iterations. $2.40 in LLM tokens. Flawless result.

Now what? Let it disappear into conversation history? Make someone else spend another 20 minutes and $2.40 to rediscover this?

No. Capture it. Templatize it. Share it.

Welcome to Planner Repositories.

**WHAT ARE PLANNER REPOSITORIES?**

Collections of execution patterns‚ÄîRAG champion cases from successful queries shared across your organization.

```
THINK OF THEM AS:
‚Ä¢ Cookbooks for AI agents (step-by-step recipes)
‚Ä¢ Playbooks for analysts (proven strategies)
‚Ä¢ Accelerators for new users (deploy expert patterns instantly)

EVERY PLANNER REPOSITORY CONTAINS:
‚îú‚îÄ Champion case metadata (query pattern, context, tools)
‚îú‚îÄ Strategic plan (high-level approach)
‚îú‚îÄ Tool execution sequence (tactical steps)
‚îî‚îÄ Expected outcomes (what success looks like)
```

Not generic templates. Actual successful executions captured from real work.
**CREATING A TEMPLATE FROM RAG CASE:**

You just finished this query:

"Analyze Q4 revenue by product category, identify top 3 performers, and create a trend chart comparing to Q3."

```
EXECUTION DETAILS:
Tools: base_readQuery ‚Üí TDA_LLMTask ‚Üí TDA_Charting
Cost: $1.85
Duration: 34 seconds
Result: Perfect insights with professional chart
```

Worth saving. See "Create Template" button? Click it.orth saving. Look at the conversation turn‚Äîsee the "Create Template" button? 
Click it.
**TEMPLATE CREATION FORM:**

Uderia auto-populates:

```
TEMPLATE:

Name: "Quarterly Revenue Analysis with Trend Comparison"

Description: "Analyzes revenue by product category for specified 
quarter, identifies top performers, generates comparative trend chart."

Category: Financial Analysis
Tags: #revenue #quarterly-analysis #trending #visualization

Captured Elements:
‚îú‚îÄ Query Pattern: "Analyze {time_period} revenue by {dimension}..."
‚îú‚îÄ Tool Sequence: base_readQuery ‚Üí TDA_LLMTask ‚Üí TDA_Charting
‚îú‚îÄ Context: Access to Sales and SaleDetails tables
‚îî‚îÄ Expected Cost: ~$1.50-$2.00 per execution

Privacy Level:
‚óã Public (anyone in org can discover)
‚óã Unlisted (link only)
‚óã Private (only you)
```

Choose **Public**. Click "Save Template."

20 minutes of work ‚Üí Reusable organizational asset.
You just transformed 20 minutes of work into a reusable organizational asset.

**BROWSING PLANNER COLLECTIONS:**

Marketplace ‚Üí Planner Repositories. Organizational knowledge base:

```
FEATURED COLLECTIONS:

1. Customer Insights Playbook | üìã 23 patterns | 4.8‚≠ê
   Author: @ANALYST
   "Customer behavior analysis, churn prediction, segmentation"
   Top patterns:
   ‚îú‚îÄ Customer Churn Risk Identification (142 uses)
   ‚îú‚îÄ High-Value Customer Segmentation (98 uses)
   ‚îî‚îÄ Purchase Frequency Analysis (87 uses)

2. Database Performance Optimization | üìã 31 patterns | 4.9‚≠ê
   Author: @DBA
   "Query optimization, index analysis, troubleshooting"
   Top patterns:
   ‚îú‚îÄ Slow Query Diagnosis (234 uses)
   ‚îú‚îÄ Index Recommendation Strategy (156 uses)
   ‚îî‚îÄ Table Statistics Refresh (89 uses)

3. Sales Analytics Accelerators | üìã 17 patterns | 4.7‚≠ê
   Author: @ANALYST + @PROD
   "Revenue analysis, forecasting, product performance"
   Includes YOUR template (12 uses already!)
```

Click "Customer Insights Playbook" to explore.

**COLLECTION DETAIL VIEW:**

23 champion case patterns organized by category.

```
PATTERN PREVIEW:

Customer Churn Risk Identification

Query Template: "Identify customers with {condition} 
                 indicating churn risk in {timeframe}"

Tools: base_readQuery, base_tableList, TDA_LLMTask
Avg Cost: $0.85
Deployments: 142
Rating: 4.9‚≠ê

Preview: "Analyzes purchase gaps, service ticket volume, 
engagement drops to identify at-risk customers. Prioritized 
list with risk scores."
```

Proven pattern‚Äî142 successful uses. Not experimenting. Deploying certified expertise.

**DEPLOYING A COLLECTION:**

**DEPLOYING A COLLECTION:**

Click "Deploy to My Repository."

```
DEPLOYMENT OPTIONS:

Target Profile: @ANALYST
Deployment Mode:
‚óã Subscribe (reference-based, auto-updates)
‚óã Fork (independent copy, you control updates)

Subscribe means:
‚úì Instant access to all 23 patterns
‚úì New patterns added ‚Üí You receive automatically
‚úì Patterns improved ‚Üí You benefit immediately
‚úì No storage duplication
```

Click "Deploy." Done.

Activate @ANALYST ‚Üí Intelligence View ‚Üí Planner Repository:

```
YOUR REPOSITORY:
‚îú‚îÄ Your templates: (1 pattern)
‚îú‚îÄ Subscribed: Customer Insights Playbook (23 patterns)
‚îî‚îÄ Total available: 24 patterns
```

@ANALYST profile just became 24x more capable. Instantly.

Let's test it. Ask a new query:

"Which customers haven't purchased in 60 days and have multiple service tickets?"

Watch the RAG system:

```
üéØ RAG RETRIEVAL:

Searching planner repository for relevant patterns...

Match found:
  Collection: Customer Insights Playbook
  Pattern: "Customer Churn Risk Identification"
  Similarity: 0.94 (excellent match)
  
  Retrieved champion case:
    Query Pattern: "Identify customers with {no recent purchases} and {high service volume}"
    Strategic Approach:
      1. Query Sales table for customers with SaleDate < (TODAY - 60 days)
      2. Join with ServiceTickets table
      3. Filter WHERE ticket_count >= 2
      4. Analyze demographic patterns
    Tool Sequence: base_readQuery ‚Üí TDA_LLMTask
    Expected Cost: ~$0.85

Injecting champion case into context...
```

Your query matched a deployed pattern. The LLM receives:
- Your question
- A proven strategy for similar questions
- Expected tool sequence
- Cost estimate

Execution:
- Uses champion case guidance
- Adapts to your specific query (60 days, multiple tickets)
- Executes with proven approach
- Cost: $0.91 (within expected range)
- Result: Perfect

**THE MULTIPLICATION EFFECT:**

```
COMPARISON:

Traditional Approach:
‚îú‚îÄ You ask churn question
‚îú‚îÄ AI starts from scratch, experiments
‚îú‚îÄ Cost: $2.50 (trial and error)
‚îú‚îÄ Time: 45 seconds (multiple iterations)
‚îî‚îÄ Quality: 7/10 (decent but not optimal)

With Deployed Planner Repository:
‚îú‚îÄ You ask churn question
‚îú‚îÄ RAG finds "Customer Churn Risk Identification"
‚îú‚îÄ AI executes proven strategy immediately
‚îú‚îÄ Cost: $0.91 (64% savings)
‚îú‚îÄ Time: 18 seconds (single execution)
‚îî‚îÄ Quality: 9.5/10 (expert-level, first try)
```

Not just saving cost. Executing at expert level immediately.

You're not just saving cost. You're executing at expert level immediately.

**CONTRIBUTION CYCLE:**

```
ORGANIZATIONAL LEARNING AT AI SPEED:

1. You execute 10 queries this week
2. 3 are novel, high-quality results
3. You click "Create Template" for all 3
4. You publish to "Sales Analytics Accelerators"
5. 50 colleagues have that collection deployed
6. Tomorrow ‚Üí All 50 benefit from your work
7. They add their own successful patterns
8. Next week ‚Üí You benefit from their contributions
```

Every success ‚Üí Collective knowledge. Every pattern deployment ‚Üí Multiplies impact.

**ROI OF PLANNER REPOSITORIES:**

```
ORGANIZATION: 30 analysts, 1000 queries/month

WITHOUT PLANNER REPOSITORIES:
‚Ä¢ Avg cost per query: $2.80 (trial-and-error)
‚Ä¢ Monthly cost: $2,800
‚Ä¢ Redundant development: ~40% waste

WITH PLANNER REPOSITORIES:
‚Ä¢ Pattern reuse rate: 60% (600 queries)
‚Ä¢ Pattern-matched cost: $1.10 (60% savings)
‚Ä¢ Novel query cost: $2.80 (same)
‚Ä¢ Monthly cost: (600 √ó $1.10) + (400 √ó $2.80) = $1,780
‚Ä¢ Savings: $1,020/month (36% reduction)
```

Planner Repositories eliminate redundancy and multiply expertise.

From "starting from scratch" to "standing on organizational giants' shoulders."

Capture success. Templatize excellence. Deploy expertise. Multiply impact.
```

**Screen Capture Plan:**
- [ ] Successful query conversation with "Create Template" button visible
- [ ] Click "Create Template" ‚Üí Form auto-populated
- [ ] Template creation form showing:
  - Name: "Quarterly Revenue Analysis with Trend Comparison"
  - Description (multi-line)
  - Category: Financial Analysis
  - Tags: #revenue #quarterly-analysis #trending
  - Captured elements preview (query pattern, tool sequence)
  - Privacy level radio buttons (Public selected)
- [ ] Click "Save Template" ‚Üí Success confirmation
- [ ] Navigate to Marketplace ‚Üí Planner Repositories tab
- [ ] Collection cards showing:
  - Customer Insights Playbook (23 patterns, 4.8‚≠ê)
  - Database Performance Optimization (31 patterns, 4.9‚≠ê)
  - Sales Analytics Accelerators (17 patterns, 4.7‚≠ê)
- [ ] Click "Customer Insights Playbook" ‚Üí Collection detail view
- [ ] Pattern list showing 23 patterns organized by category
- [ ] Pattern preview card:
  - "Customer Churn Risk Identification"
  - Query template, tools, cost, deployments (142), rating (4.9‚≠ê)
- [ ] "Deploy to My Repository" button
- [ ] Deployment options modal:
  - Target Profile: @ANALYST dropdown
  - Mode: Subscribe vs Fork radio buttons
- [ ] Click "Deploy" ‚Üí Success confirmation
- [ ] Switch to @ANALYST profile
- [ ] Intelligence View ‚Üí Planner Repository tab
- [ ] Repository list showing:
  - Your templates (1)
  - Subscribed: Customer Insights Playbook (23)
  - Total: 24 patterns available
- [ ] New query: "Which customers haven't purchased in 60 days..."
- [ ] Live Status Panel ‚Üí RAG Retrieval section showing:
  - Pattern match: "Customer Churn Risk Identification" (0.94 similarity)
  - Champion case details injected
- [ ] Query execution with pattern guidance
- [ ] Cost comparison: $0.91 vs expected $0.85 (within range)
- [ ] ROI calculator showing 36% monthly savings for 30-analyst org

**Supporting Materials:**
- Planner repository architecture documentation
- Template creation best practices guide
- Pattern quality guidelines
- Deployment workflow technical specifications
- ROI calculator for planner repository adoption
- Contribution cycle and organizational learning analysis

---

### 6.3 Knowledge Repositories: Share Domain Expertise
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Upload documents to knowledge collections
- Publish knowledge for community access
- Deploy knowledge to your environment

**Key Topics to Cover:**
- [ ] Creating knowledge collections
- [ ] Document upload (PDF, TXT, DOCX, MD)
- [ ] Chunking strategies
- [ ] Publishing to marketplace
- [ ] Subscribing to expert knowledge

**Narration Script:**
```
Planner Repositories capture execution patterns‚Äîthe "how to do it."

But what about domain expertise? Product specifications? Compliance regulations? Technical documentation?

Knowledge Repositories. The second pillar of Uderia's marketplace‚Äîshareable, searchable, AI-readable domain knowledge.

**WHAT ARE KNOWLEDGE REPOSITORIES?**

Collections of uploaded documents‚Äîchunked, embedded, indexed for RAG retrieval. Your AI agents search them, reference them, answer questions using expert knowledge.

```
THINK OF THEM AS:
‚Ä¢ Reference libraries your AI reads instantly
‚Ä¢ Institutional memory that never forgets
‚Ä¢ Expert knowledge accessible to every user

DIFFERENCE FROM PLANNER REPOSITORIES:
Planners ‚Üí Execution patterns ("how to analyze churn")
Knowledge ‚Üí Domain facts ("what is churn, why, benchmarks")
```

Different architecture. Different purpose. Both essential.

**CREATING A KNOWLEDGE COLLECTION:**

Marketplace ‚Üí Knowledge Repositories ‚Üí "Create Collection."

```
COLLECTION SETUP:

Name: "Fitness Equipment Product Catalog"

Description: "Complete product specifications, pricing history, 
warranty terms, technical details for all fitness equipment."

Category: Product Knowledge
Tags: #products #specifications #warranty #pricing

Visibility:
‚óã Public (all users in org)
‚óã Unlisted (link only)
‚óã Private (only you)
```

Choose **Public**‚Äîbenefits entire sales and support team.

Click "Create Collection." Empty collection ready for documents.

**DOCUMENT UPLOAD:**
**DOCUMENT UPLOAD:**

Click "Upload Documents."

```
SUPPORTED FORMATS:
PDF | TXT | DOCX | MD | CSV (experimental)

YOUR DOCUMENTS:
1. product_catalog_2024.pdf (47 pages, product specs)
2. warranty_policies.docx (12 pages, terms)
3. pricing_history_q4.txt (pricing changes)

VALIDATION:
‚úì product_catalog_2024.pdf (2.3 MB, 47 pages)
‚úì warranty_policies.docx (487 KB, 12 pages)
‚úì pricing_history_q4.txt (23 KB, plain text)
```

All accepted. Click "Process Documents."
**CHUNKING CONFIGURATION:**

Before AI-searchable, documents need chunking‚Äîsplit large documents into smaller, semantically meaningful pieces.

Why?
- LLMs have context limits (can't ingest 47-page PDF in one shot)
- Retrieval precision improves (find exact paragraph, not entire document)
- Embedding efficiency (smaller chunks = better semantic matching)

```
CHUNKING STRATEGIES:

1. Fixed Size (Default)
   Size: 1000 chars | Overlap: 200 chars
   Best for: General documents, mixed content

2. Semantic Paragraphs
   Size: ~800 chars (variable) | By paragraph boundaries
   Best for: Well-structured documents

3. Hybrid (Recommended)
   Semantic paragraphs + max 1200 char limit | Overlap: 150
   Best for: Mixed documents
```

Choose **Hybrid**. Click "Start Chunking."

**PROCESSING STATUS:**

```
üìÑ DOCUMENT PROCESSING:

Doc 1: product_catalog_2024.pdf
‚îú‚îÄ Extracting... ‚úì (47 pages, 34,521 chars)
‚îú‚îÄ Chunking... ‚úì (89 chunks)
‚îú‚îÄ Embedding... ‚úì (text-embedding-ada-002)
‚îî‚îÄ Storing... ‚úì | COMPLETE (12s)

Doc 2: warranty_policies.docx
‚îú‚îÄ Extracting... ‚úì (12 pages, 8,943 chars)
‚îú‚îÄ Chunking... ‚úì (23 chunks)
‚îî‚îÄ Complete | 4s

Doc 3: pricing_history_q4.txt
‚îú‚îÄ Reading... ‚úì (1,876 chars)
‚îú‚îÄ Chunking... ‚úì (5 chunks)
‚îî‚îÄ Complete | 2s

üìä SUMMARY:
Total: 3 docs | 117 chunks | 45,340 chars
Avg chunk: 387 chars | Processing: 18s
Status: READY FOR USE
```

Collection live. 117 searchable chunks. Fully indexed. Ready for RAG retrieval.

**PUBLISHING TO MARKETPLACE:**

Now make it discoverable. Click "Publish to Marketplace."

**Publication Settings:**

**Visibility:** Public (already set)

**Featured Tags:** #products #specifications #warranty #pricing (already set)

**PUBLISHING TO MARKETPLACE:**

Make it discoverable. Click "Publish to Marketplace."

```
PUBLICATION SETTINGS:

Visibility: Public ‚úì
Tags: #products #specifications #warranty #pricing ‚úì

Purpose: "Enables sales and support teams to answer customer 
questions about product specs, warranty, pricing with AI accuracy."

Recommended For:
‚Ä¢ Sales Representatives (product questions)
‚Ä¢ Customer Support (warranty inquiries)
‚Ä¢ Marketing Team (pricing strategies)
```

Click "Publish." Done.

Knowledge Repository now live in marketplace. Anyone in organization can discover it, subscribe, use it.ks)
   - Author: Legal Department
   - Rating: 4.9‚≠ê (31 reviews)
**SUBSCRIBING TO COMMUNITY KNOWLEDGE:**

Consumer perspective. Marketplace ‚Üí Knowledge Repositories.

```
FEATURED COLLECTIONS:

1. GDPR Compliance Guidelines | üìÑ 47 chunks | 4.9‚≠ê
   Author: Legal Department | 127 deployments
   "Complete GDPR regulations, requirements, compliance checklists"

2. SQL Optimization Best Practices | üìÑ 89 chunks | 4.8‚≠ê
   Author: @DBA | 203 deployments
   "Teradata optimization, index strategies, 10 years DBA experience"

3. Fitness Equipment Product Catalog | üìÑ 117 chunks | NEW
   Author: You | 0 deployments
   "Product specs, pricing history, warranty terms..." ‚Üê YOUR COLLECTION
```

Click "SQL Optimization Best Practices" to explore.
- **Sample chunks:**
  - "When designing composite indexes, column order matters significantly. Place the 
    most selective columns first to maximize index efficiency..."
  - "COLLECT STATISTICS is not optional in Teradata. Without statistics, the optimizer 
    makes uninformed decisions leading to poor query plans..."

This is 10 years of DBA expertise, distilled into 89 searchable chunks. Available instantly.

**DEPLOYING KNOWLEDGE COLLECTION:**

Click "Deploy to My Repository."

```
DEPLOYMENT:

Target Profile: @DBA
Mode: Subscribe (auto-updates enabled)
```

Click "Deploy."

Activate @DBA ‚Üí Intelligence View ‚Üí Knowledge Repository:

```
SUBSCRIBED COLLECTIONS:
‚îú‚îÄ SQL Optimization Best Practices (89 chunks)
‚îî‚îÄ GDPR Compliance Guidelines (47 chunks)

Total available: 136 chunks
```

@DBA profile now has instant access to 136 expert knowledge chunks. Zero learning curve. Immediate expertise.

**USING DEPLOYED KNOWLEDGE:**

Let's test it. Ask:

"What's the best approach for indexing a table with 500 million rows?"

Watch the RAG retrieval:

```
üìö KNOWLEDGE RETRIEVAL:

Searching knowledge repositories for relevant information...

Matches found:

1. SQL Optimization Best Practices ‚Üí Chunk #34 (Similarity: 0.91)
   "For tables exceeding 100 million rows, consider partitioned primary indexes (PPI) 
   rather than non-partitioned. PPIs enable partition elimination, drastically reducing 
   I/O for time-series queries. Choose partition column based on most common query filters..."

2. SQL Optimization Best Practices ‚Üí Chunk #22 (Similarity: 0.87)
   "Index design for very large tables requires balancing insert performance with query 
   speed. Consider: (1) Fewer indexes = faster inserts, (2) Covering indexes for common 
   queries, (3) Index compression for space efficiency..."

3. SQL Optimization Best Practices ‚Üí Chunk #48 (Similarity: 0.82)
   "Common mistake: Over-indexing large tables. Each index adds overhead to INSERT/UPDATE 
   operations. Aim for 3-5 strategic indexes rather than 15+ reactive indexes..."

Injecting top 3 chunks into context...
```

The LLM receives:
- Your question
- Three expert knowledge chunks (PPI strategy, design tradeoffs, common mistakes)
- No generic advice‚Äîactual Teradata-specific expertise

Execution:
```
Based on the SQL Optimization Best Practices knowledge repository:

For a 500M row table, I recommend:

1. **Primary Index Strategy:**
   Use Partitioned Primary Index (PPI) rather than traditional NUPI. This enables 
   partition elimination for time-series queries, reducing I/O by 90%+.
   
   Choose partition column based on your most common filter (e.g., SaleDate for sales data).

2. **Secondary Indexes (Limit to 3-5 strategic indexes):**
   - Covering index for most frequent query pattern
   - Index on foreign key columns for join optimization
   - Consider compressed indexes to save space

3. **Avoid Over-Indexing:**
   Each index adds 15-20% overhead to INSERT operations. With 500M rows, this compounds 
   significantly. Focus on high-impact indexes only.

4. **Statistics Collection:**
   Run COLLECT STATISTICS on indexed columns monthly. Without stats, optimizer makes 
   poor decisions regardless of index quality.

Would you like me to generate the DDL for a PPI table with recommended indexes?
```

This isn't generic AI advice. This is expert DBA knowledge, retrieved from the deployed 
collection, applied to your specific question.

**THE KNOWLEDGE MULTIPLICATION:**

```
WITHOUT KNOWLEDGE REPOSITORY:
‚îú‚îÄ Junior DBA asks indexing question
‚îú‚îÄ AI gives generic advice ("add indexes on queried columns")
‚îú‚îÄ Quality: 5/10 (not wrong, not expert)
‚îî‚îÄ Implements mediocre solution

WITH KNOWLEDGE REPOSITORY:
‚îú‚îÄ Junior DBA asks same question
‚îú‚îÄ RAG retrieves expert DBA knowledge chunks
‚îú‚îÄ AI gives Teradata-specific, battle-tested advice
‚îú‚îÄ Quality: 9/10 (expert-level, first try)
‚îî‚îÄ Implements optimal solution
```

**ROI OF KNOWLEDGE REPOSITORIES:**

```
TRADITIONAL APPROACH:
‚îú‚îÄ Expert DBA: 10 years learning optimization
‚îú‚îÄ Knowledge resides in expert's head
‚îú‚îÄ Junior DBAs ask repeatedly (bottleneck)
‚îî‚îÄ Expert leaves ‚Üí Knowledge lost

KNOWLEDGE REPOSITORY APPROACH:
‚îú‚îÄ Expert documents once (8 hours)
‚îú‚îÄ Publishes to marketplace (89 chunks)
‚îú‚îÄ 50 users deploy instantly
‚îú‚îÄ Every user: 10 years expertise immediately
‚îî‚îÄ Expert leaves ‚Üí Knowledge persists
```

From knowledge locked in experts' heads to knowledge accessible to everyone, instantly, forever.

Capture expertise. Chunk and embed. Publish and deploy. Multiply knowledge., instantly, forever.

Capture expertise. Chunk and embed. Publish and deploy. Multiply knowledge.
```

**Screen Capture Plan:**
- [ ] Marketplace ‚Üí Knowledge Repositories ‚Üí "Create Collection" button
- [ ] Collection setup form:
  - Name: "Fitness Equipment Product Catalog"
  - Description (multi-line)
  - Category: Product Knowledge
  - Tags: #products #specifications #warranty
  - Visibility: Public radio selected
- [ ] Click "Create Collection" ‚Üí Empty collection created
- [ ] "Upload Documents" button ‚Üí Upload interface
- [ ] Supported formats list (PDF, TXT, DOCX, MD, CSV)
- [ ] Drag-and-drop three files:
  - product_catalog_2024.pdf (2.3 MB, 47 pages) ‚úì
  - warranty_policies.docx (487 KB, 12 pages) ‚úì
  - pricing_history_q4.txt (23 KB) ‚úì
- [ ] "Process Documents" button
- [ ] Chunking strategy options modal:
  - Fixed Size (1000 chars, 200 overlap)
  - Semantic Paragraphs (variable)
  - Hybrid (recommended) ‚Üê selected
- [ ] "Start Chunking" ‚Üí Processing status panel
- [ ] Processing progress showing:
  - Document 1: Extracting ‚Üí Chunking (89 chunks) ‚Üí Embedding ‚Üí Complete
  - Document 2: 23 chunks ‚Üí Complete
  - Document 3: 5 chunks ‚Üí Complete
  - Summary: 3 docs, 117 chunks, 18 seconds
- [ ] "Publish to Marketplace" button
- [ ] Publication settings:
  - Visibility: Public
  - Tags: (shown)
  - Collection Purpose (multi-line)
  - Recommended For (bullet list)
- [ ] Click "Publish" ‚Üí Success confirmation
- [ ] Marketplace ‚Üí Knowledge Repositories ‚Üí Featured collections
- [ ] Collection cards showing:
  - GDPR Compliance Guidelines (47 chunks, 4.9‚≠ê, 127 deployments)
  - SQL Optimization Best Practices (89 chunks, 4.8‚≠ê, 203 deployments)
  - Fitness Equipment Product Catalog (117 chunks, NEW, 0 deployments)
- [ ] Click "SQL Optimization Best Practices" ‚Üí Collection preview
- [ ] Document sources list (5 files, chunk counts)
- [ ] Sample chunks displayed with text previews
- [ ] "Deploy to My Repository" button ‚Üí Deployment modal
- [ ] Target Profile: @DBA, Mode: Subscribe
- [ ] Click "Deploy" ‚Üí Success
- [ ] Switch to @DBA profile ‚Üí Intelligence View ‚Üí Knowledge Repository tab
- [ ] Subscribed collections:
  - SQL Optimization (89 chunks)
  - GDPR Compliance (47 chunks)
  - Total: 136 chunks
- [ ] New query: "What's the best approach for indexing a table with 500M rows?"
- [ ] Live Status Panel ‚Üí Knowledge Retrieval section
- [ ] 3 chunk matches displayed:
  - Chunk #34 (0.91 similarity) - PPI recommendation
  - Chunk #22 (0.87) - Design tradeoffs
  - Chunk #48 (0.82) - Over-indexing warning
- [ ] AI response using injected knowledge chunks
- [ ] Expert-level advice (PPI strategy, 3-5 indexes, statistics) displayed

**Supporting Materials:**
- Knowledge repository architecture documentation
- Document upload and processing guide
- Chunking strategies technical reference
- Embedding model specifications (text-embedding-ada-002)
- RAG retrieval algorithm documentation
- Knowledge quality best practices
- ROI analysis for knowledge repository adoption

---

### 6.4 Subscribe, Fork, Rate: Community Ecosystem
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Subscribe to collections (reference-based)
- Fork collections for customization
- Rate and review for quality assurance

**Key Topics to Cover:**
- [ ] Subscribe workflow (no data duplication)
- [ ] Fork workflow (independent copy)
- [ ] Rating system (1-5 stars)
- [ ] Publishing options (Public, Unlisted, Private)
- [ ] Community quality assurance

**Narration Script:**
```
Marketplaces don't work without ecosystems.

Consume content (Subscribe). Customize it (Fork). Ensure quality (Rate).

These three mechanisms transform Uderia's marketplace from "file dump" to "living, quality-assured community."

**SUBSCRIBE: REFERENCE-BASED DEPLOYMENT**

You discovered a great planner collection: "Customer Analytics Playbook" (45 patterns, 4.9‚≠ê).

Click "Deploy to My Repository" ‚Üí Choose **Subscribe** mode.

What happens?

**Technical Architecture:**
- Uderia creates a REFERENCE in your profile (not a copy)
- Your profile points to the original collection in the author's repository
- When you query, RAG searches both your patterns AND subscribed collections
- No data duplication‚Äîsingle source of truth

**Benefits of Subscribe:**

‚úì **Auto-Updates:** Author adds new patterns? You get them immediately (no re-deployment)
‚úì **Space Efficiency:** No storage duplication (one 45-pattern collection, infinite subscribers)
‚úì **Quality Assurance:** Author improves a pattern? You benefit instantly
‚úì **Author Attribution:** Usage stats flow back to author (recognition for contributions)

**Use Subscribe When:**
- You trust the author to maintain quality
- You want automatic updates
- The collection serves your needs as-is
- You're consuming standard organizational knowledge

**Example Workflow:**

Day 1: You subscribe to "Customer Analytics Playbook" (45 patterns)
Day 15: Author adds "Customer Lifetime Value Prediction" pattern (now 46 patterns)
Day 15 (30 seconds later): You automatically have access to new pattern. Zero action required.

This is live, continuous knowledge sharing.

**FORK: INDEPENDENT COPY**

"Sales Forecasting Templates" (23 patterns, 4.7‚≠ê).

Good starting point. Customize for your industry? "Deploy to My Repository" ‚Üí **Fork**.

```
TECHNICAL ARCHITECTURE:
‚îú‚îÄ Uderia creates COMPLETE COPY in your repository
‚îú‚îÄ Your fork is independent (no link to author)
‚îú‚îÄ Modify, delete, add patterns freely
‚îú‚îÄ Changes don't affect original
‚îî‚îÄ Original updates don't propagate to fork

BENEFITS:
‚úì Full Control (modify patterns to fit your needs)
‚úì Experimentation (test changes without affecting others)
‚úì Specialization (adapt for industry-specific requirements)
‚úì Independence (no dependency on author)

USE FORK WHEN:
‚Ä¢ Need to customize patterns heavily
‚Ä¢ Adapting for specific industry/use case
‚Ä¢ Want frozen version (no surprise updates)
‚Ä¢ Plan to extend significantly
```

Example: Fork (23 patterns) ‚Üí Delete 5 (fitness irrelevant) ‚Üí Modify 10 (fitness_db tables) ‚Üí Add 8 (seasonality forecasting) ‚Üí "Fitness Industry Forecasting Playbook" (26 patterns, customized).

Original unchanged. Your fork tailored perfectly.

**SUBSCRIBE VS FORK DECISION MATRIX:**

| Scenario | Subscribe | Fork |
|----------|-----------|------|
| Standard organizational knowledge | ‚úì | |
| Trust author to maintain quality | ‚úì | |
| Want automatic updates | ‚úì | |
| Need heavy customization | | ‚úì |
| Industry-specific adaptation | | ‚úì |
| Want frozen version | | ‚úì |
| Experimental changes | | ‚úì |
| Minimal storage duplication | ‚úì | |

Think of it like software:
- **Subscribe** = Using npm package (linked, updates automatically)
- **Fork** = Copying code into your repo (independent, you control changes)

**RATE & REVIEW: QUALITY ASSURANCE**

You've been using "Customer Analytics Playbook" for two weeks. It's excellent. Time to 
rate it.

Go to Marketplace ‚Üí Find the collection ‚Üí Click "Rate & Review."

**Rating Interface:**

**Overall Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5 stars)

**Category Ratings:**
- **Accuracy:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Patterns execute correctly every time)
- **Usefulness:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Covers all common customer analysis scenarios)
- **Documentation:** ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ (Good descriptions, could use more examples)
- **Cost Efficiency:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Patterns well-optimized, no wasteful queries)

**Written Review:**
"Deployed this to @ANALYST profile two weeks ago. Has saved me hours of query development. 
The 'Customer Churn Risk' pattern alone has been used 34 times with perfect results every 
time. Highly recommend for any analytics team."

**Usage Stats (Automatically Included):**
- Deployed to: @ANALYST profile
- Patterns used: 18 of 45 (40% adoption)
- Total executions: 67 times
- Success rate: 98.5% (66 successful, 1 failed due to missing table)
- Avg cost savings: $1.85 per pattern use

Click "Submit Review."

Your review is now visible in the marketplace. Other users can see:
- Your rating (5‚≠ê)
- Your specific category feedback
- Your usage experience
- Your empirical results (67 executions, 98.5% success)

This isn't subjective opinion. It's data-backed quality assurance.

**HOW RATINGS AFFECT MARKETPLACE:**

Collection: "Customer Analytics Playbook"
- **Before your review:** 4.8‚≠ê (67 reviews)
- **After your review:** 4.8‚≠ê (68 reviews) ‚Üê Your 5‚≠ê maintains high average

**Marketplace Sorting:**

When users browse, they see:
- **Top Rated** (4.5‚≠ê+): Your review helps maintain this collection's featured status
- **Most Used** (deployments): Your deployment adds to popularity
- **Trending** (recent activity): Your review signals active community engagement

High ratings = More visibility = More deployments = More feedback = Quality reinforcement cycle.

**LOW RATINGS = QUALITY SIGNAL:**

Not all collections are great. You try "Database Quick Fixes" (3.2‚≠ê, 12 reviews).

Result: Patterns fail 40% of the time. Outdated SQL syntax. Poor documentation.

You rate it:
- Overall: ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ (2 stars)
- Accuracy: ‚≠ê‚òÜ‚òÜ‚òÜ‚òÜ (Multiple patterns failed)
- Usefulness: ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ (Concepts good, execution poor)
- Documentation: ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ (Insufficient examples)

Review: "Attempted 5 patterns, 2 failed with SQL syntax errors. Appears unmaintained. 
Recommend avoiding until author updates."

This review:
- Warns other users (saves them wasted time)
- Signals to author (feedback for improvement)
- Triggers marketplace moderators (if rating drops below 2.5‚≠ê, admin review)

Community-driven quality control. Bad collections get flagged. Good collections get promoted.

**VISIBILITY SETTINGS: PUBLISHING OPTIONS**

When you create a collection, you choose visibility:

**Public (Searchable):**
- Appears in marketplace browse/search
- Anyone in organization can discover
- Eligible for "Top Rated" and "Trending" features
- Use for: Broadly useful, production-ready collections

**Unlisted (Link-Only):**
- Does NOT appear in marketplace search
- Accessible only via direct link
- You control who can access (share link selectively)
- Use for: Beta testing, team-specific collections, works-in-progress

**Private (You Only):**
- Completely invisible to others
- Only creator can access
- Not deployable by anyone else
- Use for: Personal experiments, sensitive content, drafts

**Workflow Example:**

Week 1: Create "Advanced Cohort Analysis" collection ‚Üí Set to **Private** (testing)
Week 2: Validate patterns with your team ‚Üí Change to **Unlisted** (share link with team)
Week 3: Team validates quality ‚Üí Change to **Public** (marketplace launch)
Week 4: Community deploys, rates, reviews ‚Üí Becomes **Top Rated** (4.9‚≠ê)

Progressive publication. Quality gates at every stage.

**TOP-RATED COLLECTIONS: QUALITY SHOWCASE**

Go to Marketplace ‚Üí Filter by "Top Rated" (4.5‚≠ê+).

You see the organizational hall of fame:

1. **Database Performance Optimization** (4.9‚≠ê, 203 deployments)
   - 31 patterns for query tuning, index strategies, statistics
   - 98.7% success rate across 4,521 executions
   - Author: @DBA profile (senior DBA team)

2. **Customer Analytics Playbook** (4.8‚≠ê, 189 deployments)
   - 45 patterns for churn, segmentation, lifetime value
   - 97.2% success rate across 3,847 executions
   - Author: @ANALYST profile (business intelligence team)

3. **GDPR Compliance Knowledge Base** (4.9‚≠ê, 127 deployments)
   - 47 document chunks covering regulations, requirements, checklists
   - Used in 2,143 queries
   - Author: Legal Department

These collections are battle-tested, community-validated, production-ready. Deploy with confidence.

**THE ECOSYSTEM IN ACTION:**

**Month 1:** 10 users, 5 collections, 23 deployments
**Month 3:** 50 users, 28 collections, 187 deployments
**Month 6:** 100 users, 67 collections, 892 deployments
**Month 12:** 200 users, 134 collections, 3,456 deployments

**Network Effect:**
- More users ‚Üí More collections
- More collections ‚Üí More deployments
- More deployments ‚Üí More ratings/reviews
- More ratings ‚Üí Better quality signals
- Better quality ‚Üí More user trust
- More trust ‚Üí More contributions

The marketplace becomes self-sustaining, self-improving, self-governing.

**ROI OF COMMUNITY ECOSYSTEM:**

**Without Marketplace:**
- Knowledge siloed in individuals
- Redundant pattern development
- No quality assurance
- Expertise lost when employees leave

**With Marketplace + Subscribe/Fork/Rate:**
- Knowledge shared organization-wide
- Pattern reuse reduces costs by 40%
- Community ratings ensure quality
- Expertise becomes institutional asset

From isolated experts to collective intelligence. From redundant work to efficient reuse. 
From unknown quality to community-validated excellence.

Subscribe for convenience. Fork for customization. Rate for quality. Build the ecosystem.
```

**Screen Capture Plan:**
- [ ] Collection detail page with "Deploy to My Repository" button
- [ ] Deployment modal showing Subscribe vs Fork options:
  - Subscribe (radio selected): "Reference-based, auto-updates, space-efficient"
  - Fork: "Independent copy, full control, no auto-updates"
- [ ] Select Subscribe ‚Üí Click Deploy ‚Üí Success notification
- [ ] Profile Intelligence View showing subscribed collection (reference icon)
- [ ] Subscribe vs Fork decision matrix table visualization
- [ ] Two weeks later: Author adds new pattern
- [ ] Your profile automatically shows new pattern count (45‚Üí46)
- [ ] Same collection, now choose Fork
- [ ] Fork creation showing:
  - "Creating independent copy..."
  - Copy complete: 23 patterns cloned
  - "You can now modify freely"
- [ ] Forked collection in your repository (edit mode available)
- [ ] Modify pattern example (delete 2, edit 3, add 1)
- [ ] Result: Your customized version (22 patterns, modified)
- [ ] "Rate & Review" interface:
  - Overall rating: 5 star selector
  - Category ratings: Accuracy, Usefulness, Documentation, Cost Efficiency
  - Written review text box
  - Usage stats auto-populated (67 executions, 98.5% success)
- [ ] Submit review ‚Üí Success confirmation
- [ ] Collection page updated showing your review
- [ ] Rating distribution: 68 reviews, 4.8‚≠ê average
- [ ] Low-rated collection example (3.2‚≠ê)
- [ ] Poor review with specific failures documented
- [ ] Warning badge: "Below 3.5‚≠ê - Use with caution"
- [ ] Visibility settings interface:
  - Public radio ("Searchable, discoverable")
  - Unlisted radio ("Link-only access")
  - Private radio ("You only")
- [ ] Progressive publication workflow diagram:
  - Week 1: Private (testing)
  - Week 2: Unlisted (team validation)
  - Week 3: Public (marketplace launch)
  - Week 4: Top Rated (community validation)
- [ ] Marketplace filtered by "Top Rated"
- [ ] Three featured collections with 4.8-4.9‚≠ê ratings
- [ ] Collection stats: Deployments, executions, success rates
- [ ] Ecosystem growth chart:
  - X-axis: Months 1-12
  - Y-axis: Collections, deployments, users
  - Exponential growth curves
- [ ] Network effect visualization diagram
- [ ] ROI comparison: Siloed vs Marketplace ecosystem

**Supporting Materials:**
- Subscribe vs Fork technical architecture documentation
- Reference-based deployment specifications
- Rating system algorithm and quality scoring
- Community moderation guidelines
- Visibility and publication workflow guide
- Network effects analysis and ROI projections
- Marketplace governance policies

---

## Module 7: FINANCIAL GOVERNANCE
**Goal:** Complete transparency and control over LLM spending

### 7.1 Consumption Profiles & Rate Limiting
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Understand consumption profile tiers
- Configure rate limits and quotas
- Monitor usage enforcement

**Key Topics to Cover:**
- [ ] Four profile tiers (Free, Pro, Enterprise, Unlimited)
- [ ] Per-user rate limits (hourly/daily prompts)
- [ ] Monthly token quotas
- [ ] Configuration change limits
- [ ] Admin bypass capabilities
- [ ] Real-time enforcement

**Narration Script:**
```
LLM costs spiral out of control fast.

One user runs 500 queries in a day experimenting. Another accidentally creates an infinite loop. A third trains a complex RAG case consuming $200 in tokens.

Without governance, your LLM bill becomes unpredictable.

Uderia solves this with Consumption Profiles‚Äîfine-grained rate limiting and quota management at the user level.

**THE FOUR CONSUMPTION TIERS:**

Admin ‚Üí User Management ‚Üí Consumption Profiles. Four tiers:

```
TIER 1: FREE (Evaluation Users)
‚îú‚îÄ Hourly: 10 prompts
‚îú‚îÄ Daily: 50 prompts
‚îú‚îÄ Monthly: 100K tokens (~$0.15 ceiling)
‚îú‚îÄ Config Changes: 3/day
‚îî‚îÄ Use: Trial users, demos, proof-of-concept

TIER 2: PRO (Individual Contributors)
‚îú‚îÄ Hourly: 30 prompts
‚îú‚îÄ Daily: 200 prompts
‚îú‚îÄ Monthly: 1M tokens (~$1.50 ceiling)
‚îú‚îÄ Config Changes: 10/day
‚îî‚îÄ Use: Analysts, developers, standard users

TIER 3: ENTERPRISE (Power Users)
‚îú‚îÄ Hourly: 100 prompts
‚îú‚îÄ Daily: 1,000 prompts
‚îú‚îÄ Monthly: 10M tokens (~$15.00 ceiling)
‚îú‚îÄ Config Changes: 50/day
‚îî‚îÄ Use: Data scientists, DBAs, automation

TIER 4: UNLIMITED (Administrators & Production)
‚îú‚îÄ Hourly: None
‚îú‚îÄ Daily: None
‚îú‚îÄ Monthly: None
‚îú‚îÄ Config Changes: Unlimited
‚îî‚îÄ Use: Admins, CI/CD, production API
```

Each tier balances access with cost control. Most start at PRO. Power users ‚Üí ENTERPRISE. Admins ‚Üí UNLIMITED.

**ASSIGNING CONSUMPTION PROFILES:**
**ASSIGNING CONSUMPTION PROFILES:**

Click user: alice@company.com

```
USER DETAILS:

Name: Alice Johnson
Role: User (standard permissions)
Current Profile: PRO (1M tokens/month)

Usage This Month:
‚îú‚îÄ Tokens: 347,521 / 1,000,000 (34.8%)
‚îú‚îÄ Prompts Today: 23 / 200 (11.5%)
‚îî‚îÄ Status: ‚úì Within limits
```

Alice operating normally. Needs more capacity for special project?

"Edit Profile" ‚Üí Change to **ENTERPRISE** ‚Üí Save.

Alice now has 10M tokens/month (10x increase), 1,000 prompts/day, 100 prompts/hour.

**RATE LIMIT ENFORCEMENT IN ACTION:**

Test user: bob@company.com with **FREE** profile (10/hour, 50/day).

Bob starts querying:

```
QUERY SEQUENCE:

Prompt 1: "Show me Q4 sales by region" ‚úì (1/10)
Prompt 2: "Break down by product category" ‚úì (2/10)
Prompt 3: "Add customer segmentation" ‚úì (3/10)
...
Prompt 10: "Generate executive summary" ‚úì (10/10)
Prompt 11: "Create visualization chart" ‚úó RATE LIMITED
```

Bob hits hourly limit. Uderia responds:

```
‚ö†Ô∏è RATE LIMIT EXCEEDED

You have reached your hourly limit (10 prompts).

Profile: FREE | Next reset: 42 minutes

To increase limits, contact admin or upgrade to PRO 
(200 prompts/day, 1M tokens/month).
```

Bob can't query for 42 minutes. System enforces automatically. No manual policing.r 42 minutes. The system enforces limits automatically. No manual 
policing required.

**QUOTA EXHAUSTION:**

alice@company.com (PRO profile, 1M tokens/month) at month-end:

```
USAGE STATS:
Tokens Used: 987,432 / 1,000,000 (98.7%)
Remaining: 12,568 tokens (~19 prompts left)
```

Alice runs complex query:

```
QUERY VALIDATION:

Query: "Analyze 500K customer records, identify churn 
        patterns, generate report"

Estimated: 23,450 tokens
Available: 12,568 tokens
Result: ‚úó INSUFFICIENT QUOTA
```

Uderia blocks:

```
‚ö†Ô∏è MONTHLY QUOTA EXCEEDED

This query requires 23,450 tokens.
You have 12,568 tokens remaining.

Profile: PRO (1M tokens/month)
Quota resets: December 1, 2025 (3 days)

Options:
1. Wait for reset (3 days)
2. Contact admin for quota increase
3. Simplify query to fit remaining tokens
```

Alice can't run expensive queries. Waits for reset or requests upgrade.

Financial governance in action‚Äîpreventing runaway costs before they happen.

**CONFIGURATION CHANGE LIMITS:**

Subtle but important control. FREE profile: 3 configuration changes/day. Why?

```
SCENARIO WITHOUT LIMITS:
‚îú‚îÄ User creates 10 profiles (different LLM providers)
‚îú‚îÄ Switches between profiles 100 times/day
‚îú‚îÄ Each switch triggers LLM calls for initialization
‚îî‚îÄ Result: Excessive overhead, wasted tokens
```

Configuration limit prevents abuse. Bob (FREE) tries:

```
PROFILE SWITCHES:

Change 1: @ANALYST ‚Üí @DBA ‚úì (1/3)
Change 2: @DBA ‚Üí @QUALITY ‚úì (2/3)
Change 3: @QUALITY ‚Üí @LOCAL ‚úì (3/3)
Change 4: @LOCAL ‚Üí @PROD ‚úó BLOCKED
```

Limit reached:

```
‚ö†Ô∏è CONFIGURATION CHANGE LIMIT REACHED

You have made 3 profile switches today (FREE tier limit).

Next reset: 6 hours

Upgrade to PRO (10 changes/day) or ENTERPRISE (50 changes/day).
```

Can't profile-hop excessively. Prevents abuse and accidental overhead.
**ADMIN BYPASS & MONITORING:**

Admins have UNLIMITED profile by default:

```
ADMIN CAPABILITIES:
‚Ä¢ No rate limits (query 1000 times/hour)
‚Ä¢ No quota restrictions (unlimited tokens)
‚Ä¢ No configuration limits (switch profiles freely)

WHY:
‚Ä¢ Troubleshooting access (testing for users)
‚Ä¢ Maintenance operations (bulk queries, validation)
‚Ä¢ Emergency overrides (time-sensitive analysis)
```

Admins monitor global usage. Admin ‚Üí Consumption Analytics:

```
GLOBAL USAGE SUMMARY:

Total Users: 127
Active This Month: 89 (70%)
Total Tokens: 34.2M tokens
Total Cost: $51.30
Avg Cost Per Active User: $0.58

USERS NEAR QUOTA (Top 5):
1. alice@company.com (PRO): 987K/1M (98.7%)
2. sarah@company.com (ENTERPRISE): 9.2M/10M (92%)
3. mike@company.com (PRO): 856K/1M (85.6%)

RATE LIMIT VIOLATIONS (Last 24h):
1. bob@company.com (FREE): 12 violations
2. carol@company.com (PRO): 3 violations
```

Admins can:
**EMERGENCY QUOTA OVERRIDE:**

Alice hits monthly quota but has urgent Q4 board presentation tomorrow.

Admin action:

```
EMERGENCY QUOTA GRANT:

1. User Management ‚Üí alice@company.com
2. Click "Grant Emergency Quota"
3. Add: 500,000 tokens (temporary extension)
4. Expiration: 48 hours
5. Save
```

Alice now has:

```
QUOTA BREAKDOWN:
‚îú‚îÄ Base Quota: 1M tokens (exhausted)
‚îú‚îÄ Emergency Extension: +500K tokens (granted)
‚îî‚îÄ Total Available: 500K tokens for 48 hours
```

After 48 hours ‚Üí Emergency quota expires. Monthly reset ‚Üí Replenishes base quota.

Controlled flexibility‚Äîgovernance with escape hatches for legitimate needs.

**ROI OF CONSUMPTION PROFILES:**

```
ORGANIZATION: 100 users, NO GOVERNANCE

10 power users √ó $50/month = $500
30 moderate users √ó $15/month = $450
60 light users √ó $3/month = $180
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: $1,130/month

Issue: No visibility, no control, cost creep
```

```
ORGANIZATION: 100 users, WITH PROFILES

10 power users ‚Üí ENTERPRISE ($15 cap) = $150
30 moderate users ‚Üí PRO ($1.50 cap) = $45
60 light users ‚Üí FREE ($0.15 cap) = $9
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: $204/month

Savings: $926/month (82% reduction)
```

Consumption profiles don't just track costs. They eliminate runaway spending.

From "unpredictable bills" to "controlled, predictable LLM budgets."

From "after-the-fact shock" to "proactive governance."

Rate limits. Quotas. Visibility. Control. Financial governance.
```

**Screen Capture Plan:**
- [ ] Admin ‚Üí User Management ‚Üí Consumption Profiles
- [ ] Four tier cards displayed:
  - FREE (10/hr, 50/day, 100K tokens/month)
  - PRO (30/hr, 200/day, 1M tokens/month)
  - ENTERPRISE (100/hr, 1000/day, 10M tokens/month)
  - UNLIMITED (no limits)
- [ ] User details for alice@company.com:
  - Current: PRO profile
  - Usage: 347,521/1,000,000 tokens (34.8%)
  - Prompts today: 23/200
  - Status: ‚úì Within limits
- [ ] Edit profile ‚Üí Change to ENTERPRISE ‚Üí Save
- [ ] Confirmation: Limits updated instantly
- [ ] Test user: bob@company.com (FREE profile)
- [ ] Query sequence showing rate limit hit:
  - Prompts 1-10: ‚úì Success
  - Prompt 11: ‚úó Rate limited
- [ ] Rate limit error message:
  - "You have reached your hourly limit (10 prompts)"
  - "Next reset: 42 minutes"
  - Upgrade suggestion
- [ ] alice@company.com month-end:
  - Usage: 987,432/1,000,000 tokens (98.7%)
  - Remaining: 12,568 tokens
- [ ] Complex query attempt:
  - Required: 23,450 tokens
  - Available: 12,568 tokens
  - Result: ‚úó Insufficient quota
- [ ] Quota exceeded error with options:
  - Wait for reset (3 days)
  - Contact admin
  - Simplify query
- [ ] Configuration change limit demonstration:
  - bob (FREE): 3 switches ‚úì, 4th switch ‚úó blocked
  - Error: "Configuration change limit reached"
- [ ] Admin ‚Üí Consumption Analytics dashboard:
  - Global usage: 127 users, 34.2M tokens, $51.30
  - Users near quota (top 5 list)
  - Rate limit violations (last 24h)
- [ ] Emergency quota override interface:
  - alice@company.com selected
  - "Grant Emergency Quota" button
  - Add 500K tokens, 48hr expiration
  - Save ‚Üí Emergency extension granted
- [ ] ROI comparison chart:
  - No governance: $1,130/month
  - With profiles: $204/month
  - Savings: $926/month (82%)

**Supporting Materials:**
- Consumption profiles documentation
- Rate limiting technical specifications
- Quota management guide
- Admin override procedures
- Cost control best practices
- ROI calculator for consumption governance

---

### 7.2 Cost Analytics & Optimization
**Duration:** 4-5 minutes  
**Learning Objectives:**
- Analyze cost distribution by provider
- Identify most expensive models and queries
- Explore cost trends and optimization opportunities

**Key Topics to Cover:**
- [ ] Cost distribution by provider chart
- [ ] Top 5 most expensive models
- [ ] 30-day cost trend visualization
- [ ] Most expensive sessions/queries
- [ ] RAG efficiency savings metrics
- [ ] Optimization recommendations

**Narration Script:**
```
Consumption profiles control spending. But how do you optimize what you're already spending?

Where are inefficiencies? Which models cost most? Which queries need redesign?

Uderia's Cost Analytics dashboard answers with forensic precision.

**COST DISTRIBUTION BY PROVIDER:**

Admin ‚Üí Cost Analytics ‚Üí Provider Breakdown. Pie chart:

```
PROVIDER COST DISTRIBUTION (Last 30 Days):

Anthropic (Claude): $127.45 (48.2%) ‚Äî 342,891 tokens
Google (Gemini): $78.32 (29.6%) ‚Äî 5,234,782 tokens
OpenAI (GPT-4): $43.21 (16.3%) ‚Äî 87,432 tokens
AWS Bedrock: $12.18 (4.6%) ‚Äî 102,341 tokens
Ollama (Local): $3.45 (1.3%) ‚Äî Infrastructure only
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: $264.61 for 5.77M tokens
```

Story:

**Anthropic dominates cost (48%)** ‚Äî Claude Sonnet heavily used. Only 342K tokens = high per-token cost. Premium model, premium price.

**Google delivers volume (5.2M tokens)** ‚Äî Low cost (29.6% spend). Gemini Flash = workhorse for high-volume, low-complexity queries.

**OpenAI expensive per token** ‚Äî $43 for 87K tokens. GPT-4 costs $0.49/execution average. Use sparingly for complex reasoning only.

**Ollama nearly free** ‚Äî $3.45 infrastructure, zero API cost. Only 102K tokens. Opportunity: Shift workload to local models.

Breakdown reveals optimization opportunities immediately.

**TOP 5 MOST EXPENSIVE MODELS:**

Click "Model Breakdown" tab. Ranked by total cost:

```
1. Claude Sonnet 3.5 (Anthropic)
   Cost: $89.34 | Tokens: 241,203 | Avg: $0.37/query
   Usage: 243 queries | Profile: @DBA, @QUALITY

2. Gemini 1.5 Flash (Google)
   Cost: $78.32 | Tokens: 5,234,782 | Avg: $0.015/query
   Usage: 5,221 queries | Profile: @ANALYST, @COST

3. GPT-4o (OpenAI)
   Cost: $43.21 | Tokens: 87,432 | Avg: $0.49/query
   Usage: 88 queries | Profile: @PROD (rarely)

4. Claude Opus 3 (Anthropic)
   Cost: $38.11 | Tokens: 101,688 | Avg: $0.95/query
   Usage: 40 queries | Profile: Custom (special projects)

5. Claude Sonnet 3.5 v2 (AWS Bedrock)
   Cost: $12.18 | Tokens: 102,341 | Avg: $0.12/query
   Usage: 101 queries | Profile: @PROD (Bedrock routing)
```

**Key Insight:** Claude Opus ($0.95/query) + GPT-4o ($0.49/query) = only 128 queries. But represent $81 of $264 spend (30.6%).

Optimization: Reserve Opus/GPT-4 for complex reasoning. Use Sonnet or Gemini otherwise. Potential savings: $40-50/month.

**30-DAY COST TREND:**

Click "Trend Analysis" tab. Line chart:

```
DAILY COST (Last 30 Days):

Nov 7: $4.23
Nov 8: $6.45
Nov 9: $5.87
...
Nov 20: $12.34 ‚Üê Spike!
Nov 21: $11.89
Nov 22: $14.56 ‚Üê Another spike!
...
Dec 5: $7.23
Dec 6: $8.91
Dec 7: $9.12 (today)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Avg Daily: $8.82
Projected Monthly: $264.60
```

Spikes on Nov 20-22 ($12-14/day vs $8 average). Click Nov 20 spike:

```
HIGH-COST SESSIONS (Nov 20):

1. sarah@company.com | Q4_Board_Prep
   Cost: $4.87 | Queries: 23

2. mike@company.com | Customer_Analysis
   Cost: $3.21 | Queries: 47

3. alice@company.com | Performance_Tuning
   Cost: $2.45 | Queries: 12
```

Sarah: 23 expensive queries for Q4 board presentation. Legitimate use case, one-time event.

Mike: 47 queries on customer analysis. Might be inefficient‚Äîexploring without clear plan.

**Optimization Opportunity:** Provide Mike with pre-built planner templates for customer analysis. Reduce trial-and-error, cut costs.

**MOST EXPENSIVE SESSIONS:**

Click "Expensive Sessions" tab. Sorted by cost:

```
TOP 10 COSTLIEST SESSIONS (Last 30 Days):

1. Q4_Board_Presentation | sarah@company.com
   Cost: $18.45 | Queries: 67 | Avg: $0.28/query
   Models: Opus (15), Sonnet (52)
   ‚úì High-stakes analysis, appropriate selection

2. Database_Migration_Planning | mike@company.com
   Cost: $14.87 | Queries: 31 | Avg: $0.48/query
   Models: GPT-4o (23), Opus (8)
   ‚úì Complex reasoning justified premium models

3. Customer_Churn_Exploration | alice@company.com
   Cost: $11.23 | Queries: 89 | Avg: $0.13/query
   Models: Sonnet (67), Flash (22)
   ‚ö† Good mix, but 89 queries exploratory
   ‚Üí Deploy "Customer Insights Playbook"

4. Product_Performance_Deep_Dive | bob@company.com
   Cost: $9.87 | Queries: 156 | Avg: $0.06/query
   Models: Flash (156)
   ‚ö† Cost-efficient model, but high query count
   ‚Üí Consider batch processing or planners
```

Patterns:
- High-stakes sessions (board prep, migrations) justify premium models
- Exploratory sessions (churn, product) would benefit from planners
- High query counts signal inefficiency‚Äîusers iterating without guidance

**RAG EFFICIENCY SAVINGS:**

Where it gets interesting. Click "RAG Impact" tab.

Uderia tracks two scenarios per query:
1. **Actual cost** (with RAG champion case injection)
2. **Estimated cost without RAG** (cold start, trial-and-error)

```
RAG SAVINGS ANALYSIS (Last 30 Days):

Total Queries with RAG Boost: 1,247 queries
Average Match Quality: 0.78 (good)

COST BREAKDOWN:
Actual (with RAG): $87.34
Estimated (without RAG): $143.21
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Savings: $55.87 (39% reduction)

TOP RAG CONTRIBUTORS:
1. Customer Insights Playbook
   $18.45 saved (267 pattern matches)
2. Database Performance Optimization
   $14.23 saved (189 pattern matches)
3. Sales Analytics Accelerators
   $11.34 saved (312 pattern matches)
4. SQL Best Practices (Knowledge)
   $8.76 saved (143 knowledge retrievals)
```

Planner repositories saved $55.87 this month. That's 39% of what you would have spent without them.

Remember Module 6.2? We said planners deliver 36% savings. This proves it‚Äîempirically.

**ROI:** Spent 8 hours creating these collections. They save $55/month. Payback period: less than 2 months. Every month after = pure savings.

**OPTIMIZATION RECOMMENDATIONS:**

Uderia generates automated insights. Click "Optimization Insights":

```
üí° COST OPTIMIZATION RECOMMENDATIONS:

1. Shift Gemini Workload to Ollama (Local)
   Current: 5.2M tokens on Gemini Flash ($78.32)
   Opportunity: 60% are simple data retrieval
   Action: Configure @LOCAL profile for basic queries
   Potential Savings: $47/month

2. Reserve Claude Opus for Complex Reasoning Only
   Current: 40 queries on Opus ($38.11, avg $0.95/query)
   Observation: 18 queries similar to existing Sonnet patterns
   Action: Use Sonnet ($0.37) for 18, Opus for remaining 22
   Potential Savings: $10.44/month

3. Deploy Missing Planner Collections
   Current: Mike ran 89 churn queries exploratorily ($11.23)
   Opportunity: "Customer Insights Playbook" exists but not deployed
   Action: Deploy collection to Mike's @ANALYST profile
   Potential Savings: $4.50/month (40% reduction)

4. Cross-Tier Knowledge Transfer (Module 4.2 Concept)
   Current: Using Opus ($0.95/query) for complex analysis
   Opportunity: Train RAG on Opus, execute on Flash ($0.015/query)
   Example: Segmentation trained on Opus (5 queries, $4.75)
            then executed on Flash (95 queries, $1.43)
            vs all Opus (100 queries, $95)
   Potential Savings: $88.82 per 100-query workflow (93% reduction)

5. Batch Processing for High-Volume Users
   Current: Bob ran 156 queries in one session ($9.87)
   Opportunity: Many queries similar, batch into multi-part request
   Action: Recommend TDA_LLMTask for batch analysis
   Potential Savings: $6/month (156‚Üí40 batch requests)

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total Potential Savings: $156.76/month (59% reduction)
```

Not generic tips. Data-driven, user-specific, action-oriented recommendations.

Admins click "Apply Recommendation" to:
- Auto-deploy collections to specific users
- Adjust profile configurations
- Send optimization suggestions to users

From "black box spending" to "forensic cost visibility and actionable optimization."

From "after-the-fact reports" to "real-time savings opportunities."

Cost analytics. Pattern detection. RAG ROI tracking. Automated recommendations. Financial intelligence.
```

**Screen Capture Plan:**
- [ ] Admin ‚Üí Cost Analytics ‚Üí Provider Breakdown
- [ ] Pie chart showing:
  - Anthropic: $127.45 (48.2%)
  - Google: $78.32 (29.6%)
  - OpenAI: $43.21 (16.3%)
  - AWS Bedrock: $12.18 (4.6%)
  - Ollama: $3.45 (1.3%)
  - Total: $264.61
- [ ] Model Breakdown tab ‚Üí Top 5 models table:
  - Claude Sonnet 3.5: $89.34, 241K tokens, $0.37/query, 243 queries
  - Gemini Flash: $78.32, 5.2M tokens, $0.015/query, 5,221 queries
  - GPT-4o: $43.21, 87K tokens, $0.49/query, 88 queries
  - Claude Opus: $38.11, 101K tokens, $0.95/query, 40 queries
  - Bedrock Sonnet: $12.18, 102K tokens, $0.12/query, 101 queries
- [ ] Key insight callout: Opus+GPT-4 = 128 queries but 30.6% of spend
- [ ] Trend Analysis tab ‚Üí 30-day line chart
- [ ] Daily costs plotted with spikes on Nov 20-22
- [ ] Click Nov 20 ‚Üí Drill-down showing 3 high-cost sessions
- [ ] Expensive Sessions tab ‚Üí Top 10 table:
  - Q4_Board_Presentation: $18.45, 67 queries, sarah
  - Database_Migration: $14.87, 31 queries, mike
  - Customer_Churn: $11.23, 89 queries, alice (with recommendation)
  - Product_Performance: $9.87, 156 queries, bob (with recommendation)
- [ ] RAG Impact tab ‚Üí Savings analysis:
  - 1,247 queries with RAG boost
  - Actual: $87.34, Without RAG: $143.21
  - Savings: $55.87 (39%)
- [ ] Top RAG contributors list:
  - Customer Insights: $18.45 saved
  - DB Performance: $14.23 saved
  - Sales Analytics: $11.34 saved
- [ ] Optimization Insights panel ‚Üí 5 recommendations:
  - Shift to Ollama: $47/month savings
  - Reserve Opus: $10.44/month savings
  - Deploy collections: $4.50/month savings
  - Cross-tier transfer: $88.82 savings (example)
  - Batch processing: $6/month savings
  - Total potential: $156.76/month (59% reduction)
- [ ] "Apply Recommendation" buttons next to each insight

**Supporting Materials:**
- Cost analytics technical documentation
- Provider pricing comparison reference
- Model selection optimization guide
- RAG savings calculation methodology
- Cross-tier knowledge transfer case studies
- Batch processing best practices

---

### 7.3 Budget Alerts & Forecasting
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Configure budget thresholds and alerts
- Monitor spending against forecasts
- Receive proactive cost warnings

**Key Topics to Cover:**
- [ ] Monthly budget threshold configuration
- [ ] Real-time spending tracking
- [ ] Automated alert triggers (50%, 75%, 90%, 100%)
- [ ] Cost forecasting based on usage trends
- [ ] Email/Slack notifications
- [ ] Budget enforcement options

**Narration Script:**
```
Cost analytics show where money went. But what about preventing overspending before it happens?

Budget Alerts and Forecasting‚Äîproactive cost management.

**SETTING MONTHLY BUDGET THRESHOLDS:**

Admin ‚Üí Financial Governance ‚Üí Budget Configuration:

```
ORGANIZATION BUDGET SETTINGS:

Monthly Budget: $500.00
(Approved LLM spending limit)

ALERT THRESHOLDS:
‚îú‚îÄ ‚ö†Ô∏è 50% Warning ($250) ‚Üí Email to admins
‚îú‚îÄ ‚ö†Ô∏è 75% Warning ($375) ‚Üí Email to admins + finance
‚îú‚îÄ üö® 90% Critical ($450) ‚Üí Email + Slack + restrict new users
‚îî‚îÄ üõë 100% Exhausted ($500) ‚Üí Email + Slack + BLOCK non-admin queries

ENFORCEMENT MODE:
‚Ä¢ Soft Cap (Alerts only, no blocking) ‚úì Selected
‚Ä¢ Hard Cap (Block queries when exhausted)
```

Soft Cap‚Äîadmins get warnings, users aren't blocked. For tighter control, switch to Hard Cap.

Click "Save Budget Settings."

**REAL-TIME SPENDING TRACKER:**

Budget Dashboard (same page, scroll down):

```
üìä BUDGET STATUS (December 2025):

Spent: $264.61 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë (52.9%)
Remaining: $235.39
Days Left: 24 days

PACE ANALYSIS:
Days Elapsed: 7 of 31 (22.6%)
Budget Used: 52.9%
Status: ‚ö†Ô∏è SPENDING FASTER THAN PACE

Daily Average: $37.80/day
Budget Allows: $16.13/day (to stay within $500)
Overage: +$21.67/day (135% over pace)
```

Warning signal. 7 days into December, already used 53% of budget. At current pace, you'll hit $500 by December 13‚Äî18 days early.

System detected automatically. No manual calculation needed.

**ALERT TRIGGER HISTORY:**

Scroll to "Alert History":

```
ALERT LOG (December 2025):

Dec 1, 08:00 AM - Budget Reset
  Budget: $500.00 | Spend: $0.00

Dec 4, 02:34 PM - ‚ö†Ô∏è 50% WARNING TRIGGERED
  Spent: $252.18 (50.4%)
  Recipients: admin@company.com, finance@company.com
  Action: Email sent

Dec 6, 11:47 AM - ‚ö†Ô∏è 75% WARNING TRIGGERED
  Spent: $377.45 (75.5%)
  Recipients: admin, finance, ceo
  Action: Email + Slack notification
  Message: "LLM budget at 75%. Projected overage by Dec 13.
           Review cost analytics."

Dec 7 (Today), 09:12 AM - üö® 90% CRITICAL (PENDING)
  Current: $264.61 (52.9%)
  Forecast: Will trigger Dec 9 at current pace
  Action: Pre-alert sent to admins
```

Already triggered 50% and 75% warnings. 90% critical alert forecasted for Dec 9 if spending continues at current rate.

**COST FORECASTING:**

Click "View Forecast" tab. Uderia projects spend based on:
1. Historical usage (last 30 days)
2. Current month trajectory (first 7 days)
3. Known upcoming events (user activity patterns)

```
FORECAST MODEL: Linear Projection

Current Spend (Dec 7): $264.61
Daily Average (7 days): $37.80/day
Remaining Days: 24 days

PROJECTED SPEND:
Dec 31 Forecast: $264.61 + (24 √ó $37.80) = $1,171.81

Budget: $500.00
Projected Overage: $671.81 (234% over budget)

‚ùå BUDGET BREACH FORECAST: December 13, 2025 (6 days)
```

Alarming. Without intervention, you'll exhaust budget in 6 days and overspend by $671 for the month.

But Uderia also shows alternate scenarios:

```
SCENARIO ANALYSIS:

1. Current Pace (No Changes)
   Dec 31 Total: $1,171.81
   Overage: $671.81
   Likelihood: 85%

2. Apply Optimization Recommendations (Module 7.2)
   Shift 60% to Ollama: -$47/month
   Reserve Opus: -$10/month
   Deploy planners: -$4.50/month
   Adjusted Daily: $24.75/day (35% reduction)
   Dec 31 Total: $264.61 + (24 √ó $24.75) = $858.61
   Overage: $358.61
   Likelihood: 60%

3. Aggressive Cost Controls
   All optimizations: -$61.50/month
   Shift 50% to cheaper models: -$80/month
   Batch processing: -$20/month
   Adjusted Daily: $14.20/day (62% reduction)
   Dec 31 Total: $264.61 + (24 √ó $14.20) = $605.41
   Overage: $105.41
   Likelihood: 40% (requires behavior changes)

4. Budget Increase
   Raise to $1,200/month
   No behavior changes needed
   Dec 31 Total: $1,171.81
   Overage: $0 (within new budget)
   Likelihood: 100% (if finance approves)
```

Admins choose:
- **Optimize** (Scenario 2 or 3)
- **Increase Budget** (Scenario 4)
- **Enforce Hard Cap** (block queries at $500)

**EMAIL ALERT EXAMPLE:**

What admins received when 75% warning triggered (Dec 6):

```
From: uderia-alerts@company.com
To: admin@company.com, finance@company.com, ceo@company.com
Subject: üö® LLM Budget Alert: 75% Threshold Reached

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Hi Team,

Your organization has reached 75% of the monthly LLM budget:

üìä BUDGET STATUS:
Monthly Budget: $500.00
Spent to Date: $377.45 (75.5%)
Remaining: $122.55 (24.5%)
Days Remaining: 25 days

‚ö†Ô∏è PACE WARNING:
Spending 3.2x faster than budget allows
Budget exhausted by: December 13 (19 days early)
Projected month-end: $1,171.81 (234% over budget)

üí° RECOMMENDED ACTIONS:
1. Review Cost Analytics Dashboard
2. Deploy planner repositories to high-usage users
   (potential $61/month savings)
3. Configure consumption profiles to limit high-cost patterns
4. Consider budget increase if justified by business value

üîó QUICK LINKS:
Cost Analytics Dashboard
Optimization Recommendations
Budget Configuration

Next Alert: 90% threshold ($450) - Forecasted Dec 9

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Uderia Financial Governance System
```

Actionable email:
- Clear status (75.5% spent)
- Specific warning (exhausted Dec 13)
- Concrete recommendations (deploy planners, configure profiles)
- Direct links (one-click access)

Admins don't guess. They act on data.

**SLACK INTEGRATION:**

Uderia also sends Slack notifications (if configured). In your #finance-alerts channel:

```
üö® Uderia Budget Alert

75% of monthly LLM budget reached ($377.45 / $500.00)

Projected overage: $671.81 by month-end
Forecast: Budget exhausted by Dec 13 at current pace

[View Dashboard] [Optimization Tips] [Adjust Budget]
```

Real-time visibility. Instant awareness. Immediate action.

**HARD CAP ENFORCEMENT:**

Let's say you decide to enforce a Hard Cap. Go back to Budget Configuration.
**HARD CAP ENFORCEMENT:**

Decide to enforce Hard Cap. Back to Budget Configuration:

```
ENFORCEMENT MODE:
‚Ä¢ Soft Cap (Alerts only) ‚Üê Current
‚Ä¢ Hard Cap (Block queries when exhausted) ‚úì Select
```

Save. Simulate reaching 100% budget:

```
Dec 13, 10:23 AM - üõë 100% BUDGET EXHAUSTED

Spent: $500.00 (100%) | Remaining: $0.00

ENFORCEMENT: HARD CAP ACTIVE

ACTIONS TAKEN:
‚îú‚îÄ All non-admin queries BLOCKED
‚îú‚îÄ Email sent to admin, finance, ceo
‚îú‚îÄ Slack notification to #finance-alerts
‚îî‚îÄ User-facing message updated:

   "‚ö†Ô∏è Monthly LLM budget exhausted. Organization reached 
   $500/month limit. Contact administrator to increase budget 
   or wait for reset (Jan 1). Admins can query for emergency 
   operations."
```

Users see when they try to query:

```
üõë BUDGET LIMIT REACHED

Your organization has exhausted its monthly LLM budget 
($500.00).

Next reset: January 1, 2025 (19 days)

Contact administrator if you need urgent access.
```

Hard Cap protects against runaway costs. No overspend. Absolute control.
But what if CEO needs urgent analysis despite budget exhaustion?
**ADMIN EMERGENCY OVERRIDE:**

CEO needs urgent analysis despite budget exhaustion? Admin action:

```
EMERGENCY EXTENSION:

1. Budget Configuration ‚Üí "Grant Emergency Extension"
2. Amount: $200 (temporary increase)
3. Expiration: 48 hours
4. Reason: "Q4 board presentation - CEO urgent request"
5. Save
```

Budget temporarily increases to $700. CEO can query. After 48 hours ‚Üí Extension expires.

Controlled flexibility‚Äîgovernance with emergency escape hatches.
```
SCENARIO: NO BUDGET ALERTS

December spend: $1,171.81 (uncontrolled)
January surprise: Finance reviews bill, shocked
Reaction: "Why did we spend $1,200 on LLMs?!"
Result: Post-facto investigation, no prevention
```

```
SCENARIO: WITH BUDGET ALERTS & FORECASTING

Dec 4: 50% warning ‚Üí Admin reviews
Dec 6: 75% warning ‚Üí Optimization deployed
Dec 7-9: Planner repositories deployed, users educated
Result: Daily spend drops to $22/day (42% reduction)
Dec 31 Final: $733 spent (vs $1,171 without action)

Savings: $438 (37% reduction from uncontrolled spending)
```

Proactive alerts prevented $438 in overspend. That's 87% of the original $500 budget.

From "surprised by bills" to "controlling costs in real-time."

From "reactive panic" to "proactive governance."

Budget thresholds. Real-time tracking. Forecasting. Automated alerts. Emergency controls. Financial predictability.
```

**Screen Capture Plan:**
- [ ] Admin ‚Üí Financial Governance ‚Üí Budget Configuration
- [ ] Monthly budget: $500.00
- [ ] Alert thresholds:
  - 50%: $250 (Email to admins)
  - 75%: $375 (Email to admins + finance)
  - 90%: $450 (Email + Slack + restrict new users)
  - 100%: $500 (Email + Slack + BLOCK queries)
- [ ] Enforcement mode:
  - Soft Cap (alerts only) ‚Üê selected
  - Hard Cap (block when exhausted)
- [ ] Budget Dashboard showing:
  - Spent: $264.61 (52.9% of budget)
  - Remaining: $235.39
  - Progress bar visualization
  - Pace analysis: ‚ö†Ô∏è Spending 135% faster than pace
  - Daily average: $37.80 vs allowed $16.13
- [ ] Alert History log:
  - Dec 1: Budget reset
  - Dec 4: 50% warning triggered
  - Dec 6: 75% warning triggered
  - Dec 9: 90% forecast
- [ ] Forecast tab ‚Üí Projections:
  - Current pace: $1,171.81 (234% over)
  - Budget breach: Dec 13 (6 days)
- [ ] Scenario analysis table:
  - Scenario 1 (no changes): $1,171.81 overage
  - Scenario 2 (optimizations): $858.61
  - Scenario 3 (aggressive): $605.41
  - Scenario 4 (increase budget): $1,200 new budget
- [ ] Email alert example (75% warning):
  - Subject line
  - Budget status summary
  - Pace warning with specific dates
  - Recommended actions (numbered list)
  - Quick links to dashboards
- [ ] Slack notification in #finance-alerts:
  - 75% alert message
  - Projected overage
  - Action buttons
- [ ] Hard Cap enforcement demonstration:
  - Change to Hard Cap mode
  - 100% budget reached simulation
  - User-facing block message
  - Admin notification
- [ ] Emergency extension interface:
  - Grant Emergency Extension form
  - Amount: $200
  - Expiration: 48 hours
  - Reason field
  - Save ‚Üí Temporary budget increase to $700
- [ ] ROI comparison:
  - No alerts: $1,171.81 spent
  - With alerts: $733 spent
  - Savings: $438 (37% reduction)

**Supporting Materials:**
- Budget configuration guide
- Alert threshold best practices
- Forecasting methodology documentation
- Hard Cap vs Soft Cap comparison
- Email/Slack integration setup guide
- Emergency override procedures
- Cost governance case studies

---

## Module 8: Administration
**Goal:** Complete platform administration and governance capabilities

### 8.1 User & Access Management
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Manage user accounts
- Assign roles and permissions
- Monitor user activity

**Key Topics to Cover:**
- [ ] User administration panel
- [ ] Creating/editing users
- [ ] Role assignment (User, Developer, Admin)
- [ ] Consumption profile assignment
- [ ] User activity monitoring
- [ ] Account suspension/activation

**Narration Script:**
```
Multi-user platforms need governance. Who can access what? Who's consuming resources?

Uderia's User & Access Management provides complete administrative control.

**USER ADMINISTRATION PANEL:**

Admin ‚Üí User Management. Table of all users:

```
ACTIVE USERS (127 total):

Name              Email                Role      Profile      Status      Last Active  Cost
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Alice Johnson     alice@company.com    User      PRO          Active      2 hrs ago    $34.56
Bob Smith         bob@company.com      User      FREE         Active      1 day ago    $2.18
Sarah Davis       sarah@company.com    Developer ENTERPRISE   Active      30 min ago   $67.89
Mike Chen         mike@company.com     Admin     UNLIMITED    Active      5 min ago    $12.34
Carol Williams    carol@company.com    User      PRO          Suspended   14 days ago  $0.00
```

At a glance:
- 127 users across three roles (User, Developer, Admin)
- Four consumption profiles (FREE, PRO, ENTERPRISE, UNLIMITED)
- Activity status (active vs dormant)
- Cost accountability (per-user spend)

Complete visibility into your user base.

**CREATING A NEW USER:**

Click "Add User." User creation form:

```
NEW USER DETAILS:

BASIC INFORMATION:
‚îú‚îÄ Username: john.doe
‚îú‚îÄ Email: john.doe@company.com
‚îú‚îÄ Full Name: John Doe
‚îî‚îÄ Department: Analytics (optional)

PASSWORD:
‚îú‚îÄ Initial Password: [Auto-generated: xK9$mP2#vL7q]
‚îî‚îÄ ‚úì Force password change on first login

ROLE ASSIGNMENT:
‚îú‚îÄ ‚ö™ User (Standard permissions) ‚úì Selected
‚îú‚îÄ ‚ö™ Developer (Advanced tools + debugging)
‚îî‚îÄ ‚ö™ Admin (Full system control)

CONSUMPTION PROFILE:
‚îú‚îÄ ‚ö™ FREE (10/hr, 50/day, 100K tokens/month)
‚îú‚îÄ ‚ö™ PRO (30/hr, 200/day, 1M tokens/month) ‚úì Selected
‚îú‚îÄ ‚ö™ ENTERPRISE (100/hr, 1000/day, 10M tokens/month)
‚îî‚îÄ ‚ö™ UNLIMITED (No limits)

DEFAULT PROFILE DEPLOYMENT:
‚îú‚îÄ ‚úì @ANALYST (Gemini Flash + Analytics tools)
‚îú‚îÄ ‚úì @DBA (if John needs database access)
‚îî‚îÄ ‚úó @QUALITY (not needed for analysts)
```

Click "Create User." Uderia creates:
1. User account with credentials
2. PRO consumption profile (1M tokens/month)
3. @ANALYST and @DBA profiles deployed automatically
4. Welcome email sent with login instructions

John can log in immediately and start working‚Äîzero friction onboarding.

**EDITING EXISTING USERS:**

Click alice@company.com to edit. Full profile:

```
USER: Alice Johnson
‚îú‚îÄ Email: alice@company.com
‚îú‚îÄ Current Role: User
‚îú‚îÄ Consumption Profile: PRO (1M tokens/month)
‚îî‚îÄ Status: Active

USAGE STATISTICS (Current Month):
‚îú‚îÄ Tokens: 347,521 / 1,000,000 (34.8%)
‚îú‚îÄ Queries: 234
‚îú‚îÄ Sessions: 12
‚îú‚îÄ Total Cost: $34.56
‚îî‚îÄ Most Used: @ANALYST (187 queries)

DEPLOYED PROFILES:
‚îú‚îÄ @ANALYST (Gemini Flash, Analytics tools)
‚îú‚îÄ @DBA (Claude Sonnet, Database tools)
‚îî‚îÄ Custom_CustomerAnalysis (Alice's personal)
```

Alice needs elevated permissions for special project.

**Change Role:** User ‚Üí **Developer**

Grants Alice:
- Developer tools (custom MCP servers, debugging)
- Advanced integrations
- Debug mode visibility

Save. Role updates instantly. Next login ‚Üí Developer features visible.

**ROLE PERMISSIONS COMPARISON:**

What each role can do:

```
USER (Standard):
‚úì Execute queries via UI
‚úì Create and manage own profiles
‚úì Access marketplace (subscribe to collections)
‚úì View own cost analytics
‚úó Access system configuration
‚úó Manage other users
‚úó Generate long-lived access tokens

DEVELOPER (Advanced):
‚úì All User permissions
‚úì Create custom MCP servers
‚úì Access debug logs and advanced monitoring
‚úì Use developer mode features
‚úì API integration with full documentation
‚úó Manage other users
‚úó Modify system-wide settings

ADMIN (Full Control):
‚úì All Developer permissions
‚úì Manage users (create, edit, suspend, delete)
‚úì Configure LLM providers and MCP servers
‚úì Access global cost analytics (all users)
‚úì Generate long-lived access tokens
‚úì Modify consumption profiles and budgets
‚úì System configuration and maintenance
```

Role determines capability. Choose appropriately.

**CONSUMPTION PROFILE REASSIGNMENT:**

Alice (PRO profile) consistently hits 1M token limit. Time to upgrade.

Alice's user edit page:

```
CONSUMPTION PROFILE CHANGE:
‚îú‚îÄ Current: PRO (1M tokens/month)
‚îú‚îÄ Change to: ENTERPRISE (10M tokens/month)
‚îú‚îÄ Reason: "High-volume Q4 analytics project"
‚îî‚îÄ Expiration: December 31, 2025 (temporary upgrade)
```

Save. Alice now has:
- 10M tokens/month (10x capacity)
- 1,000 prompts/day
- 100 prompts/hour

January 1 ‚Üí Automatically reverts to PRO (scheduled downgrade).

Time-bound escalation‚Äîgrant power when needed, revoke when project ends.

**USER ACTIVITY MONITORING:**

Click "Activity Log" tab for alice@company.com. Detailed history:

```
USER ACTIVITY LOG (Last 30 Days):

Dec 7, 09:15 AM - Login (IP: 192.168.1.45)
Dec 7, 09:18 AM - Profile Switch: @ANALYST activated
Dec 7, 09:20 AM - Query: "Show Q4 revenue by region" ($0.23)
Dec 7, 09:25 AM - Query: "Break down by product category" ($0.31)
Dec 7, 10:12 AM - Profile Switch: @DBA activated
Dec 7, 10:15 AM - Query: "Analyze slow queries in last 7 days" ($0.45)
Dec 7, 11:03 AM - Marketplace: Subscribed to "Customer Insights Playbook"
Dec 7, 11:30 AM - Created profile: "Custom_CustomerAnalysis"
Dec 7, 02:45 PM - Logout

Dec 6, 08:30 AM - Login (IP: 192.168.1.45)
...
```

Every action tracked:
- Logins/logouts (IP addresses for security)
- Profile switches (which personas)
- Query executions (with costs)
- Marketplace interactions (subscriptions, forks)
- Profile creations (custom configurations)

Forensic accountability. Know exactly what Alice did, when, and how much it cost.

**ACCOUNT SUSPENSION:**

Carol Williams left the company two weeks ago. Still "Active" in system.

carol@company.com ‚Üí Click "Suspend Account":

```
SUSPENSION CONFIRMATION:
‚îú‚îÄ User: carol@company.com
‚îú‚îÄ Reason: "Employee departure - offboarding"
‚îî‚îÄ Effect:
   ‚îú‚îÄ User cannot log in
   ‚îú‚îÄ Existing sessions terminated immediately
   ‚îú‚îÄ All scheduled queries cancelled
   ‚îú‚îÄ Profile data preserved (for auditing)
   ‚îî‚îÄ No token consumption allowed
```

Click "Suspend." Status ‚Üí **Suspended**. If she tries to log in:

```
‚ùå ACCOUNT SUSPENDED

Your account has been suspended by your administrator.

Reason: Employee departure - offboarding

Contact: admin@company.com
```

Data remains for auditing. Zero access.

**ACCOUNT REACTIVATION:**

Two months later, Carol returns as a contractor. Reactivate her account.

Go to carol@company.com (Status: Suspended) ‚Üí Click "Reactivate Account."

**Reactivation Options:**
- ‚úì Restore previous role (User)
- ‚úì Restore previous consumption profile (PRO)
- ‚úì Restore deployed profiles (@ANALYST, @DBA)
- ‚ö™ Reset password (force new password)
- ‚ö™ Clear activity history (start fresh)

Check "Reset password" for security (she's returning after absence).

Click "Reactivate."

Carol receives:
- Reactivation email with temporary password
- Access to all previous profiles and data
- Audit log entry: "Account reactivated by admin@company.com on Dec 7, 2025"

Seamless offboarding and re-onboarding.

**BULK USER OPERATIONS:**

Need to upgrade 15 analysts from PRO to ENTERPRISE for Q4 project.

Click "Bulk Actions" ‚Üí Select users:

```
BULK USER SELECTION:
‚úì alice@company.com
‚úì john.doe@company.com
‚úì mike@company.com
... (15 total selected)
```

Bulk operation:

```
BULK OPERATION:
‚îú‚îÄ Action: Change Consumption Profile
‚îú‚îÄ New Profile: ENTERPRISE
‚îú‚îÄ Expiration: December 31, 2025 (auto-revert to PRO)
‚îî‚îÄ Notification: Send email to affected users
```

Click "Apply." Uderia updates all 15 users in 2 seconds. Each receives:

```
Subject: Your Uderia consumption profile has been upgraded

Hi Alice,

Your consumption profile has been upgraded from PRO to ENTERPRISE 
for the Q4 analytics project.

NEW LIMITS:
Hourly: 100 prompts (was 30)
Daily: 1,000 prompts (was 200)
Monthly: 10M tokens (was 1M)

Expires: December 31, 2025
Auto-revert to PRO on January 1.

Happy querying!
```

No manual user-by-user editing. Efficient bulk management.

**USAGE INSIGHTS:**

Click "Usage Insights" tab. Organizational patterns:

```
TOP 5 MOST ACTIVE USERS (Last 30 Days):

1. alice@company.com - 234 queries, $34.56
2. sarah@company.com - 189 queries, $67.89 (expensive models)
3. mike@company.com - 156 queries, $12.34
4. bob@company.com - 98 queries, $2.18 (cost-efficient)
5. john.doe@company.com - 45 queries, $8.21 (new user, ramping up)

DORMANT USERS (No activity 30+ days):
‚Ä¢ emma@company.com (Last active: 47 days ago)
‚Ä¢ ryan@company.com (Last active: 62 days ago)

Recommendation: Suspend dormant users to prevent credential leakage.

ROLE DISTRIBUTION:
Users: 112 (88%)
Developers: 12 (9%)
Admins: 3 (3%)
```

Balanced hierarchy. Most users standard, few with elevated privileges (security best practice).

From "unmanaged user chaos" to "complete administrative control."

From "who has access to what?" to "forensic visibility and governance."

User creation. Role management. Activity tracking. Suspension/reactivation. Bulk operations. Complete control.
```

**Screen Capture Plan:**
- [ ] Admin ‚Üí User Management landing page
- [ ] User table showing 127 users:
  - Columns: Name, Email, Role, Profile, Status, Last Active, Total Cost
  - 5 sample users displayed (Alice, Bob, Sarah, Mike, Carol)
- [ ] "Add User" button ‚Üí User creation form:
  - Basic info (username, email, name, department)
  - Auto-generated password with "Force change on first login" checkbox
  - Role radio buttons (User selected)
  - Consumption profile radio (PRO selected)
  - Default profile deployment checkboxes (@ANALYST, @DBA checked)
- [ ] Click "Create User" ‚Üí Success confirmation
- [ ] Welcome email preview sent to john.doe@company.com
- [ ] Click alice@company.com ‚Üí User detail view:
  - Current role: User
  - Consumption: PRO, 347,521/1M tokens (34.8%)
  - Usage stats: 234 queries, $34.56
  - Deployed profiles: @ANALYST, @DBA, Custom_CustomerAnalysis
- [ ] Edit role: User ‚Üí Developer dropdown
- [ ] Save ‚Üí Role updated confirmation
- [ ] Role permissions comparison table:
  - USER: UI access, own profiles, marketplace, own analytics
  - DEVELOPER: + custom MCP, debug logs, API integration
  - ADMIN: + user management, system config, global analytics
- [ ] Change Alice's consumption profile:
  - Current: PRO ‚Üí New: ENTERPRISE
  - Expiration: Dec 31, 2025
  - Reason field: "High-volume Q4 analytics project"
- [ ] Save ‚Üí Profile upgraded, scheduled revert displayed
- [ ] Activity Log tab for alice@company.com:
  - Timestamped entries: Login, profile switches, queries (with costs), marketplace actions
  - Last 30 days of activity
- [ ] carol@company.com (Active) ‚Üí "Suspend Account" button
- [ ] Suspension confirmation modal:
  - Reason: "Employee departure"
  - Effects listed (no login, sessions terminated, data preserved)
- [ ] Click "Suspend" ‚Üí Status changes to Suspended
- [ ] Carol's login attempt showing: "Account Suspended" error
- [ ] Later: carol@company.com (Suspended) ‚Üí "Reactivate Account"
- [ ] Reactivation options:
  - Restore role, profile, deployed profiles (checked)
  - Reset password (checked)
  - Clear history (unchecked)
- [ ] Click "Reactivate" ‚Üí Reactivation email sent
- [ ] Bulk Actions interface:
  - 15 users selected (checkboxes)
  - Action: Change Consumption Profile ‚Üí ENTERPRISE
  - Expiration: Dec 31, 2025
  - Notification: Send email (checked)
- [ ] Click "Apply" ‚Üí 15 users updated in 2 seconds
- [ ] Bulk notification email preview
- [ ] Usage Insights tab:
  - Top 5 active users with query counts and costs
  - Dormant users list (30+ days inactive)
  - Role distribution pie chart (88% User, 9% Developer, 3% Admin)

**Supporting Materials:**
- User management documentation
- Role permissions matrix (detailed)
- Consumption profile assignment guide
- Activity monitoring and auditing guide
- Suspension vs deletion comparison
- Bulk operations reference
- Security best practices for user administration

---

### 8.2 Long-Lived Access Tokens
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Generate long-lived access tokens for API integrations
- Understand token security and scope
- Manage token lifecycle and revocation

**Key Topics to Cover:**
- [ ] Token generation (90 days or 1 year)
- [ ] One-time display security pattern
- [ ] SHA256 hashed storage
- [ ] Usage tracking (last used, count, IP)
- [ ] Token revocation and soft-delete
- [ ] API authentication with tokens

**Narration Script:**
```
JWT session tokens (Module 5.3) expire after 24 hours. Perfect for human users.

But what about CI/CD pipelines? Airflow workflows? Production automation?

Long-Lived Access Tokens‚Äîpersistent credentials for unattended programmatic access.

**TOKEN GENERATION:**

Admin ‚Üí API Tokens ‚Üí "Generate New Token":

```
TOKEN CONFIGURATION:
‚îú‚îÄ Name: airflow_prod_pipeline
‚îú‚îÄ Description: "Production Airflow DAG for nightly analytics"
‚îú‚îÄ Associated User: automation@company.com (service account)
‚îú‚îÄ Lifespan: 1 year
‚îú‚îÄ Scopes: query:execute, session:read (least privilege)
‚îú‚îÄ Profile Restriction: @COST only (prevent expensive models)
‚îî‚îÄ Rate Limit: 100/hour, 200/day
```

Click "Generate."

**ONE-TIME DISPLAY:**

Uderia shows:

```
üîê TOKEN: uderia_sk_live_xK9mP2vL7qR4nT8hS6jY3wZ1aF5cD9bE

‚ö†Ô∏è SECURITY WARNING:
Shown ONCE only. Store securely:
‚úì AWS Secrets Manager / HashiCorp Vault
‚úó NOT in code repositories
‚úó NOT in plaintext files
```

Copy token ‚Üí Store in AWS Secrets Manager ‚Üí Click "I Have Saved This Token."

Plaintext disappears forever. Only SHA256 hash stored in database.

**API USAGE:**

Airflow DAG:

```python
import requests
import os

token = os.environ['UDERIA_TOKEN']  # From AWS Secrets

headers = {'Authorization': f'Bearer {token}'}
payload = {'profile': '@COST', 'query': '...'}

response = requests.post(
    'https://uderia.company.com/api/v1/query',
    headers=headers,
    json=payload
)
```

Token authenticates as automation@company.com with limited scope.

**TOKEN MANAGEMENT:**

Admin ‚Üí API Tokens shows all active tokens:

| Token Name | User | Scopes | Last Used | Created | Expires | Status |
|------------|------|--------|-----------|---------|---------|--------|
| airflow_prod | automation | query:execute | 2h ago | Dec 1 | Dec 1, 2025 | Active |
| legacy_token | old-service | admin:all | 47 days ago | Mar 5 | NEVER | ‚ö†Ô∏è Risky |

**Security red flag:** legacy_token has admin:all scope, never expires, dormant 47 days.

Click "Revoke" ‚Üí Token immediately invalidated ‚Üí API calls return 401 Unauthorized.

**TOKEN ROTATION:**

Best practice: Rotate every 90-180 days.

1. Generate new token: airflow_prod_v2
2. Update Airflow with new token
3. Test in staging
4. Deploy to production
5. Verify 1 week
6. Revoke old token

Seamless rotation with zero downtime.

**AUDIT LOGGING:**

Every token operation logged:

```
TOKEN AUDIT LOG:

Dec 7, 10:23 AM - Token Generated
  Name: airflow_prod_v2
  User: automation@company.com
  Scopes: query:execute
  By: admin@company.com

Dec 7, 09:45 AM - Token Revoked
  Name: legacy_token
  Reason: "Over-privileged, dormant"
  By: admin@company.com
```

Forensic accountability for all API access.

From "authentication blocking automation" to "secure, always-on programmatic access."

Long-lived tokens. Scoped permissions. Rotation. Audit trails. Automation enabled.
```

**Screen Capture Plan:**
- [ ] Admin ‚Üí API Tokens ‚Üí "Generate New Token"
- [ ] Token creation form:
  - Name: airflow_prod_pipeline
  - Description, User: automation@company.com
  - Lifespan: 1 year
  - Scopes: query:execute, session:read (checkboxes)
  - Profile restriction: @COST
  - Rate limits: 100/hr, 200/day
- [ ] Click "Generate" ‚Üí One-time token display:
  - Token: uderia_sk_live_xK9mP2vL7qR4nT8hS6jY3wZ1aF5cD9bE
  - Security warning (red box)
  - "Shown ONCE only"
  - [Copy] [Download .env] buttons
- [ ] AWS Secrets Manager storage command
- [ ] Click "I Have Saved" ‚Üí Plaintext disappears
- [ ] Python code example (Airflow DAG with Bearer token)
- [ ] Admin ‚Üí API Tokens ‚Üí Active tokens table:
  - 2 tokens shown
  - legacy_token with ‚ö†Ô∏è warnings (admin:all, NEVER expires, 47 days dormant)
- [ ] Click legacy_token ‚Üí "Revoke" button
- [ ] Revocation confirmation ‚Üí Status: Revoked
- [ ] API call with revoked token ‚Üí 401 Unauthorized
- [ ] Token rotation workflow diagram (5 steps)
- [ ] Audit log showing token operations

**Supporting Materials:**
- Long-lived token documentation
- API authentication guide with code samples
- Scope and permissions reference
- Token rotation procedures
- Security best practices
- AWS Secrets Manager integration

---

### 8.3 System Configuration & Monitoring
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Configure system-wide settings
- Monitor platform health
- Manage LLM providers and MCP servers

**Key Topics to Cover:**
- [ ] System settings panel
- [ ] LLM provider configuration
- [ ] MCP server management
- [ ] Security settings
- [ ] Audit log access
- [ ] Platform customization

**Narration Script:**
```
Users manage profiles. Admins manage the entire system.

System Configuration‚Äîcontrol panel for platform-wide settings.

**LLM PROVIDER CONFIGURATION:**

Admin ‚Üí System Configuration ‚Üí "LLM Providers":

```
ACTIVE PROVIDERS:
‚úì Anthropic (Claude)
‚úì OpenAI (GPT-4)
‚úì Google AI (Gemini)
‚úì AWS Bedrock
‚úì Ollama (Local)
‚úó Azure OpenAI (Not Configured)
```

Add Azure OpenAI. Click "Add Provider" ‚Üí Select "Azure OpenAI":

```
AZURE OPENAI CONFIGURATION:
‚îú‚îÄ API Endpoint: https://your-resource.openai.azure.com/
‚îú‚îÄ API Key: ******************************** (encrypted)
‚îú‚îÄ API Version: 2024-06-01
‚îú‚îÄ Deployment Mapping:
‚îÇ  ‚îú‚îÄ gpt-4o-prod ‚Üí GPT-4o
‚îÇ  ‚îî‚îÄ gpt-4o-mini-prod ‚Üí GPT-4o-mini
‚îú‚îÄ Region: East US
‚îî‚îÄ Retry Config: 3 max, 2s delay, 120s timeout
```

Click "Test Connection" ‚Üí ‚úì All checks passed ‚Üí "Save Provider."

Azure OpenAI now available system-wide.

**MCP SERVER REGISTRATION:**

Click "MCP Servers":

```
ACTIVE SERVERS:
1. base_mcp (Database) - ‚úì Running
2. qlty_mcp (Quality) - ‚úì Running
3. dba_mcp (Admin) - ‚úì Running
4. TDA_mcp (Platform) - ‚úì Running
5. custom_erp (ERP Integration) - ‚ö†Ô∏è Unstable
```

Register Salesforce MCP server. Click "Register MCP Server":

```
SALESFORCE MCP CONFIGURATION:
‚îú‚îÄ Server Name: salesforce_mcp_server
‚îú‚îÄ Description: "Salesforce CRM integration"
‚îú‚îÄ Endpoint: https://salesforce-mcp.company.com/mcp
‚îú‚îÄ Authentication: OAuth 2.0
‚îÇ  ‚îú‚îÄ Client ID: 3MVG9...
‚îÇ  ‚îú‚îÄ Client Secret: ********************************
‚îÇ  ‚îî‚îÄ Token Endpoint: https://login.salesforce.com/.../token
‚îî‚îÄ Tools Auto-Discovered:
   ‚îú‚îÄ salesforce_getAccount
   ‚îú‚îÄ salesforce_getOpportunities
   ‚îú‚îÄ salesforce_createLead
   ‚îî‚îÄ salesforce_updateContact
```

Click "Test Connection" ‚Üí ‚úì 4 tools registered ‚Üí "Register Server."

Salesforce tools now available for custom profiles.

**DEFAULT PROFILES:**

Click "Default Profiles."

New users automatically get:
- **@ANALYST** (Gemini Flash, base tools)
- **@LOCAL** (optional for compliance users)

Edit @ANALYST ‚Üí Add:
- Knowledge: "SQL Optimization Best Practices"
- Planner: "Customer Insights Playbook"

Save. All new users get @ANALYST pre-loaded with expert knowledge.

**SECURITY SETTINGS:**

Click "Security Settings":

```
AUTHENTICATION:
‚îú‚îÄ JWT Secret: ******* (last rotated 23 days ago)
‚îú‚îÄ Session Duration: 24 hours
‚îî‚îÄ Password Requirements: 12 chars, complexity rules

ENCRYPTION:
‚îú‚îÄ Algorithm: AES-256-GCM
‚îú‚îÄ All credentials encrypted ‚úì
‚îî‚îÄ Last key rotation: 68 days ago

NETWORK SECURITY:
‚îú‚îÄ HTTPS enforcement ‚úì
‚îú‚îÄ CORS policy configured
‚îî‚îÄ Rate limiting: 5 login attempts, 1000 API/hour

AUDIT LOGGING:
‚îú‚îÄ Retention: 90 days
‚îî‚îÄ Events: Logins, auth failures, role changes, config changes
```

Enterprise-grade security configured.

**PLATFORM SETTINGS:**

Click "Platform Settings":

```
BRANDING:
‚îú‚îÄ Platform Name: Uderia (or customize)
‚îú‚îÄ Logo: [Upload Custom]
‚îú‚îÄ Primary Color: #4A90E2
‚îî‚îÄ Contact: support@company.com

FEATURES:
‚úì Marketplace Enabled
‚úì Hybrid Intelligence
‚úì Cost Analytics
‚úó Beta Features
```

```
NOTIFICATIONS:
‚îú‚îÄ Email: SMTP configured ‚úì
‚îî‚îÄ Slack: Webhook configured ‚úì
```

Save all settings.

**CONFIGURATION AUDIT:**

Admin ‚Üí System Configuration ‚Üí "Audit Log":

```
CONFIGURATION AUDIT LOG:

Dec 7, 11:23 AM - MCP Server Registered
  Server: salesforce_mcp
  By: admin@company.com

Dec 7, 10:15 AM - Default Profile Modified
  Profile: @ANALYST
  Added: SQL Best Practices collection
  By: admin@company.com

Dec 5, 02:34 PM - LLM Provider Added
  Provider: Azure OpenAI
  By: admin@company.com

Nov 15, 09:00 AM - JWT Secret Rotated
  Sessions invalidated: 127 users
  By: admin@company.com
```

Forensic accountability for every system change.

From "fixed configuration" to "customizable, enterprise-ready system."

Providers. MCP servers. Security. Branding. Complete control.
```

**Screen Capture Plan:**
- [ ] Admin ‚Üí System Configuration landing page (5 categories)
- [ ] Click "LLM Providers" ‚Üí Provider list (6 providers)
- [ ] "Add Provider" ‚Üí Azure OpenAI form:
  - Endpoint, API Key (masked), API Version
  - Deployment mapping
  - Retry config
- [ ] "Test Connection" ‚Üí Validation results ‚úì
- [ ] "Save Provider" ‚Üí Azure added
- [ ] Click "MCP Servers" ‚Üí Server list (5 servers)
- [ ] "Register MCP Server" ‚Üí Salesforce form:
  - Name, endpoint, OAuth config
  - Auto-discovered tools (4 listed)
- [ ] "Test Connection" ‚Üí ‚úì Registered
- [ ] Click "Default Profiles" ‚Üí @ANALYST shown
- [ ] Edit ‚Üí Add collections ‚Üí Save
- [ ] Click "Security Settings" ‚Üí Configuration panel:
  - Authentication (JWT, session, passwords)
  - Encryption (AES-256, rotations)
  - Network (HTTPS, CORS, rate limits)
  - Audit logging (90-day retention)
- [ ] Click "Platform Settings" ‚Üí Branding panel:
  - Name, logo, color, contact
  - Feature flags
  - Email/Slack notifications
- [ ] Audit Log tab ‚Üí Timestamped configuration changes

**Supporting Materials:**
- System configuration comprehensive guide
- LLM provider setup docs (per provider)
- MCP server integration guide
- Security settings reference
- Platform customization guide
- Audit logging documentation

---

## Module 9: Advanced Topics
**Goal:** Deep-dive into specialized integrations and deployment scenarios

### 9.1 Flowise Integration: Low-Code Workflows
**Duration:** 4-5 minutes  
**Learning Objectives:**
- Understand Flowise + Uderia architecture
- Import pre-built agent flows
- Build custom workflows visually

**Key Topics to Cover:**
- [ ] Flowise overview and value proposition
- [ ] Pre-built Uderia agent flow
- [ ] Async submit + poll pattern implementation
- [ ] Session management in workflows
- [ ] Bearer token authentication
- [ ] TTS payload extraction for voice chatbots

**Narration Script:**
```
Uderia is powerful. But what if you're not a developer?

What if you want visual, drag-and-drop workflow orchestration?

**Flowise**‚Äîthe low-code integration layer for Uderia.

**FLOWISE OVERVIEW:**

Flowise: open-source platform for building AI agent workflows visually.

No Python. No REST API code. Just drag, drop, connect.

Uderia provides **pre-built Flowise agent flow** implementing complete async pattern:
- Submit conversation
- Poll for status
- Extract final response
- Format for voice/text chatbots

Import it.

**IMPORT UDERIA TEMPLATE:**

Flowise ‚Üí "Import Flow" ‚Üí Select `uderia_async_agent.json`. Flow appears:

```
FLOW ARCHITECTURE:

[User Input] ‚Üí [Uderia Submit] ‚Üí [Polling Loop] ‚Üí [Response Processor] ‚Üí [Output]
                     ‚Üì
               [Session Store]
```

```
NODE 1: Uderia Submit
‚îú‚îÄ Endpoint: https://uderia.company.com/api/v1/submit
‚îú‚îÄ Method: POST
‚îú‚îÄ Headers: Authorization: Bearer {token}
‚îú‚îÄ Body:
‚îÇ  {
‚îÇ    "query": "{{userInput}}",
‚îÇ    "profile": "@ANALYST",
‚îÇ    "session_id": "{{sessionId}}"
‚îÇ  }
‚îî‚îÄ Output: session_id for tracking

NODE 2: Polling Loop
‚îú‚îÄ Endpoint: https://uderia.company.com/api/v1/status/{{sessionId}}
‚îú‚îÄ Method: GET
‚îú‚îÄ Interval: 2 seconds
‚îú‚îÄ Max attempts: 30 (1 minute timeout)
‚îî‚îÄ Stop condition: status == "complete"

NODE 3: Response Processor
‚îú‚îÄ Extracts final_answer from status response
‚îî‚îÄ Applies text formatting:
   ‚îú‚îÄ Remove Markdown for TTS (voice chatbots)
   ‚îú‚îÄ Preserve Markdown for text chat
   ‚îî‚îÄ Extract charts separately (if needed)

NODE 4: Session Store
‚îú‚îÄ Persists session_id for context continuity
‚îú‚îÄ Enables multi-turn conversations
‚îî‚îÄ Links follow-up questions to original analysis
```

**AUTHENTICATION:**

Configure Bearer Token:

```
BEARER TOKEN SETUP:
‚îú‚îÄ Click "Uderia Submit" node
‚îú‚îÄ Headers ‚Üí Add Authorization
‚îú‚îÄ Value: Bearer uderia_sk_live_xK9mP2vL...
‚îî‚îÄ Token from Admin ‚Üí API Tokens (Module 8.2)
```

Secure. Persistent. No user login required.

**TESTING THE FLOW:**

Click "Test Flow."

**Input:** "What were total sales in Q4 2024?"

Flowise executes:

```
EXECUTION LOG:
1. Submit to Uderia ‚Üí session_id: 82b9...
2. Poll status (attempt 1): in-progress
3. Poll status (attempt 2): in-progress
4. Poll status (attempt 3): complete
5. Extract response:
   Total Q4 2024 sales: $892,450
   - October: $287,320
   - November: $312,180
   - December: $292,950
```

**Output:** Clean text (no Markdown, TTS-ready).

**VOICE CHATBOT INTEGRATION:**

Connect Flowise output to:

```
VOICE PLATFORMS:
‚Ä¢ Amazon Polly (TTS)
‚Ä¢ Twilio Voice (phone calls)
‚Ä¢ Alexa Skills
‚Ä¢ Google Assistant
```

User speaks: "What were sales in Q4?"
Alexa responds: "Total Q4 2024 sales were $892,450..."

Powered by Uderia's @ANALYST profile. Zero custom code.

**TEXT CHATBOT INTEGRATION:**

Connect to:

```
TEXT PLATFORMS:
‚Ä¢ Slack (workplace assistant)
‚Ä¢ WhatsApp (customer support)
‚Ä¢ Web chat widgets
```

User types: "Show top 5 customers by revenue"
Chatbot responds with formatted Markdown table.

Uderia handles SQL, analysis, charting. Flowise handles routing.

**CUSTOM WORKFLOW EXAMPLE:**

Multi-step workflow:

```
MULTI-PROFILE ORCHESTRATION:

[User Query] ‚Üí [Uderia @ANALYST] ‚Üí [Extract SQL]
                                          ‚Üì
                            [Uderia @DBA optimize SQL]
                                          ‚Üì
                            [Execute Optimized Query]
                                          ‚Üì
                            [Uderia @QUALITY validate results]
                                          ‚Üì
                            [Final Report]
```

Three profiles orchestrated visually. No Python.

From "developer-only platform" to "accessible to business users."

Flowise + Uderia: Low-code AI orchestration for everyone.
```

**Screen Capture Plan:**
- [ ] Flowise dashboard homepage
- [ ] Click "Import Flow" ‚Üí Select `uderia_async_agent.json`
- [ ] Flow visualization:
  - User Input node
  - Uderia Submit node (show configuration panel)
  - Polling Loop node (show 2s interval, 30 attempts)
  - Response Processor node
  - Output node
  - Session Store (sidebar)
- [ ] Click "Uderia Submit" ‚Üí Configuration:
  - Endpoint URL
  - Method: POST
  - Headers: Authorization Bearer token
  - Body JSON template
- [ ] Click "Polling Loop" ‚Üí Configuration:
  - GET endpoint with {{sessionId}}
  - 2-second interval
  - Stop condition logic
- [ ] Click "Response Processor" ‚Üí Text formatting options:
  - TTS mode (remove Markdown)
  - Text mode (preserve Markdown)
- [ ] Test Flow panel ‚Üí Input: "What were Q4 2024 sales?"
- [ ] Execution log showing:
  - Submit ‚Üí session_id returned
  - Poll 1 ‚Üí in-progress
  - Poll 2 ‚Üí in-progress
  - Poll 3 ‚Üí complete
  - Response extracted
- [ ] Output panel showing clean text response
- [ ] Voice integration diagram:
  - Flowise ‚Üí Amazon Polly ‚Üí Alexa
- [ ] Text integration diagram:
  - Flowise ‚Üí Slack API ‚Üí Workplace chat
- [ ] Custom multi-profile workflow:
  - 3 Uderia nodes (@ANALYST, @DBA, @QUALITY)
  - Visual connections between nodes

**Supporting Materials:**
- docs/Flowise/FLOWISE_INTEGRATION.md
- uderia_async_agent.json (pre-built flow template)
- Flowise integration guide
- Voice chatbot setup guide
- Multi-profile workflow examples

---

### 9.2 Docker Deployment: Production Setup
**Duration:** 4-5 minutes  
**Learning Objectives:**
- Build Uderia Docker image
- Configure production deployment
- Manage multi-user container

**Key Topics to Cover:**
- [ ] Dockerfile overview
- [ ] Docker Compose configuration
- [ ] Environment variable overrides
- [ ] Volume mounts (sessions, logs, keys)
- [ ] Multi-user shared container
- [ ] Load balancer configuration
- [ ] Horizontal scaling considerations

**Narration Script:**
```
Local development works. Now: production deployment.

Docker containerization for consistent, scalable, multi-user environments.

Uderia includes production-ready `Dockerfile` and `docker-compose.yml`.

**DOCKERFILE OVERVIEW:**

Open `Dockerfile`:

```dockerfile
FROM python:3.13-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc postgresql-client curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python packages
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY src/ ./src/
COPY static/ ./static/
COPY templates/ ./templates/
COPY tda_config.json .

# Create directories for persistent data
RUN mkdir -p /app/tda_sessions /app/logs /app/tda_keys

# Expose port
EXPOSE 5000

# Run application
CMD ["python", "src/trusted_data_agent/main.py"]
```

Lightweight base (Python 3.13). Minimal dependencies. Production-ready.

**DOCKER COMPOSE CONFIGURATION:**

Open `docker-compose.yml`:

```yaml
version: '3.8'

services:
  uderia:
    build: .
    container_name: uderia_production
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=production
      - JWT_SECRET=${JWT_SECRET}
      - DATABASE_URL=${DATABASE_URL}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./tda_sessions:/app/tda_sessions
      - ./logs:/app/logs
      - ./tda_keys:/app/tda_keys
      - ./tda_config.json:/app/tda_config.json
    restart: unless-stopped
    networks:
      - uderia_network

  chromadb:
    image: chromadb/chroma:latest
    container_name: uderia_chromadb
    volumes:
      - chroma_data:/chroma/chroma
    ports:
      - "8000:8000"
    restart: unless-stopped
    networks:
      - uderia_network

volumes:
  chroma_data:

networks:
  uderia_network:
    driver: bridge
```

Two services: Uderia app + ChromaDB vector store.

**ENVIRONMENT VARIABLES:**

Create `.env` file (never commit to Git):

```bash
JWT_SECRET=your-secret-key-here
DATABASE_URL=teradata://prod-server:1025/fitness_db
ANTHROPIC_API_KEY=sk-ant-api03-...
GOOGLE_API_KEY=AIza...
OPENAI_API_KEY=sk-proj-...
```

Secure credential management. No hardcoded secrets.

**VOLUME MOUNTS:**

Three critical volumes:

```
1. tda_sessions/ - Multi-user conversation persistence
   ‚îú‚îÄ Each user gets isolated directory
   ‚îú‚îÄ Survives container restarts
   ‚îî‚îÄ Example: tda_sessions/82b9231b-.../session.json

2. logs/ - Application and audit logging
   ‚îú‚îÄ app.log (General application events)
   ‚îú‚îÄ consumption.log (Token usage tracking)
   ‚îî‚îÄ audit.log (Security events)

3. tda_keys/ - Encrypted credential storage
   ‚îú‚îÄ API keys for MCP servers
   ‚îú‚îÄ OAuth tokens
   ‚îî‚îÄ Encrypted at rest (AES-256-GCM)
```

Data persists across deployments.

**BUILD AND RUN:**

Terminal:

```bash
# Build image
docker-compose build

# Start services
docker-compose up -d

# View logs
docker-compose logs -f uderia

# Check status
docker-compose ps
```

Output:

```
NAME                  STATUS    PORTS
uderia_production     Up        0.0.0.0:5000->5000/tcp
uderia_chromadb       Up        0.0.0.0:8000->8000/tcp
```

Both services running. Multi-user ready.

**MULTI-USER ACCESS:**

Single container supports 100+ concurrent users:

```
CONCURRENT SESSIONS:
‚Ä¢ User 1: sarah@company.com ‚Üí Session 82b9... ‚Üí @ANALYST
‚Ä¢ User 2: marcus@company.com ‚Üí Session f4c2... ‚Üí @DBA
‚Ä¢ User 3: priya@company.com ‚Üí Session a1d8... ‚Üí @QUALITY
```

Isolated sessions. Shared infrastructure. Cost-efficient.

**PRODUCTION DEPLOYMENT:**

Deploy to:

```
CLOUD PLATFORMS:
‚Ä¢ AWS ECS (Elastic Container Service)
‚Ä¢ Google Cloud Run
‚Ä¢ Azure Container Instances
‚Ä¢ Kubernetes (advanced orchestration)

ADD:
‚Ä¢ NGINX reverse proxy (SSL termination, load balancing)
‚Ä¢ PostgreSQL for user management (replace SQLite)
‚Ä¢ Redis for session caching (optional performance boost)
‚Ä¢ Prometheus monitoring
‚Ä¢ Grafana dashboards
```

Enterprise-ready architecture.

**HORIZONTAL SCALING:**

Multiple Uderia containers behind load balancer:

```
SCALE ARCHITECTURE:

[Load Balancer]
    |
    ‚îú‚îÄ [Uderia Container 1] (users 1-50)
    ‚îú‚îÄ [Uderia Container 2] (users 51-100)
    ‚îî‚îÄ [Uderia Container 3] (users 101-150)
         |
    [Shared ChromaDB]
    [Shared PostgreSQL]
```

Stateless design enables infinite horizontal scaling.

From "local prototype" to "production-grade, multi-user platform."

Docker. Compose. Volumes. Environment variables. Scale with confidence.
```

**Screen Capture Plan:**
- [ ] VS Code: Open `Dockerfile`
  - Highlight: FROM python:3.13-slim
  - Highlight: COPY requirements.txt, RUN pip install
  - Highlight: EXPOSE 5000, CMD
- [ ] VS Code: Open `docker-compose.yml`
  - Highlight: Two services (uderia, chromadb)
  - Highlight: Ports mapping (5000:5000)
  - Highlight: Environment variables block
  - Highlight: Volume mounts (3 directories)
  - Highlight: restart: unless-stopped
- [ ] VS Code: Open `.env` file
  - Show masked API keys
  - JWT_SECRET, DATABASE_URL, provider keys
- [ ] Terminal: `docker-compose build`
  - Build progress (downloading base image, installing packages)
  - "Successfully built" message
- [ ] Terminal: `docker-compose up -d`
  - Starting services
  - "Started" confirmation
- [ ] Terminal: `docker-compose ps`
  - Table showing 2 containers running
  - Status: Up, Ports exposed
- [ ] Terminal: `docker-compose logs -f uderia`
  - Live log stream showing:
    - "Starting Uderia platform..."
    - "Connected to ChromaDB"
    - "Server running on 0.0.0.0:5000"
- [ ] File explorer: `tda_sessions/` directory
  - Multiple session folders (82b9..., f4c2..., a1d8...)
  - Show session.json inside one folder
- [ ] File explorer: `logs/` directory
  - app.log, consumption.log, audit.log
  - Show file sizes growing
- [ ] Browser: http://localhost:5000
  - Login as User 1 (sarah@company.com)
  - New tab: Login as User 2 (marcus@company.com)
  - New tab: Login as User 3 (priya@company.com)
  - All three sessions active simultaneously
- [ ] Production architecture diagram:
  - Load balancer ‚Üí 3 Uderia containers
  - Shared ChromaDB, PostgreSQL
  - Monitoring stack (Prometheus, Grafana)
- [ ] Deployment checklist overlay:
  - ‚úì Environment variables secured
  - ‚úì Volumes mounted
  - ‚úì SSL configured
  - ‚úì Monitoring enabled
  - ‚úì Backup strategy

**Supporting Materials:**
- Docker deployment comprehensive guide
- docker-compose.yml reference with all options
- Production deployment checklist
- Cloud provider deployment guides (AWS, GCP, Azure)
- Kubernetes manifests (advanced)
- Monitoring and logging setup guide

---

### 9.3 Developer Mode & Custom Integrations
**Duration:** 3-4 minutes  
**Learning Objectives:**
- Enable developer features
- Create custom MCP servers
- Build custom integrations

**Key Topics to Cover:**
- [ ] Developer role activation
- [ ] Custom MCP server development
- [ ] Tool/prompt registration
- [ ] Custom prompt templates
- [ ] Plugin system for Planner Repository Constructors
- [ ] REST API extension patterns

**Narration Script:**
```
Uderia provides 6 pre-built profiles. But what if you need custom integrations?

Proprietary systems? Internal APIs? Custom workflows?

Developer Mode unlocks the extensibility layer.

**DEVELOPER ROLE ACTIVATION:**

Admin ‚Üí Users ‚Üí Edit your account ‚Üí Role: **Developer**.

New sidebar menu appears:

```
DEVELOPER MENU:
‚Ä¢ üõ†Ô∏è Developer Tools
‚Ä¢ üì¶ Custom MCP Servers
‚Ä¢ üîå Plugin System
‚Ä¢ üìù Custom Prompts
‚Ä¢ üîç API Explorer
```

Build a custom integration.

**CUSTOM MCP SERVER:**

Scenario: Company uses **internal ERP system** (not publicly available). Want @ANALYST to query ERP data directly.

Create custom MCP server: `erp_mcp`

**Step 1: Define Tools**

Developer Tools ‚Üí "Create MCP Server" ‚Üí `erp_mcp`. Add tool:

```python
TOOL DEFINITION:
name: "erp_getInventory"
description: "Query current inventory levels from ERP"
parameters:
  - product_code: string (required)
  - warehouse: string (optional)
returns: JSON with stock levels, location, reorder status
```

**Step 2: Implement Tool**

Connect to internal ERP API:

```python
import requests

def erp_getInventory(product_code, warehouse=None):
    headers = {'Authorization': f'Bearer {ERP_TOKEN}'}
    params = {'product': product_code}
    if warehouse:
        params['warehouse'] = warehouse
    
    response = requests.get(
        'https://erp.company.com/api/inventory',
        headers=headers,
        params=params
    )
    
    return response.json()
```

**Step 3: Register with Uderia**

Developer Tools ‚Üí MCP Servers ‚Üí `erp_mcp` ‚Üí "Deploy." Uderia validates:

```
VALIDATION:
‚úì Tool callable
‚úì Parameters schema valid
‚úì Authentication working
‚úì Response format JSON
```

Tool now available.

**Step 4: Attach to Profile**

Profiles ‚Üí Edit @ANALYST ‚Üí Tools ‚Üí Add `erp_getInventory`. Save profile.

**CUSTOM PROMPT TEMPLATES:**

Create prompt for ERP context:

Developer Tools ‚Üí Custom Prompts ‚Üí "Create Prompt"

**Prompt Name:** `erp_inventoryContext`
**Prompt Template:**

```
You have access to internal ERP inventory data.

When analyzing product availability:
1. Query current stock levels with erp_getInventory
2. Check warehouse distribution
3. Flag products below reorder threshold (< 50 units)
4. Recommend restocking priorities

Always include:
- Product code
- Current stock
- Warehouse location
- Days until stockout (if low)
```

Save ‚Üí Attach to @ANALYST profile.

Now @ANALYST understands ERP context automatically.

**PLUGIN SYSTEM:**

Planner Repository Constructors use plugins for custom case types.

Example: Custom "Procurement Analysis" case type.

Developer Tools ‚Üí Plugins ‚Üí "Create Plugin"

**Plugin Name:** `procurement_analysis_plugin`
**Case Type:** Procurement
**Template Structure:**

```python
class ProcurementAnalysisPlugin:
    def extract_metadata(self, conversation):
        return {
            'suppliers': extract_suppliers(conversation),
            'products': extract_products(conversation),
            'cost_metrics': extract_costs(conversation),
            'delivery_times': extract_timelines(conversation)
        }
    
    def champion_criteria(self):
        return {
            'min_cost_savings': 10000,  # $10k minimum
            'supplier_diversity': True,
            'on_time_delivery': '>95%'
        }
```

Deploy plugin ‚Üí Planner Repositories now support custom case type.

**REST API EXTENSIONS:**

Developer Mode exposes advanced API endpoints:

**Endpoint:** `/api/v1/developer/execute-custom-workflow`

```python
import requests

payload = {
    'workflow': 'multi_profile_pipeline',
    'steps': [
        {'profile': '@ANALYST', 'query': 'Find top revenue products'},
        {'profile': '@DBA', 'query': 'Optimize the SQL query'},
        {'profile': '@QUALITY', 'query': 'Validate data accuracy'}
    ],
    'session_id': '82b9...'
}

response = requests.post(
    'https://uderia.company.com/api/v1/developer/execute-custom-workflow',
    headers={'Authorization': f'Bearer {token}'},
    json=payload
)
```

Orchestrate multiple profiles programmatically. Full control.

**DEBUGGING TOOLS:**

Developer Tools ‚Üí "Live MCP Debugger"

Test tools in real-time:
- Input: `erp_getInventory(product_code='BIKE-001')`
- Output: `{"stock": 127, "warehouse": "WH-East", "reorder": false}`
- Latency: 234ms

Debug before deploying to profiles.

From "closed platform" to "fully extensible ecosystem."

Custom MCP servers. Plugins. API extensions. Developer Mode. Build anything.
```

**Screen Capture Plan:**
- [ ] Admin ‚Üí Users ‚Üí Edit user account
  - Change Role from "User" to "Developer"
  - Save
- [ ] Sidebar: New "Developer Tools" menu appears
  - üõ†Ô∏è Developer Tools
  - üì¶ Custom MCP Servers
  - üîå Plugin System
  - üìù Custom Prompts
  - üîç API Explorer
- [ ] Developer Tools ‚Üí "Create MCP Server"
  - Server Name: erp_mcp
  - Description: "Internal ERP integration"
- [ ] Add Tool: `erp_getInventory`
  - Tool definition form:
    - Name, description
    - Parameters: product_code (string), warehouse (optional)
    - Returns: JSON schema
- [ ] VS Code: Show Python implementation
  - Function connecting to ERP API
  - Authentication, parameters, response handling
- [ ] Developer Tools ‚Üí "Deploy" button
  - Validation checks (4 green checkmarks)
  - "Tool Deployed Successfully"
- [ ] Profiles ‚Üí Edit @ANALYST ‚Üí Tools tab
  - Search: erp_getInventory
  - Add to profile
  - Save
- [ ] Developer Tools ‚Üí Custom Prompts ‚Üí "Create Prompt"
  - Prompt Name: erp_inventoryContext
  - Prompt Template editor (showing full template)
  - Attach to @ANALYST profile
  - Save
- [ ] Developer Tools ‚Üí Plugins ‚Üí "Create Plugin"
  - Plugin Name: procurement_analysis_plugin
  - Case Type: Procurement
  - VS Code: Show plugin class code
- [ ] Developer Tools ‚Üí "Live MCP Debugger"
  - Input: erp_getInventory(product_code='BIKE-001')
  - Execute button
  - Output JSON displayed
  - Latency: 234ms
- [ ] VS Code: Custom workflow API example
  - Python code showing multi-profile pipeline
  - POST to /api/v1/developer/execute-custom-workflow
- [ ] Terminal: Execute custom workflow
  - Response showing 3 steps completed

**Supporting Materials:**
- Developer Mode comprehensive guide
- Custom MCP server development tutorial
- Plugin system architecture documentation
- REST API extension reference
- Debugging tools guide
- Example custom integrations (Salesforce, SAP, ServiceNow)

---

### 9.4 Installing Your Own Uderia Platform
**Duration:** 5-6 minutes  
**Learning Objectives:**
- Clone repository and setup
- Configure environment
- Run platform locally
- Verify installation

**Key Topics to Cover:**
- [ ] Prerequisites (Python, Node.js, etc.)
- [ ] Git clone from GitHub
- [ ] Virtual environment setup
- [ ] Dependencies installation (requirements.txt)
- [ ] Configuration file setup (tda_config.json)
- [ ] Database initialization
- [ ] First run and verification
- [ ] Troubleshooting common issues

**Narration Script:**
```
You've seen what Uderia can do. Now: install your own instance.

Zero to running platform in 10 minutes.

**PREREQUISITES:**

Verify:

```bash
python3 --version  # Python 3.11 or higher
node --version     # Node.js 18+ (for optional UI builds)
git --version      # Git 2.0+
```

If missing:

```
INSTALL:
‚Ä¢ Python: python.org
‚Ä¢ Node: nodejs.org
‚Ä¢ Git: git-scm.com

ALSO NEEDED:
‚Ä¢ Database: Teradata, PostgreSQL, MySQL, or SQLite (testing)
‚Ä¢ LLM API Keys: At least one (Anthropic, OpenAI, Google, or local Ollama)
```

**CLONE REPOSITORY:**

Terminal:

```bash
cd ~/projects
git clone https://github.com/your-org/uderia.git
cd uderia
```

Repository structure:

```
uderia/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ trusted_data_agent/  # Core platform code
‚îú‚îÄ‚îÄ static/                   # UI assets
‚îú‚îÄ‚îÄ templates/                # HTML templates
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ tda_config.json          # Configuration (you'll edit)
‚îú‚îÄ‚îÄ Dockerfile               # Container setup
‚îú‚îÄ‚îÄ docker-compose.yml       # Multi-service orchestration
‚îî‚îÄ‚îÄ README.md                # Detailed docs
```

**VIRTUAL ENVIRONMENT:**

Create isolated Python environment:

```bash
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# OR
venv\Scripts\activate     # Windows
```

Prompt changes: `(venv) $`

**INSTALL DEPENDENCIES:**

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

Installs 47 packages (~2-3 minutes):

```
KEY PACKAGES:
‚Ä¢ Flask (web framework)
‚Ä¢ anthropic, openai, google-generativeai (LLM clients)
‚Ä¢ chromadb (vector store)
‚Ä¢ teradatasql (database connector)
‚Ä¢ PyJWT (authentication)
‚Ä¢ And more...
```

Output:

```
Successfully installed:
  anthropic-0.39.0
  chromadb-0.5.23
  Flask-3.1.0
  ...
  ‚úì All dependencies installed
```

**CONFIGURE PLATFORM:**

Open `tda_config.json` in editor:

```json
{
  "database": {
    "type": "teradata",
    "host": "your-teradata-server.com",
    "port": 1025,
    "username": "dbc",
    "password": "your-password",
    "default_database": "fitness_db"
  },
  "llm_providers": {
    "anthropic": {
      "api_key": "sk-ant-api03-...",
      "default_model": "claude-sonnet-4-20250514"
    },
    "google": {
      "api_key": "AIza...",
      "default_model": "gemini-1.5-flash"
    },
    "openai": {
      "api_key": "sk-proj-...",
      "default_model": "gpt-4o"
    },
    "ollama": {
      "base_url": "http://localhost:11434",
      "default_model": "llama3.1:70b"
    }
  },
  "chromadb": {
    "host": "localhost",
    "port": 8000,
    "persist_directory": "./chroma_data"
  },
  "security": {
    "jwt_secret": "your-random-secret-key-here",
    "session_duration_hours": 24
  },
  "server": {
    "host": "0.0.0.0",
    "port": 5000,
    "debug": false
  }
}
```

**Critical:** Replace placeholder values with your actual credentials.

For testing without real database, use SQLite:

```json
"database": {
  "type": "sqlite",
  "path": "./test.db"
}
```

**INITIALIZE DATABASE:**

Create user management tables:

```bash
python maintenance/initialize_database.py
```

Output:

```
INITIALIZATION:

Creating tables...
‚úì users table created
‚úì profiles table created
‚úì consumption_logs table created
‚úì api_tokens table created

Creating admin user...
‚úì Admin: admin@uderia.local / admin123

Initialization complete!
```

Default admin credentials created.

**START CHROMADB:**

Separate terminal:

```bash
chroma run --path ./chroma_data --port 8000
```

Output:

```
Running Chroma
Listening on http://localhost:8000
```

Vector store ready.

**START UDERIA:**

Original terminal:

```bash
python src/trusted_data_agent/main.py
```

Output:

```
üöÄ UDERIA PLATFORM STARTING...

‚úì Configuration loaded
‚úì Database connected: Teradata (fitness_db)
‚úì ChromaDB connected: localhost:8000
‚úì LLM Providers initialized:
  ‚Ä¢ Anthropic (Claude Sonnet 3.5)
  ‚Ä¢ Google AI (Gemini 1.5 Flash)
  ‚Ä¢ OpenAI (GPT-4o)
  ‚Ä¢ Ollama (Llama 3.1 70B)
‚úì MCP Servers loaded: 4 active
‚úì Profile system ready: 6 default profiles

üåê Server running on http://localhost:5000

Press CTRL+C to stop
```

Platform running!

**FIRST ACCESS:**

Open browser: `http://localhost:5000`

Login screen appears.

**Credentials:**
- Username: `admin@uderia.local`
- Password: `admin123`

Click "Login" ‚Üí Dashboard loads.

You see:
- Welcome message
- 6 default profiles (@ANALYST, @DBA, @QUALITY, @LOCAL, @PROD, @COST)
- "Start Conversation" button

**VERIFICATION TEST:**

Click @ANALYST ‚Üí "Start Conversation."

**Query:** "List all databases available."

Response:

```
VERIFICATION RESULTS:

Available databases:
1. fitness_db (default)
   ‚Ä¢ Tables: Customers, Products, Sales, ServiceTickets
   ‚Ä¢ Size: 2.3 GB
   ‚Ä¢ Last updated: Dec 7, 2025

Database connection: ‚úì Successful
MCP tools: ‚úì Working
LLM provider: ‚úì Gemini Flash responding
```

Installation verified!

**TROUBLESHOOTING:**

Common issues:

```
ISSUE 1: "Database connection failed"
Fix:
‚Ä¢ Check tda_config.json credentials
‚Ä¢ Verify server reachable: ping your-server.com

ISSUE 2: "ChromaDB not responding"
Fix:
‚Ä¢ Start ChromaDB first: chroma run --port 8000
‚Ä¢ Check port not in use: lsof -i :8000

ISSUE 3: "LLM provider error"
Fix:
‚Ä¢ Verify API keys valid
‚Ä¢ Test: curl to provider API directly

ISSUE 4: "Module not found"
Fix:
‚Ä¢ Activate virtual environment: source venv/bin/activate
‚Ä¢ Reinstall: pip install -r requirements.txt
```

From "interested observer" to "platform owner."

Installed. Configured. Running. Your Uderia instance is live.
```

**Screen Capture Plan:**
- [ ] Terminal: Prerequisites check
  - `python3 --version` ‚Üí Python 3.13.0
  - `node --version` ‚Üí v20.11.0
  - `git --version` ‚Üí git 2.43.0
- [ ] Browser: GitHub repository page
  - https://github.com/your-org/uderia
  - Green "Code" button
  - Copy HTTPS URL
- [ ] Terminal: Clone repository
  - `cd ~/projects`
  - `git clone https://github.com/your-org/uderia.git`
  - Progress: Cloning... 100%
  - `cd uderia`
  - `ls -la` showing repository structure
- [ ] Terminal: Virtual environment
  - `python3 -m venv venv`
  - `source venv/bin/activate`
  - Prompt changes to `(venv) $`
- [ ] Terminal: Install dependencies
  - `pip install --upgrade pip`
  - `pip install -r requirements.txt`
  - Progress bar, package installation
  - "Successfully installed" message with 47 packages
- [ ] VS Code: Open `tda_config.json`
  - Show database configuration section
  - Show LLM providers section with API keys (masked)
  - Show ChromaDB configuration
  - Show security settings
  - Highlight: "Replace with your credentials"
- [ ] Terminal: Initialize database
  - `python maintenance/initialize_database.py`
  - Output showing table creation
  - Admin user created message
- [ ] Terminal 2: Start ChromaDB
  - `chroma run --path ./chroma_data --port 8000`
  - "Running Chroma" message
  - "Listening on http://localhost:8000"
- [ ] Terminal 1: Start Uderia
  - `python src/trusted_data_agent/main.py`
  - Startup sequence (7 checkmarks)
  - "Server running on http://localhost:5000"
- [ ] Browser: Navigate to http://localhost:5000
  - Login screen appears
  - Enter: admin@uderia.local / admin123
  - Click "Login"
- [ ] Browser: Dashboard loads
  - Welcome message
  - 6 profile cards displayed
  - "Start Conversation" button
- [ ] Browser: Click @ANALYST ‚Üí Start Conversation
  - Input: "List all databases available."
  - Send
  - Response showing databases, tables, verification checkmarks
- [ ] Troubleshooting overlay:
  - 4 common issues with fixes
  - Terminal commands for debugging

**Supporting Materials:**
- README.md complete installation section
- Installation and Setup comprehensive guide
- Troubleshooting documentation with 20+ common issues
- Configuration reference for all tda_config.json options
- Video walkthrough (installation screencast)
- Docker alternative guide (Module 9.2)

---

## üé≠ Profile Ecosystem Reference
**Complete Persona Lineup Used Throughout Tutorials**

This section defines the core profiles referenced across all modules and demonstrated in user journey scenarios.

---

### Profile 1: @ANALYST - Business Intelligence Persona

**User Role:** Business Analyst / Product Manager / Marketing Analyst  
**LLM Provider:** Gemini 1.5 Flash  
**Cost Profile:** Budget-conscious ($0.075 per 1M input tokens)  
**Speed:** Fast (optimized for exploratory work)

**MCP Tools Available:**
- `base_databaseList` - List databases
- `base_tableList` - List tables  
- `base_tableDDL` - Get table structure
- `base_readQuery` - Execute SQL queries
- `TDA_Charting` - Generate visualizations (bar charts, line graphs, pie charts)
- `TDA_LLMTask` - Natural language analysis and synthesis
- `TDA_FinalReport` - Format executive-ready reports
- `TDA_CurrentDate` - Date/time utilities

**MCP Prompts Available:**
- `base_databaseBusinessDesc` - Business context for databases
- `base_tableBusinessDesc` - Business context for tables

**Knowledge Collections Attached:**
- "Business Metrics Definitions" (15 documents) - KPI calculation formulas, reporting standards
- "fitness_db Business Context" (8 documents) - Product categories, customer segments, sales cycles
- "Executive Reporting Templates" (12 documents) - Dashboard layouts, summary formats

**Planner Repositories Attached:**
- "Executive_Reporting_Patterns" (23 champion cases) - Board presentations, monthly reports
- "Customer_Segmentation_Analysis" (17 champion cases) - Demographic analysis, cohort studies
- "Trend_Analysis_Patterns" (31 champion cases) - Time-series queries, YoY comparisons

**Primary Use Cases:**
- Revenue analysis and trending
- Customer demographics and segmentation
- Product performance analysis
- Executive dashboards and visualizations
- Business insight generation

**Behavioral Characteristics:**
- Business-friendly language (avoids technical jargon)
- Always includes visualizations when appropriate
- Frames insights in actionable recommendations
- Focuses on "why" and "so what" not just "what"

**Tutorial Appearances:**
- Module 1.3: First conversation (revenue analysis)
- Module 2.2: Profile introduction and "god mode" demonstration
- Module 4.1: Strategic planning with business context
- Module 6.2: Marketplace planner repository usage
- Module 10.1: Complete business analyst journey

---

### Profile 2: @DBA - Database Administrator Persona

**User Role:** Database Administrator / Data Engineer / SQL Developer  
**LLM Provider:** Claude Sonnet 3.5  
**Cost Profile:** Balanced ($3 per 1M input tokens)  
**Focus:** Reliability and SQL optimization

**MCP Tools Available:**
- `base_databaseList` - List databases
- `base_tableList` - List tables
- `base_tableDDL` - Get table DDL
- `base_readQuery` - Execute SQL queries
- `dba_databaseVersion` - System version info
- `qlty_profileColumn` - Data profiling and statistics
- `qlty_validateData` - Data quality checks
- `TDA_FinalReport` - Technical report formatting
- `TDA_SystemLog` - System logging

**MCP Prompts Available:**
- `dba_databaseLineage` - Data lineage analysis
- `dba_databaseHealthAssessment` - System health diagnostics
- `dba_userActivityAnalysis` - User activity tracking
- `dba_systemVoice` - System status narratives
- `dba_tableArchive` - Archiving recommendations
- `dba_tableDropImpact` - Impact analysis for schema changes

**Knowledge Collections Attached:**
- "Teradata SQL Optimization Guide" (23 documents) - Query performance, index strategies, join optimization
- "fitness_db Schema Documentation" (5 documents) - Table relationships, constraints, business rules
- "Database Performance Tuning" (18 documents) - Execution plan analysis, statistics management
- "SQL Best Practices" (14 documents) - Code standards, anti-patterns

**Planner Repositories Attached:**
- "DBA_Champion_Cases" (47 champion cases) - Optimized SQL patterns, performance-tuned queries
- "Performance_Troubleshooting_Playbooks" (22 cases) - Diagnostic workflows, optimization strategies
- "Schema_Analysis_Patterns" (15 cases) - DDL extraction, relationship mapping

**Primary Use Cases:**
- Query optimization and tuning
- Schema analysis and documentation
- Performance troubleshooting
- Data profiling and quality assessment
- System health monitoring

**Behavioral Characteristics:**
- Technical precision (includes execution plans, statistics)
- Optimization-focused (always suggests performance improvements)
- Security-conscious (validates permissions, checks constraints)
- Uses technical terminology appropriately

**Tutorial Appearances:**
- Module 2.2: Profile creation and "god mode" demonstration (first persona)
- Module 3.2: Capabilities discovery (MCP tools showcase)
- Module 4.1: Strategic planning with SQL optimization
- Module 8.2: System administration tasks
- Module 10.2: Complete DBA journey

---

### Profile 3: @QUALITY - Data Quality Engineer Persona

**User Role:** Data Quality Analyst / Data Steward / Governance Specialist  
**LLM Provider:** Claude Sonnet 3.5  
**Cost Profile:** Balanced ($3 per 1M input tokens)  
**Focus:** Thoroughness and validation

**MCP Tools Available:**
- `base_readQuery` - Data sampling queries
- `base_tableList` - List tables for audit
- `base_tableDDL` - Get table structure for validation
- `qlty_profileColumn` - Column profiling (nulls, distributions, outliers)
- `qlty_validateData` - Constraint validation, referential integrity
- `TDA_LLMTask` - Anomaly analysis and pattern detection
- `TDA_FinalReport` - Quality report formatting
- `TDA_SystemLog` - Audit logging

**MCP Prompts Available:**
- `qlty_databaseQuality` - Comprehensive quality assessment
- `base_tableBusinessDesc` - Business rules verification

**Knowledge Collections Attached:**
- "Data Quality Standards" (20 documents) - Organization-specific validation rules, acceptable ranges
- "fitness_db Constraints Documentation" (6 documents) - Expected value ranges, referential integrity rules
- "Industry Data Validation Best Practices" (16 documents) - ISO standards, quality frameworks
- "Anomaly Detection Techniques" (11 documents) - Statistical methods, outlier identification

**Planner Repositories Attached:**
- "Quality_Validation_Patterns" (29 champion cases) - Standard validation workflows, constraint checks
- "Anomaly_Detection_Workflows" (18 cases) - Statistical analysis, outlier investigation
- "Data_Profiling_Templates" (24 cases) - Column analysis, distribution studies

**Primary Use Cases:**
- Data quality audits
- Constraint validation
- Anomaly detection
- Data profiling and statistics
- Compliance verification

**Behavioral Characteristics:**
- Thorough and methodical (validates all constraints)
- Flags issues with severity levels (critical, warning, info)
- Provides remediation recommendations
- Documents quality scores and metrics

**Tutorial Appearances:**
- Module 2.2: Profile introduction and "god mode" demonstration (second persona)
- Module 3.1: Transparent execution (validation workflows)
- Module 10.3: Complete data quality analyst journey

---

### Profile 4: @LOCAL - Privacy-First Persona

**User Role:** Compliance Officer / Privacy-Conscious User / Regulated Industry Analyst  
**LLM Provider:** Ollama Llama 3.1 70B (On-Premises)  
**Cost Profile:** Zero cloud cost ($0.00)  
**Deployment:** Fully local execution, zero external API calls

**MCP Tools Available:**
- Same tools as @ANALYST profile
- All execution happens on-premises
- No data leaves organizational boundaries

**Knowledge Collections Attached:**
- Same collections as @ANALYST
- RAG retrieval happens locally
- Documents stored on-premises

**Planner Repositories Attached:**
- Same patterns as @ANALYST
- Champion cases stored locally
- Learning happens within sovereign infrastructure

**Primary Use Cases:**
- Customer PII analysis (names, emails, addresses)
- Financial data analysis (sensitive transactions)
- Healthcare data (HIPAA compliance)
- Regulated industry workflows
- Competitive intelligence (confidential data)

**Behavioral Characteristics:**
- Identical capabilities to cloud profiles
- Zero latency for data access (local MCP tools)
- Slightly slower LLM inference (local model)
- Complete data sovereignty guaranteed

**Key Differentiator:**
- Cloud-level reasoning architecture (strategic planning, RAG, optimization)
- Local execution (all sensitive data stays on-premises)
- No compromise on intelligence or features

**Tutorial Appearances:**
- Module 2.2: Profile introduction and "god mode" demonstration (fourth persona)
- Module 5.2: Hybrid intelligence (local model showcase)
- Module 5.3: Security and sovereignty
- Module 10.4: Complete compliance officer journey

---

### Profile 5: @PROD - Production Workhorse Persona

**User Role:** Default profile for automation, API calls, scheduled jobs  
**LLM Provider:** Claude Sonnet 3.5  
**Cost Profile:** Balanced ($3 per 1M input tokens)  
**Focus:** Reliability for production workloads

**MCP Tools Available:**
- All `base_*` tools (complete database access)
- All `TDA_*` platform tools (charting, reporting, logging)
- `dba_databaseVersion` (system info)
- `qlty_*` tools (optional, can be enabled)

**Knowledge Collections Attached:**
- "General Purpose Documentation" (12 documents) - Cross-domain knowledge base
- "fitness_db Complete Schema Guide" (10 documents) - Full database documentation

**Planner Repositories Attached:**
- "Cross_Domain_Champion_Cases" (89 cases) - Diverse query patterns from all domains
- "Production_Automation_Patterns" (34 cases) - Scheduled reports, batch jobs

**Primary Use Cases:**
- REST API automation
- Apache Airflow DAG execution
- Scheduled batch reports
- Production data pipelines
- Generic multi-purpose queries

**Behavioral Characteristics:**
- Reliable and consistent (minimizes experimental features)
- Optimized for automation (predictable outputs)
- Error-tolerant (graceful handling of failures)
- Well-documented results (structured JSON outputs)

**Tutorial Appearances:**
- Module 2.1: Two-in-one paradigm (conversational ‚Üí API)
- Module 2.3: REST API demonstrations
- Module 2.4: Airflow integration
- Module 9.1: Flowise workflow automation
- Module 10.5: "God mode" orchestration (as production executor)

---

### Profile 6: @COST - Budget-Conscious Persona (Optional)

**User Role:** Cost optimization, high-volume queries, development/testing  
**LLM Provider:** Gemini 1.5 Flash  
**Cost Profile:** Minimal ($0.075 per 1M input tokens - cheapest available)  
**Focus:** Maximum cost efficiency

**MCP Tools Available:**
- Same as @PROD (all standard tools)

**Knowledge Collections Attached:**
- Minimal collections (reduces RAG retrieval overhead)
- "Essential Reference Only" (3 documents)

**Planner Repositories Attached:**
- "Fast_Path_Patterns" (56 cases) - Optimized for speed and low token usage
- "Simple_Query_Templates" (42 cases) - Straightforward, single-phase executions

**Primary Use Cases:**
- High-frequency simple queries
- Development and testing environments
- Batch jobs with simple logic
- Proof-of-concept demonstrations
- Training and learning scenarios

**Behavioral Characteristics:**
- Prefers fast-path execution (bypasses strategic planning when possible)
- Minimizes token usage (concise outputs)
- Leverages RAG heavily (reuses patterns instead of replanning)
- Avoids complex multi-phase strategies

**Tutorial Appearances:**
- Module 4.4: Cost tracking and optimization
- Module 7.2: Financial governance (cost comparison)
- Module 10.5: "God mode" orchestration (optional cost-conscious execution)

---

## Module 10: User Journey Scenarios
**Goal:** Demonstrate end-to-end workflows mapping personas to real-world user stories

### 10.1 Business Analyst Journey - @ANALYST Profile
**Duration:** 5-6 minutes  
**Persona:** Sarah, Product Manager preparing Q4 board presentation  
**Profile Used:** Primarily @ANALYST, brief @QUALITY validation

**Learning Objectives:**
- Follow complete exploratory data analysis workflow
- See conversational iteration and refinement
- Understand context retention across multi-turn sessions
- Demonstrate visualization and reporting

**Scenario Narrative:**
Sarah needs to present Q4 fitness equipment sales performance to the board. She has 2 hours to prepare insights.

**Key Topics to Cover:**
- [ ] Initial exploratory question: "What are Q4 revenue trends by product category?"
- [ ] Drilling down: "Why did Cardio equipment decline 18%?"
- [ ] Customer analysis: "Show demographics of customers who stopped buying Cardio"
- [ ] Correlation: "Is there a relationship between service tickets and Cardio sales decline?"
- [ ] Visualization: Generate executive-ready charts
- [ ] Quick validation: Switch to @QUALITY to verify data completeness
- [ ] Final synthesis: "Generate executive summary with recommendations"
- [ ] Export insights and share with team

**Narration Script:**
```
Meet Sarah. Product Manager. Board presentation in 2 hours.

Topic: Q4 fitness equipment sales performance.

Traditional approach: Email data team ‚Üí Wait 4 hours ‚Üí Generic report ‚Üí Still have questions ‚Üí Email again ‚Üí Miss deadline.

Uderia approach: Ask. Refine. Visualize. Present. Done.

**THE WORKFLOW:**

Sarah logs into Uderia. Profile selector shows 6 options. Selects **@ANALYST** (Gemini Flash + business tools).

Chat opens. Profile indicator: "@ANALYST Active."

**QUERY 1: EXPLORATORY**

Sarah types: *"What are Q4 2024 revenue trends by product category?"*

@ANALYST activates:

```
STRATEGIC PLANNING:
‚îú‚îÄ Tools needed: base_readQuery, TDA_Charting
‚îú‚îÄ Champion case: "Trend_Analysis_Patterns" (31 cases)
‚îî‚îÄ Knowledge: "Business Metrics Definitions"
```

Transparent execution:

```
üîß Tool: base_readQuery
   Query: SELECT ProductType, SUM(TotalAmount) as Revenue
          FROM Sales s JOIN Products p ON s.ProductID = p.ProductID
          WHERE SaleDate >= '2024-10-01' AND SaleDate <= '2024-12-31'
          GROUP BY ProductType

üìä Tool: TDA_Charting (Bar Chart)
   Categories: Cardio ($287K), Strength ($456K), Accessories ($149K)
```

Response:

```
Q4 2024 REVENUE BY CATEGORY:

[Bar chart visualization]
‚Ä¢ Strength Equipment: $456,230 (51%, +23% vs Q3)
‚Ä¢ Cardio Equipment: $287,450 (32%, -18% vs Q3) ‚ö†Ô∏è
‚Ä¢ Accessories: $149,320 (17%, +8% vs Q3)

Total Q4 Revenue: $892,450

Key Insight: Cardio equipment declined 18% QoQ. Requires investigation.
**QUERY 2: DRILL-DOWN**

Sarah: *"Why did Cardio equipment decline 18%?"*

Context retained. @ANALYST knows we're discussing Cardio from Q4.

```
üîß Tool: base_readQuery (3 queries executed)
   ‚Ä¢ Q4 Cardio sales by month
   ‚Ä¢ Q3 Cardio sales comparison
   ‚Ä¢ Customer purchase frequency analysis

üìä Tool: TDA_Charting (Line Graph)
   Trend: Oct $112K ‚Üí Nov $95K ‚Üí Dec $80K (progressive decline)
```

Response:

```
CARDIO EQUIPMENT DECLINE ANALYSIS:

[Line graph showing monthly decline]

ROOT CAUSE INDICATORS:
1. Progressive monthly decline: Oct‚ÜíNov‚ÜíDec (not one-time event)
2. Customer retention drop: 67% of Q3 Cardio buyers did NOT repurchase
3. Average transaction value stable: $1,245 (not pricing issue)
4. New customer acquisition normal: +12% new buyers

Hypothesis: Existing customer churn, not acquisition problem.
```

Sarah's hypothesis: It's a retention problem.ition problem.

**QUERY 3: CUSTOMER ANALYSIS**

Sarah: *"Show demographics of customers who stopped buying Cardio in Q4"*

```
üîß Tool: base_readQuery
   JOIN: Sales ‚Üí Customers ‚Üí Products
   Filter: Purchased Cardio in Q3 BUT NOT in Q4
   Result: 127 churned Cardio customers identified
```

Response:

```
CHURNED CARDIO CUSTOMER PROFILE (n=127):

Demographics:
‚Ä¢ Age: 78% are 35-54 (prime fitness demographic)
‚Ä¢ Location: 62% in urban areas (Seattle, Portland, SF)
‚Ä¢ Avg Lifetime Value: $3,450 (valuable customers!)
‚Ä¢ Purchase History: Average 3.2 prior Cardio purchases (loyal)

Concern: Losing high-value, loyal customers in key markets.
```

Pattern emerging. Sarah needs one more piece.mers in key markets.

---

Pattern emerging. But Sarah needs one more piece.

**QUERY 4: CORRELATION ANALYSIS**

Sarah: *"Is there a relationship between service tickets and Cardio sales decline?"*

Strategic Planning retrieves case: "Service Impact on Revenue Analysis"

```
üîß Tool: base_readQuery
   JOIN: ServiceTickets ‚Üí Sales ‚Üí Products
   Analysis: Ticket volume vs sales by product type

üìä Tool: TDA_LLMTask
   Correlation: Cardio service tickets +34% in Q4
```

Response:

```
SERVICE TICKET CORRELATION:

Product Type | Q4 Service Tickets | Q4 Revenue Change
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Strength     | 23 (+5%)           | +23% ‚úÖ
Accessories  | 12 (-8%)           | +8% ‚úÖ
Cardio       | 89 (+34%) ‚ö†Ô∏è      | -18% ‚ùå

Critical Finding: Cardio had 34% MORE service tickets in Q4.

Top Issues:
1. Treadmill belt malfunction (23 tickets)
2. Display screen failure (18 tickets)
3. Motor overheating (15 tickets)

Insight: Quality issues driving churn. Customers stopped buying 
         after poor service experiences.
```

Sarah has her story. Now needs validation.

**QUICK VALIDATION: Switch to @QUALITY**

Profile dropdown ‚Üí Select **@QUALITY**

Capabilities reload (Claude Sonnet + validation tools).

Sarah: *"Validate completeness of Q4 sales data for Cardio products"*

```
üîß Tool: qlty_profileColumn
   Table: Sales
   Columns: TotalAmount, SaleDate, ProductID

‚úì Completeness: 100% (no nulls)
‚úì Referential Integrity: All ProductIDs valid
‚úì Date Range: Oct 1 - Dec 31 complete
‚úì Amount Distribution: Normal (no outliers)
```

Response: **Data Quality Score: 99.7/100** ‚úÖ

"Q4 Cardio sales data fully validated. Analysis conclusions are reliable."

---

Data validated. Back to synthesis.

**RETURN TO @ANALYST: Final Report**

Profile dropdown ‚Üí Back to **@ANALYST**

Sarah: *"Generate executive summary with recommendations"*

Strategic Planning activates "Executive_Reporting_Patterns" repository (23 champion cases).

Knowledge retrieval: "Executive Reporting Templates" collection.

```
üîß Tool: TDA_FinalReport
   Format: Executive Summary
   Audience: Board of Directors
   Length: 1-page, 3-minute read
```

Response:

---

**EXECUTIVE SUMMARY: Q4 2024 Fitness Equipment Sales**
*Prepared by: Sarah Mitchell, Product Manager*
*Date: December 7, 2024*

**OVERALL PERFORMANCE**
Q4 total revenue: **$892,450** (+8% vs Q3, -5% vs target)

**KEY FINDINGS**

‚úÖ **Strength Equipment:** $456K, +23% growth (exceeding expectations)
‚úÖ **Accessories:** $149K, +8% growth (on track)
‚ùå **Cardio Equipment:** $287K, -18% decline (critical concern)

**ROOT CAUSE ANALYSIS: CARDIO DECLINE**

**The Problem:**
- 127 high-value customers (avg $3,450 LTV) stopped purchasing Cardio equipment in Q4
- 78% are prime demographic (age 35-54) in key urban markets

**The Cause:**
- Service tickets for Cardio equipment increased 34% (89 tickets)
- Top issues: Treadmill malfunctions (23), display failures (18), motor problems (15)
- Customer sentiment: Quality concerns driving churn

**RECOMMENDATIONS**

1. **Immediate:** Supplier quality audit for Cardio equipment (Est. 2 weeks)
2. **Short-term:** Proactive outreach to 127 churned customers with service credits ($50K budget)
3. **Long-term:** Enhanced quality testing before product launches

**FINANCIAL IMPACT**
- Lost revenue from churned customers: ~$438K annually
- Recovery potential with retention campaign: ~$280K (64% recovery rate based on historical data)
- **Net ROI:** $230K revenue recovery for $50K investment (4.6x return)

**BOARD ASK**
Approve $50K retention campaign budget to recover high-value Cardio customers.

---

Sarah: *Perfect.*

**EXPORT & SHARE**

Click "Export" ‚Üí Download PDF

Email to board members. 

1 hour 45 minutes total. 15 minutes to spare.

**WHAT JUST HAPPENED:**

Traditional approach:
- Data team request: 4 hours
- Generic report received
- Still need answers
- Second request: 4 more hours
- Manually create slides: 2 hours
- **Total: 10+ hours, missed deadline**

Uderia approach with @ANALYST:
- Exploratory query: 30 seconds
- Drill-down analysis: 45 seconds
- Customer demographics: 35 seconds
- Correlation analysis: 40 seconds
- Switch to @QUALITY validation: 20 seconds
- Executive summary generation: 50 seconds
- **Total: 1 hour 45 minutes (including thinking time)**

**THE POWER:**
- **No SQL writing** (business questions in plain English)
- **Context retention** (each query built on previous answers)
- **Automatic visualizations** (charts generated without manual work)
- **Knowledge-enhanced** (champion cases + business context)
- **Multi-profile validation** (@QUALITY confirmed data reliability)
- **Board-ready output** (executive summary formatted automatically)

Sarah went from "question" to "boardroom-ready presentation" in under 2 hours.

One person. One profile. Zero data team bottleneck.

This is @ANALYST. This is Uderia. This is the future of business intelligence.
```

**Screen Capture Plan:**
- [ ] Profile indicator: @ANALYST active
- [ ] Initial query with revenue trends
- [ ] TDA_Charting generates bar chart visualization
- [ ] Follow-up questions building context
- [ ] Switch to @QUALITY briefly for validation
- [ ] Return to @ANALYST for final synthesis
- [ ] TDA_FinalReport creates executive summary
- [ ] Export/download functionality

**Supporting Materials:**
- Complete Q4 sales dataset
- Expected visualizations (mockups)
- Executive summary template

---

### 10.2 Database Administrator Journey - @DBA Profile
**Duration:** 5-6 minutes  
**Persona:** Marcus, DBA investigating slow query performance  
**Profile Used:** Primarily @DBA, switch to @PROD for API testing

**Learning Objectives:**
- Demonstrate technical SQL optimization workflow
- Show schema analysis and profiling
- Illustrate performance tuning process
- Operationalize optimized query via API

**Scenario Narrative:**
Marcus received complaints that the "Monthly Sales Report" query is taking 45 seconds. He needs to optimize it and deploy the fix.

**Key Topics to Cover:**
- [ ] Analyze current query: "Show me the execution plan for monthly sales aggregation"
- [ ] Schema inspection: Use `base_tableDDL` to examine table structures
- [ ] Identify bottleneck: "What indexes exist on the Sales table?"
- [ ] Data profiling: Use `qlty_profileColumn` to understand data distribution
- [ ] Generate optimized SQL: "Rewrite this query with proper index usage"
- [ ] Benchmark: Compare old vs new execution time
- [ ] Document: "Generate technical documentation for this optimization"
- [ ] Operationalize: Switch to @PROD, test via REST API
- [ ] Deploy: Update Airflow DAG with optimized query

**Narration Script:**
```
Meet Marcus. Database Administrator. 15 years tuning SQL.

Just got paged: "Monthly Sales Report taking 45 seconds. Users complaining."

The report runs 200 times per day. 45 seconds √ó 200 = 2.5 hours of cumulative wait time daily.

Traditional approach: Request query ‚Üí Analyze execution plan manually ‚Üí Research index strategy ‚Üí Test locally ‚Üí Document ‚Üí Deploy ‚Üí Hope it works.

Uderia approach: Ask. Analyze. Optimize. Validate. Deploy. Done.

**THE INVESTIGATION:**

Marcus logs into Uderia. Profile selector: **@DBA** (Claude Sonnet + database tools).

Chat opens. Profile indicator: "@DBA Active."

**QUERY 1: ANALYZE CURRENT PERFORMANCE**

Marcus types: *"Show me the execution plan and performance for the monthly sales aggregation query"*

He pastes the problematic query:

```sql
SELECT 
    p.ProductName,
    COUNT(*) as SalesCount,
    SUM(s.TotalAmount) as TotalRevenue,
    AVG(s.TotalAmount) as AvgSaleAmount
FROM Sales s
JOIN Products p ON s.ProductID = p.ProductID
WHERE s.SaleDate >= CURRENT_DATE - INTERVAL '30' DAY
GROUP BY p.ProductName
ORDER BY TotalRevenue DESC;
```

@DBA activates:

```
STRATEGIC PLANNING:
‚îú‚îÄ Champion case: "Performance_Troubleshooting_Playbooks" (22 cases)
‚îî‚îÄ Tool: base_readQuery with EXPLAIN
```

Execution:

```
üîß Tool: base_readQuery (EXPLAIN mode)
   Analyzing query execution plan...
   
‚ö†Ô∏è PERFORMANCE BOTTLENECK DETECTED:
   ‚Ä¢ Full table scan on Sales (237,450 rows)
   ‚Ä¢ No index on SaleDate column
   ‚Ä¢ JOIN operation: 237K √ó 1,285 rows = 305M comparisons
   ‚Ä¢ Execution time: 45.3 seconds
```

Response:

```
PERFORMANCE ANALYSIS: Monthly Sales Report

Current Performance: 45.3 seconds ‚ùå

Execution Plan Issues:
1. Full table scan on Sales table (237,450 rows scanned)
2. No index utilized for SaleDate filter
3. Inefficient JOIN (nested loop with 305M row comparisons)
4. Temporary sort for ORDER BY (67MB memory usage)

Impact:
‚Ä¢ Query runs 200x daily
‚Ä¢ Cumulative wait time: 2.5 hours/day
‚Ä¢ Database load: 15% CPU usage during each execution

Recommendation: Index creation + query restructuring required.
```

Marcus needs to see the schema.

**QUERY 2: SCHEMA INSPECTION**

Marcus: *"Show me the DDL for the Sales table, focusing on indexes and constraints"*

```
üîß Tool: base_tableDDL
   Table: Sales | Including: Indexes, Primary Keys, Foreign Keys
```

Response:

```sql
CREATE TABLE Sales (
    SaleID INTEGER PRIMARY KEY,
    SaleDate DATE NOT NULL,
    CustomerID INTEGER,
    ProductID INTEGER,
    TotalAmount DECIMAL(10,2),
    FOREIGN KEY (CustomerID) REFERENCES Customers(CustomerID),
    FOREIGN KEY (ProductID) REFERENCES Products(ProductID)
);

-- Existing Indexes:
PRIMARY KEY idx_sales_pk ON (SaleID)
-- ‚ö†Ô∏è NO index on SaleDate
-- ‚ö†Ô∏è NO index on ProductID (FK not indexed)

CRITICAL FINDINGS:
1. Missing index on SaleDate (filter column)
2. Missing index on ProductID (JOIN column, despite being FK)
3. Primary key only (SaleID not used in this query)

Schema Issue Confirmed: Date filters and JOINs have no index support.
```

Marcus needs data distribution insights before recommending indexes.

**QUERY 3: DATA PROFILING**

Marcus: *"Profile the SaleDate and ProductID columns to understand data distribution and cardinality"*

```
üîß Tool: qlty_profileColumn
   Table: Sales | Columns: SaleDate, ProductID
   Analysis: Distribution, cardinality, selectivity
```

Response:

```
COLUMN PROFILING RESULTS:

SaleDate Column:
‚Ä¢ Date range: 2022-01-15 to 2024-12-06 (1,056 days)
‚Ä¢ Distribution: Relatively uniform (220-240 sales per day)
‚Ä¢ Last 30 days: 6,847 rows (2.9% of total)
‚Ä¢ Selectivity: High (good index candidate) ‚úÖ

ProductID Column:
‚Ä¢ Distinct values: 1,285 products
‚Ä¢ Distribution: Skewed (top 20 products = 45% of sales)
‚Ä¢ Average rows per ProductID: 185
‚Ä¢ Selectivity: Moderate (JOIN benefits from index) ‚úÖ

INDEX STRATEGY RECOMMENDATION:
1. Composite index: (SaleDate, ProductID) for optimal performance
2. Alternative: Separate indexes if storage constrained
3. Estimated selectivity improvement: 97.1% (access 2.9% of rows, not 100%)
```

Data supports the index strategy. Now: optimization.

**QUERY 4: GENERATE OPTIMIZED QUERY**
**QUERY 4: GENERATE OPTIMIZED QUERY**

Marcus: *"Rewrite this query with optimal index usage and query structure for Teradata"*

```
STRATEGIC PLANNING:
‚îú‚îÄ Repository: "DBA_Champion_Cases" (47 cases)
‚îî‚îÄ Knowledge: "Teradata SQL Optimization Guide"

üîß Tool: TDA_LLMTask
   Task: SQL optimization with index hints
   Context: Teradata-specific (COLLECT STATISTICS, index strategy)
```

Response:

```sql
-- Step 1: Create optimal indexes (run ONCE)
CREATE INDEX idx_sales_date_product ON Sales (SaleDate, ProductID);
COLLECT STATISTICS ON Sales COLUMN (SaleDate, ProductID);

-- Step 2: Optimized query structure
SELECT 
    p.ProductName,
    COUNT(*) as SalesCount,
    SUM(s.TotalAmount) as TotalRevenue,
    AVG(s.TotalAmount) as AvgSaleAmount
FROM Sales s
INNER JOIN Products p ON s.ProductID = p.ProductID
WHERE s.SaleDate >= CURRENT_DATE - INTERVAL '30' DAY
GROUP BY p.ProductName
ORDER BY TotalRevenue DESC;

OPTIMIZATION TECHNIQUES APPLIED:
‚úÖ Composite index on (SaleDate, ProductID)
‚úÖ COLLECT STATISTICS for query optimizer
‚úÖ INNER JOIN explicit (same result, clearer intent)
‚úÖ Filter-first strategy (date filter before JOIN reduces rows)

Expected Performance: 2-3 seconds (93% improvement)
```

Marcus needs to validate before deploying.
**QUERY 5: BENCHMARK COMPARISON**

Marcus: *"Execute both versions and compare performance"*

```
**QUERY 5: BENCHMARK COMPARISON**

Marcus: *"Execute both versions and compare performance"*

```
üîß Tool: base_readQuery (with timing)
   Test 1: Original query
   Test 2: Optimized query (after index creation)
```

Response:

```
PERFORMANCE BENCHMARK RESULTS:

Metric          | Original | Optimized | Improvement
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Execution Time  | 45.3s    | 3.2s      | 93.0% ‚úÖ
Rows Scanned    | 237,450  | 6,847     | 97.1% ‚úÖ
Memory Used     | 67 MB    | 4 MB      | 94.0% ‚úÖ
CPU Usage       | 15%      | 1.2%      | 92.0% ‚úÖ

DAILY IMPACT:
‚Ä¢ Before: 200 queries √ó 45.3s = 2.5 hours cumulative wait time
‚Ä¢ After: 200 queries √ó 3.2s = 10.7 minutes cumulative wait time
‚Ä¢ Time saved: 2 hours 19 minutes daily per user
‚Ä¢ Database load reduction: 92% less CPU usage

Validation: Optimized query returns identical results ‚úÖ
```

Validated. Now: operationalize for production.
Capabilities reload (production automation tools).

Marcus: *"Generate REST API call for this optimized query with JSON response format"*

```
üîß Tool: TDA_FinalReport
   Format: REST API documentation
   Profile: @PROD (production execution)
```

Response:

**REST API Implementation**

**Endpoint:** POST `/api/v1/query`

**Request:**
```json
{
  "profile": "@PROD",
  "query": "Execute optimized monthly sales report",
  "sql": "SELECT p.ProductName, COUNT(*) as SalesCount, SUM(s.TotalAmount) as TotalRevenue, AVG(s.TotalAmount) as AvgSaleAmount FROM Sales s INNER JOIN Products p ON s.ProductID = p.ProductID WHERE s.SaleDate >= CURRENT_DATE - INTERVAL '30' DAY GROUP BY p.ProductName ORDER BY TotalRevenue DESC;",
  "format": "json"
}
```

**Response (3.2s):**
```json
{
  "status": "complete",
  "execution_time": "3.2s",
  "rows": 1285,
  "data": [
    {"ProductName": "ProRunner 3000", "SalesCount": 234, "TotalRevenue": 291450.00, "AvgSaleAmount": 1245.09},
    {"ProductName": "FlexBench Pro", "SalesCount": 189, "TotalRevenue": 245678.00, "AvgSaleAmount": 1300.41},
    ...
  ]
}
```

**Python Client Example:**
```python
import requests

response = requests.post(
    'https://uderia.company.com/api/v1/query',
    headers={'Authorization': 'Bearer uderia_sk_live_...'},
    json={
        'profile': '@PROD',
        'sql': optimized_query,
        'format': 'json'
    }
)

results = response.json()
print(f"Execution: {results['execution_time']}")  # 3.2s
```

---

**QUERY 6: AIRFLOW DAG UPDATE**

Marcus: *"Generate Airflow DAG code to run this optimized report daily at 6 AM"*

```
üîß Tool: TDA_FinalReport
   Format: Python/Airflow DAG
   Schedule: Daily 6 AM
```

Response:

**Airflow DAG: Monthly Sales Report (Optimized)**

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests

def run_monthly_sales_report():
    """Execute optimized sales report via Uderia API"""
    
    response = requests.post(
        'https://uderia.company.com/api/v1/query',
        headers={'Authorization': f'Bearer {os.environ["UDERIA_TOKEN"]}'},
        json={
            'profile': '@PROD',
            'sql': '''
                SELECT p.ProductName, COUNT(*) as SalesCount,
                       SUM(s.TotalAmount) as TotalRevenue,
                       AVG(s.TotalAmount) as AvgSaleAmount
                FROM Sales s
                INNER JOIN Products p ON s.ProductID = p.ProductID
                WHERE s.SaleDate >= CURRENT_DATE - INTERVAL '30' DAY
                GROUP BY p.ProductName
                ORDER BY TotalRevenue DESC;
            ''',
            'format': 'json'
        }
    )
    
    if response.status_code == 200:
        results = response.json()
        print(f"‚úÖ Report generated: {results['rows']} products")
        print(f"‚ö° Execution time: {results['execution_time']}")
        # Save to S3, send email, etc.
    else:
        raise Exception(f"Report failed: {response.text}")

default_args = {
    'owner': 'marcus',
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'monthly_sales_report_optimized',
    default_args=default_args,
    description='Optimized monthly sales report (3.2s vs 45s)',
    schedule_interval='0 6 * * *',  # Daily at 6 AM
    start_date=datetime(2024, 12, 7),
    catchup=False
)

report_task = PythonOperator(
    task_id='generate_monthly_report',
    python_callable=run_monthly_sales_report,
    dag=dag
)
```

**Deployment:**
1. Save as `dags/monthly_sales_report_optimized.py`
2. Deploy to Airflow
3. Verify execution in Airflow UI
4. Monitor performance (should see 3.2s execution time)

---

**FINAL DOCUMENTATION**

Marcus: *"Generate technical documentation for this optimization"*

```
üîß Tool: TDA_FinalReport
   Format: Technical documentation
   Audience: DBA team
```

Response:

---

**TECHNICAL DOCUMENTATION: Monthly Sales Report Optimization**
*Author: Marcus Chen, DBA*
*Date: December 7, 2024*

**PROBLEM**
Monthly sales aggregation query executing in 45.3 seconds, causing user complaints and database load.

**ROOT CAUSE**
1. Full table scan on Sales table (237,450 rows)
2. No index on SaleDate (date range filter column)
3. No index on ProductID (JOIN column)
4. Inefficient nested loop JOIN

**SOLUTION**
Created composite index: `idx_sales_date_product ON Sales (SaleDate, ProductID)`

**SQL CHANGES**
```sql
-- Index creation
CREATE INDEX idx_sales_date_product ON Sales (SaleDate, ProductID);
COLLECT STATISTICS ON Sales COLUMN (SaleDate, ProductID);

-- Query unchanged (optimizer now uses index automatically)
```

**PERFORMANCE IMPROVEMENT**
- Execution time: 45.3s ‚Üí 3.2s (93% faster)
- Rows scanned: 237K ‚Üí 6.8K (97% reduction)
- Daily time saved: 2h 19m across 200 executions
- CPU usage: 15% ‚Üí 1.2% per execution

**DEPLOYMENT**
- Index created: 2024-12-07 14:23 UTC
- Statistics collected: 2024-12-07 14:25 UTC
- Airflow DAG updated: `monthly_sales_report_optimized.py`
- First production run: 2024-12-08 06:00 UTC

**VALIDATION**
- Query results identical before/after ‚úÖ
- Production API tested: 3.2s execution ‚úÖ
- No regressions on other queries ‚úÖ

**MAINTENANCE**
- Monitor index size (currently 12 MB)
- Re-collect statistics monthly: `COLLECT STATISTICS ON Sales COLUMN (SaleDate, ProductID);`
- Review query plan quarterly

---

Done. Marcus closes laptop.

**WHAT JUST HAPPENED:**

```
TRADITIONAL DBA WORKFLOW:
1. Get slow query from user ‚Üí 15 min
2. Manually run EXPLAIN ‚Üí 10 min
3. Research index strategy ‚Üí 30 min
4. Check table DDL in schema docs ‚Üí 15 min
5. Profile data distribution manually ‚Üí 20 min
6. Design optimal index ‚Üí 20 min
7. Test in dev environment ‚Üí 30 min
8. Write deployment script ‚Üí 15 min
9. Update Airflow DAG ‚Üí 20 min
10. Write documentation ‚Üí 30 min
Total: 3+ hours

UDERIA @DBA WORKFLOW:
1. Paste query, ask for analysis ‚Üí 45 sec
2. Request DDL inspection ‚Üí 30 sec
3. Profile data distribution ‚Üí 35 sec
4. Generate optimized query ‚Üí 50 sec
5. Benchmark performance ‚Üí 40 sec
6. Switch to @PROD, get API code ‚Üí 30 sec
7. Generate Airflow DAG ‚Üí 45 sec
8. Generate documentation ‚Üí 40 sec
Total: 25 minutes
```

**THE POWER:**
- No manual EXPLAIN plans (automated execution plan analysis)
- Automatic DDL retrieval (base_tableDDL tool)
- AI-driven profiling (qlty_profileColumn with interpretation)
- Context-aware optimization (champion cases from 47 DBA patterns)
- Knowledge-enhanced (Teradata SQL optimization guide)
- Production-ready output (REST API + Airflow DAG + documentation)

**OUTCOME:** 93% faster query. 88% faster resolution. Production-deployed.

Marcus went from "performance complaint" to "optimized, documented, and deployed" in 25 minutes.

One DBA. One profile. Zero manual execution plan reading.

This is @DBA. This is Uderia. This is database administration evolved.
```

**SCREEN CAPTURE PLAN:**
- Profile indicator: @DBA active
- Query analysis with `base_readQuery` showing slow performance
- `base_tableDDL` revealing table structure
- `qlty_profileColumn` showing data distribution
- Optimized SQL generation with index hints
- Performance comparison: 45s ‚Üí 3.2s
- Switch to @PROD profile
- REST API call with optimized query
- Airflow DAG update

**SUPPORTING MATERIALS:**
- Sample slow query
- Optimized query comparison
- Performance metrics

---

### 10.3 Data Quality Analyst Journey - @QUALITY Profile
**Duration:** 4-5 minutes  
**Persona:** Priya, Data Steward conducting monthly quality audit  
**Profile Used:** @QUALITY, brief @DBA for schema context

**Learning Objectives:**
- Systematic data quality workflow
- Validation and profiling capabilities
- Anomaly detection
- Compliance-ready quality report

**Scenario:**
Priya performs monthly data quality audits for the fitness_db database. Must validate constraints, identify anomalies, and report findings.

**Key Topics:**
- Comprehensive quality assessment
- Constraint validation: referential integrity, null constraints
- Column profiling with `qlty_profileColumn`
- Anomaly detection in Sales.TotalAmount
- Business rule validation: SaleDetails.UnitPrice vs Products.Price
- Switch to @DBA for schema context
- Quality score and remediation recommendations
- Compliance audit report

**Narration Script:**
```
Meet Priya. Data Quality Analyst. Guardian of data integrity.

Monthly audit due. fitness_db database. 5 tables. 250,000+ rows.

Traditional approach: Write validation queries ‚Üí Export to Excel ‚Üí Check constraints ‚Üí Flag anomalies ‚Üí Write report ‚Üí 6 hours.

Uderia approach: Ask. Validate. Profile. Detect. Report. Done.

**THE AUDIT:**

Priya logs into Uderia. Selects **@QUALITY** (Claude Sonnet + quality tools).

Chat opens. Profile indicator: "@QUALITY Active."

**QUERY 1: COMPREHENSIVE QUALITY ASSESSMENT**

Priya types: *"Perform comprehensive quality assessment on fitness_db database"*

@QUALITY activates:

```
STRATEGIC PLANNING:
‚îú‚îÄ Repository: "Quality_Validation_Patterns" (29 champion cases)
‚îî‚îÄ Knowledge: "Data Quality Standards" (20 documents)
```

Execution:

```
üîß Tool: base_tableList
   Database: fitness_db
   Tables: Customers, Products, Sales, SaleDetails, ServiceTickets

üîß Tool: qlty_validateData
   Running constraint checks across all tables...
   
üìä Validation Progress:
   ‚Ä¢ NULL constraint validation
   ‚Ä¢ Referential integrity checks
   ‚Ä¢ Data type consistency
   ‚Ä¢ Value range validation
```

Response:

```
DATA QUALITY ASSESSMENT: fitness_db

Overall Quality Score: 97.3/100 ‚≠ê

Tables Audited: 5
Total Rows: 252,847
Validation Rules Applied: 47

**Quality Breakdown:**

| Table | Rows | Completeness | Integrity | Accuracy | Score |
|-------|------|--------------|-----------|----------|-------|
| Customers | 8,945 | 100% | 100% | 99.2% | 99.7/100 |
| Products | 1,285 | 100% | 100% | 100% | 100/100 |
| Sales | 237,450 | 99.8% | 99.9% | 98.1% | 97.9/100 |
| SaleDetails | 4,892 | 100% | 99.7% | 97.8% | 98.5/100 |
| ServiceTickets | 275 | 100% | 100% | 92.4% | 97.5/100 |

**Issues Detected:** 6 (3 critical, 2 warnings, 1 info)

---

Good overall score. But those issues need investigation.

**QUERY 2: CONSTRAINT VALIDATION DETAILS**

Priya: *"Show me details on the constraint violations found"*

```
üîß Tool: qlty_validateData (detailed mode)
   Focus: Referential integrity, NULL constraints
```

Response:

**Constraint Violation Details**

**CRITICAL ISSUES (3):**

**1. Orphaned Sales Records** üî¥
- **Table:** Sales
- **Issue:** 437 rows with CustomerID not in Customers table
- **Impact:** Referential integrity violated
- **Sample IDs:** 182547, 189234, 195678 (CustomerID: 9999, 9998, 9997)
- **Root Cause:** Test data not cleaned after migration
- **Remediation:** DELETE or UPDATE with valid CustomerIDs

**2. SaleDetails Price Mismatch** üî¥
- **Table:** SaleDetails
- **Issue:** 12 rows where UnitPrice ‚â† Products.Price
- **Impact:** Revenue calculation discrepancies
- **Sample:** SaleDetailID 4523 (UnitPrice: $1,299 vs Products.Price: $1,199)
- **Potential Loss:** $876 revenue misreported
- **Remediation:** Verify pricing rules, correct discrepancies

**3. NULL TotalAmount** üî¥
- **Table:** Sales
- **Issue:** 5 rows with NULL TotalAmount (should be NOT NULL)
- **Impact:** Revenue reporting incomplete
- **SaleIDs:** 234789, 234790, 234812, 235001, 235002
- **Date Range:** December 5-6, 2024 (data entry error?)
- **Remediation:** Calculate and populate from SaleDetails

**WARNINGS (2):**

**4. Duplicate Customer Emails** üü°
- **Table:** Customers
- **Issue:** 18 email addresses used by multiple CustomerIDs
- **Example:** john.smith@email.com (CustomerID: 1234, 5678)
- **Impact:** Potential data quality issue or valid shared accounts
- **Recommendation:** Review and consolidate or flag as shared

**5. Future ServiceTicket Dates** üü°
- **Table:** ServiceTickets
- **Issue:** 3 tickets dated in future (TicketDate > CURRENT_DATE)
- **IDs:** 256, 267, 271 (dates: Dec 15-18, 2024)
- **Impact:** Data entry error or system clock issue
- **Remediation:** Correct dates to valid values

**INFO (1):**

**6. Low Stock Products** ‚ÑπÔ∏è
- **Table:** Products
- **Issue:** 23 products with StockQuantity < 10
- **Impact:** Potential stockout risk (not a data quality issue)
- **Recommendation:** Inventory replenishment (business process)

---

Priya now has specific issues to investigate. Time for detailed profiling.

**QUERY 3: COLUMN PROFILING**

Priya: *"Profile the Sales.TotalAmount column to identify outliers and distribution patterns"*

```
üîß Tool: qlty_profileColumn
   Table: Sales
   Column: TotalAmount
   Analysis: Distribution, outliers, statistical summary
```

Response:

**Column Profile: Sales.TotalAmount**

**Statistical Summary:**
- **Row Count:** 237,450 (excluding 5 NULLs)
- **Min:** $15.99
- **Max:** $18,450.00
- **Mean:** $875.34
- **Median:** $624.50
- **Std Dev:** $456.23

**Distribution:**
```
$0-500:    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 45.2% (107,321 rows)
$501-1000: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 28.7% (68,128 rows)
$1001-2000: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 18.3% (43,453 rows)
$2001-5000: ‚ñà‚ñà‚ñà 6.9% (16,384 rows)
$5000+:     ‚ñå 0.9% (2,164 rows)
```

**Outliers Detected (3):** üö®

| SaleID | TotalAmount | Date | ProductID | Status |
|--------|-------------|------|-----------|--------|
| 189456 | **$18,450** | Nov 23 | 1047 | ‚ö†Ô∏è 21x standard deviation |
| 201234 | **$16,890** | Dec 1 | 1047 | ‚ö†Ô∏è 19x standard deviation |
| 198765 | **$0.01** | Nov 30 | 523 | ‚ö†Ô∏è Test transaction? |

**Analysis:**
- **High outliers (2):** ProductID 1047 = "Commercial Gym Bundle" ($18K legitimate)
- **Low outlier (1):** $0.01 likely test/error transaction
- **Recommendation:** Verify $0.01 transaction, flag commercial bundles separately

---

Outliers explained. Now: business rule validation.

**QUERY 4: BUSINESS RULE VALIDATION**

Priya: *"Verify that SaleDetails.UnitPrice matches Products.Price for all sales transactions"*

```
üîß Tool: base_readQuery
   Query: JOIN SaleDetails ‚Üí Products
   Validation: UnitPrice consistency check
```

Response:

**Business Rule Validation: Price Consistency**

**Rule:** SaleDetails.UnitPrice MUST equal Products.Price at time of sale

**Validation Results:**
- **Total SaleDetails Records:** 4,892
- **Matching Prices:** 4,880 (99.8%) ‚úÖ
- **Price Mismatches:** 12 (0.2%) ‚ùå

**Mismatches Identified:**

| SaleDetailID | ProductName | Expected | Actual | Diff | SaleDate |
|--------------|-------------|----------|--------|------|----------|
| 4523 | ProRunner 3000 | $1,199 | $1,299 | +$100 | Dec 3 |
| 4534 | FlexBench Pro | $899 | $799 | -$100 | Dec 3 |
| 4567 | Yoga Mat Premium | $49 | $59 | +$10 | Dec 4 |
| 4589 | Resistance Bands | $29 | $19 | -$10 | Dec 4 |
| ... | (8 more) | ... | ... | ... | ... |

**Total Revenue Impact:** +$876 (net overcharge)

**Root Cause Hypothesis:**
- All mismatches in Dec 3-4 (same date range)
- Pattern suggests manual price override during promotion
- Promotion pricing not reflected in Products table

**Remediation:**
1. **Immediate:** Correct SaleDetails records or Products.Price
2. **Process:** Implement promotion price tracking
3. **Audit:** Review all December promotional sales

---

Priya needs context on table relationships. Quick switch to @DBA.

**BRIEF SWITCH TO @DBA**

Profile dropdown ‚Üí **@DBA**

Priya: *"Show me the DDL for SaleDetails table to understand constraints and relationships"*

```
üîß Tool: base_tableDDL
   Table: SaleDetails
```

Response:

```sql
CREATE TABLE SaleDetails (
    SaleDetailID INTEGER PRIMARY KEY,
    SaleID INTEGER NOT NULL,
    ProductID INTEGER NOT NULL,
    Quantity INTEGER NOT NULL CHECK (Quantity > 0),
    UnitPrice DECIMAL(10,2) NOT NULL CHECK (UnitPrice >= 0),
    FOREIGN KEY (SaleID) REFERENCES Sales(SaleID),
    FOREIGN KEY (ProductID) REFERENCES Products(ProductID)
);
```

**Constraints:**
‚úÖ Primary Key: SaleDetailID
‚úÖ Foreign Keys: SaleID ‚Üí Sales, ProductID ‚Üí Products
‚úÖ NOT NULL: All critical fields
‚ö†Ô∏è **Missing:** UnitPrice vs Products.Price validation constraint

**Recommendation:** Add CHECK constraint or trigger to enforce price consistency.

---

Context obtained. Back to quality report.

**RETURN TO @QUALITY**

Profile dropdown ‚Üí **@QUALITY**

Priya: *"Generate monthly data quality audit report with all findings and remediation recommendations"*

Strategic Planning: "Data_Profiling_Templates" repository (24 champion cases)

Knowledge: "Industry Data Validation Best Practices" collection

```
üîß Tool: TDA_FinalReport
   Format: Compliance audit report
   Audience: Data Governance Committee
```

Response:

---

**MONTHLY DATA QUALITY AUDIT REPORT**
*fitness_db Database*
*Audit Period: December 2024*
*Auditor: Priya Sharma, Data Quality Analyst*
*Date: December 7, 2024*

**EXECUTIVE SUMMARY**

**Overall Quality Score: 97.3/100** ‚≠ê (Target: ‚â•95%)

fitness_db database quality meets organizational standards with minor issues requiring remediation.

**Audit Scope:**
- **Tables:** 5 (Customers, Products, Sales, SaleDetails, ServiceTickets)
- **Total Records:** 252,847
- **Validation Rules:** 47 automated checks
- **Execution Time:** 4 minutes 23 seconds

**KEY FINDINGS**

**‚úÖ STRENGTHS**
1. **High completeness:** 99.9% of required fields populated
2. **Strong referential integrity:** 99.8% FK relationships valid
3. **Consistent data types:** 100% type conformance
4. **Products table:** Perfect quality score (100/100)

**‚ùå CRITICAL ISSUES (3)**

**Issue 1: Orphaned Sales Records** üî¥
- **Count:** 437 records (0.18% of Sales table)
- **Impact:** Revenue attribution impossible, reporting inaccurate
- **Severity:** HIGH
- **Remediation:** DELETE invalid records or reconcile CustomerIDs
- **Owner:** Database Administration
- **Deadline:** December 15, 2024

**Issue 2: Price Mismatches (SaleDetails vs Products)** üî¥
- **Count:** 12 records (0.25% of SaleDetails)
- **Financial Impact:** $876 net revenue discrepancy
- **Severity:** HIGH
- **Remediation:** Implement promotional pricing tracking
- **Owner:** Finance & IT
- **Deadline:** December 20, 2024

**Issue 3: NULL Revenue Values** üî¥
- **Count:** 5 Sales records with NULL TotalAmount
- **Impact:** Incomplete Q4 revenue reporting
- **Severity:** CRITICAL
- **Remediation:** Calculate and populate from SaleDetails immediately
- **Owner:** Data Engineering
- **Deadline:** December 8, 2024 (within 24 hours)

**‚ö†Ô∏è WARNINGS (2)**

**Warning 1: Duplicate Customer Emails** üü°
- **Count:** 18 email addresses, 36+ customer records
- **Risk:** Marketing communications, account merges
- **Recommendation:** Review for consolidation
- **Priority:** MEDIUM

**Warning 2: Future-Dated Service Tickets** üü°
- **Count:** 3 records with dates > CURRENT_DATE
- **Likely Cause:** Data entry error or system clock issue
- **Recommendation:** Correct dates to valid values
- **Priority:** LOW

**OUTLIER ANALYSIS**

**Sales.TotalAmount Outliers:**
- 2 legitimate commercial sales ($18,450 and $16,890)
- 1 test transaction ($0.01) - recommend deletion

**DATA QUALITY SCORECARD**

| Dimension | Score | Status | Notes |
|-----------|-------|--------|-------|
| **Completeness** | 99.8% | ‚úÖ PASS | 5 NULL values in Sales.TotalAmount |
| **Accuracy** | 98.1% | ‚úÖ PASS | 12 price mismatches, 437 orphaned records |
| **Consistency** | 99.5% | ‚úÖ PASS | Minor date inconsistencies |
| **Integrity** | 99.8% | ‚úÖ PASS | 0.2% referential integrity issues |
| **Timeliness** | 100% | ‚úÖ PASS | All data current |
| **Uniqueness** | 99.3% | ‚úÖ PASS | 18 duplicate emails (under review) |

**REMEDIATION ROADMAP**

**Week 1 (Dec 8-14):**
1. ‚úÖ Populate NULL TotalAmount values (CRITICAL)
2. ‚úÖ Delete/correct test transaction ($0.01)
3. ‚úÖ Correct future-dated ServiceTickets

**Week 2 (Dec 15-21):**
4. ‚úÖ Resolve orphaned Sales records
5. ‚úÖ Implement promotional pricing tracking
6. ‚úÖ Correct SaleDetails price mismatches

**Week 3 (Dec 22-28):**
7. ‚úÖ Review duplicate customer emails
8. ‚úÖ Add database constraint for UnitPrice validation
9. ‚úÖ Update data quality monitoring rules

**COMPLIANCE STATUS**

‚úÖ **SOC 2 Requirements:** COMPLIANT (‚â•95% quality score)
‚úÖ **ISO 8000 Standards:** COMPLIANT (data accuracy >98%)
‚úÖ **Internal SLA:** COMPLIANT (monthly audit completed on time)

**RECOMMENDATIONS**

1. **Immediate:** Address 3 critical issues (within 2 weeks)
2. **Short-term:** Implement automated validation for promotional pricing
3. **Long-term:** Add database constraints to prevent future quality issues

**NEXT AUDIT:** January 7, 2025

---

**Attachments:**
- Detailed validation results (47 rules)
- Outlier analysis (full dataset)
- Remediation tracking spreadsheet

---

Done.

**WHAT JUST HAPPENED:**

Traditional data quality audit:
1. Manually write SQL for each validation ‚Üí 45 min
2. Run queries one by one ‚Üí 30 min
3. Export results to Excel ‚Üí 15 min
4. Check constraints manually ‚Üí 60 min
5. Profile columns in spreadsheet ‚Üí 45 min
6. Identify anomalies visually ‚Üí 30 min
7. Research business rules ‚Üí 30 min
8. Write audit report ‚Üí 90 min
**Total: 6+ hours**

Uderia @QUALITY workflow:
1. Request comprehensive assessment ‚Üí 45 sec
2. Review constraint violations ‚Üí 30 sec
3. Profile critical columns ‚Üí 35 sec
4. Validate business rules ‚Üí 40 sec
5. Quick DDL lookup with @DBA ‚Üí 25 sec
6. Generate compliance report ‚Üí 50 sec
**Total: 4 minutes 30 seconds**

**THE POWER:**
- **Automated validation** (47 rules executed simultaneously)
- **AI-driven profiling** (qlty_profileColumn with statistical analysis)
- **Anomaly detection** (outliers identified automatically)
- **Business rule enforcement** (price consistency checking)
- **Context-aware recommendations** (champion cases from 29 quality patterns)
- **Compliance-ready reporting** (SOC 2, ISO 8000 formatted)

97.3/100 quality score. 6 issues identified. Remediation plan delivered.

Priya went from "monthly audit" to "compliance-ready report" in 4.5 minutes.

One analyst. One profile. Zero manual Excel work.

This is @QUALITY. This is Uderia. This is data governance evolved.
```

**Screen Capture Plan:**
- [ ] Profile indicator: @QUALITY active
- [ ] Comprehensive quality assessment query
- [ ] `qlty_profileColumn` results with distributions
- [ ] Anomaly detection highlighting 3 outliers
- [ ] Business rule violation flagged
- [ ] Brief switch to @DBA for DDL context
- [ ] Quality score calculation: 97.3/100
- [ ] TDA_FinalReport generates audit summary

**Supporting Materials:**
- Quality audit checklist
- Expected anomalies documentation
- Compliance report template

---

### 10.4 Compliance Officer Journey - @LOCAL Profile
**Duration:** 5-6 minutes  
**Persona:** James, Compliance Officer analyzing customer churn (PII-heavy)  
**Profile Used:** Exclusively @LOCAL (zero cloud exposure)

**Learning Objectives:**
- Demonstrate complete privacy-preserving workflow
- Show identical capabilities with local execution
- Emphasize data sovereignty
- Prove zero compromise on intelligence

**Scenario Narrative:**
James needs to analyze why premium customers are churning, but the analysis involves sensitive PII (names, emails, addresses, purchase history). Regulatory requirements prohibit sending this data to cloud providers.

**Key Topics to Cover:**
- [ ] Profile switch: Activate @LOCAL (Ollama)
- [ ] Verify local execution: "Confirm zero cloud connectivity"
- [ ] Customer analysis: "Identify customers who stopped purchasing in Q4"
- [ ] PII-heavy queries: Full access to names, emails, addresses
- [ ] Purchase pattern analysis: "What products did churned customers previously buy?"
- [ ] Service ticket correlation: "Did they have unresolved support issues?"
- [ ] Demographic insights: Age, location, purchase frequency
- [ ] Generate confidential churn report
- [ ] Emphasize: All data stayed on-premises, zero cloud API calls

**Narration Script:**
```
Meet James. Compliance Officer. Operates under strict regulatory constraints.

Assignment: Analyze why premium customers are churning.

The problem: Analysis requires sensitive PII‚Äîfull names, email addresses, home addresses, purchase history.

Regulatory requirement: **ZERO data can leave organizational boundaries.**

GDPR. CCPA. Industry regulations. Data sovereignty non-negotiable.

Traditional cloud approach: BLOCKED. Cannot send PII to Anthropic, OpenAI, or Google.

Alternative: Build custom on-prem solution ‚Üí 6 months development, $300K budget.

Uderia approach: **@LOCAL profile. Same intelligence. Zero cloud.**

**THE SOVEREIGNTY WORKFLOW:**

James logs into Uderia. Profile selector shows 6 options.

He selects **@LOCAL** (Ollama Llama 3.1 70B - On-Premises).

Chat opens. Profile indicator: "@LOCAL Active - 100% On-Premises Execution"

**System banner appears:**

```
üîí SOVEREIGNTY MODE ACTIVE
‚úì LLM: Ollama (local server at 192.168.1.50)
‚úì Database: Direct connection (no cloud proxy)
‚úì RAG: ChromaDB (local vector store)
‚úì MCP Tools: On-premises execution only
‚õî Zero outbound traffic to cloud APIs
```

**NETWORK MONITORING (bottom corner):**

```
Cloud API Calls This Session: 0
Local Inference Requests: Active
Data Leaving Network: 0 bytes
```

James is confident. Let's verify.

**QUERY 1: VERIFY SOVEREIGNTY**

James types: *"Confirm that zero data is being sent to cloud providers during this session"*

@LOCAL responds (locally):

```
üîß System Check: Data Sovereignty
   
‚úÖ LLM Provider: Ollama Llama 3.1 70B
   - Location: 192.168.1.50:11434 (internal network)
   - API calls: Local only
   - Data transmission: 0 bytes external

‚úÖ Database Connection: Teradata fitness_db
   - Location: 10.0.5.100:1025 (internal network)
   - Direct JDBC connection
   - No cloud proxy

‚úÖ Vector Store: ChromaDB
   - Location: localhost:8000
   - Knowledge collections: Stored on-premises
   - Embeddings: Generated locally (Sentence Transformers)

‚úÖ MCP Tools: All local execution
   - base_readQuery: Direct database access
   - TDA_Charting: Local rendering
   - TDA_FinalReport: Local generation

‚õî **Guaranteed:** No PII, query text, or results transmitted externally
```

**Response:**

**Data Sovereignty: CONFIRMED** ‚úÖ

All processing happening within organizational boundaries:
- LLM reasoning: Local server
- Data access: Internal database
- Knowledge retrieval: On-premises vector store
- Tool execution: Local MCP servers

**Compliance Status:** GDPR ‚úì | CCPA ‚úì | SOC 2 ‚úì | Industry Regulations ‚úì

---

Sovereignty verified. Network monitor shows: 0 cloud API calls.

Now: the actual analysis with real PII.

**QUERY 2: IDENTIFY CHURNED CUSTOMERS**

James: *"Identify premium customers who stopped purchasing in Q4 2024. Include full names, email addresses, and home addresses."*

```
üîß Tool: base_readQuery
   Query: SELECT with PII fields (FirstName, LastName, Email, Address, City, State)
   Filter: Lifetime value >$5000, No Q4 2024 purchases

üîí Sovereignty: All PII processed locally, zero external transmission
```

Response:

**Churned Premium Customers (Q4 2024)**

**Total Identified:** 47 customers
**Combined Lifetime Value:** $287,450
**Average LTV:** $6,116

**Sample Records (PII included):**

| CustomerID | Full Name | Email | Address | City | State | LTV |
|------------|-----------|-------|---------|------|-------|-----|
| 10234 | **Jennifer Martinez** | jennifer.martinez@email.com | 1425 Oak Street | Seattle | WA | $8,450 |
| 10567 | **Michael Chen** | m.chen@techcorp.com | 892 Pine Ave | Portland | OR | $7,230 |
| 10892 | **Sarah Johnson** | s.johnson@gmail.com | 2341 Maple Dr | San Francisco | CA | $6,890 |
| 11023 | **David Rodriguez** | david.r@outlook.com | 567 Elm Street | Seattle | WA | $6,450 |
| 11234 | **Emily Thompson** | e.thompson@company.com | 3421 Cedar Ln | Portland | OR | $5,980 |

**Geographic Concentration:**
- Seattle: 23 customers (49%)
- Portland: 12 customers (26%)
- San Francisco: 8 customers (17%)
- Other: 4 customers (8%)

**Network Monitor Update:** Still 0 cloud API calls. All PII stayed local.

---

PII accessed. Zero data leakage. Continue analysis.

**QUERY 3: PURCHASE PATTERN ANALYSIS**

James: *"What products did these churned customers previously purchase? Show purchase history for the last 12 months."*

```
üîß Tool: base_readQuery
   JOIN: Customers ‚Üí Sales ‚Üí Products
   Period: December 2023 - November 2024
   
üìä Analysis: Product preferences, purchase frequency, spending patterns
```

Response:

**Purchase History: Churned Premium Customers**

**Product Category Breakdown (12 months):**

| Category | Purchases | Revenue | Avg Order |
|----------|-----------|---------|-----------|
| **Cardio Equipment** | 89 | $124,560 | $1,400 |
| **Strength Equipment** | 34 | $78,230 | $2,301 |
| **Accessories** | 156 | $18,450 | $118 |

**Top Products Purchased:**
1. **ProRunner 3000 Treadmill** - 34 units ($42,160)
2. **Elite Elliptical Pro** - 28 units ($38,920)
3. **CardioMaster 5000** - 27 units ($43,480)

**Key Insight:** 68% of revenue from Cardio equipment. These are Cardio-focused customers.

**Purchase Frequency:**
- Q1 2024: 67 transactions
- Q2 2024: 72 transactions
- Q3 2024: 58 transactions
- **Q4 2024: 0 transactions** ‚ùå (complete stop)

**Pattern:** Progressive decline leading to complete churn in Q4.

---

Clear pattern. Cardio customers stopped buying. Why?

**QUERY 4: SERVICE TICKET CORRELATION**

James: *"Did these 47 customers have unresolved service tickets? Show ticket details including issue descriptions."*

```
üîß Tool: base_readQuery
   JOIN: Customers ‚Üí ServiceTickets
   Filter: CustomerID IN (churned list)
   Status: Open or Recently Closed
```

Response:

**Service Ticket Analysis: Churned Customers**

**Tickets Filed:** 63 total (from 41 of 47 customers = 87%)

**Ticket Status:**
- Open: 12 tickets (19%)
- Closed (Resolved): 34 tickets (54%)
- Closed (Unresolved): 17 tickets (27%) ‚ö†Ô∏è

**Issue Categories:**

| Issue Type | Count | Resolution Rate |
|------------|-------|-----------------|
| Treadmill Belt Malfunction | 23 | 52% |
| Display Screen Failure | 18 | 61% |
| Motor Overheating | 15 | 47% |
| Noise/Vibration | 7 | 71% |

**Critical Finding:** 17 tickets closed as "unresolved" (customer satisfaction: FAILED)

**Sample Unresolved Tickets:**

**CustomerID 10234 (Jennifer Martinez):**
- Ticket #245: "ProRunner 3000 belt slipping after 3 months"
- Status: Closed Unresolved
- Last Update: Oct 15, 2024
- Customer Comment: "Not fixed. Belt still slipping. Very disappointed."

**CustomerID 10567 (Michael Chen):**
- Ticket #267: "Display screen went black, won't turn on"
- Status: Closed Unresolved
- Last Update: Nov 2, 2024
- Customer Comment: "Replacement part never arrived. Unusable equipment."

**Root Cause:** Poor service resolution ‚Üí Customer dissatisfaction ‚Üí Churn

---

Service failures driving churn. What about demographics?

**QUERY 5: DEMOGRAPHIC INSIGHTS**

James: *"Analyze demographic patterns of these churned customers. Age, location, income indicators, purchase frequency."*

```
üîß Tool: base_readQuery
   Analysis: Age demographics, geographic distribution, spending patterns
   
üìä Tool: TDA_LLMTask (local inference)
   Task: Demographic segmentation and insights
```

Response:

**Demographic Profile: Churned Premium Customers**

**Age Distribution:**
- 25-34: 8 customers (17%)
- **35-44: 23 customers (49%)** ‚Üê Majority
- 45-54: 12 customers (26%)
- 55+: 4 customers (8%)

**Average Age:** 41 years (prime earning years)

**Geographic Concentration:**
- **Urban areas:** 43 customers (91%)
- Suburban: 4 customers (9%)
- Rural: 0 customers

**Income Indicators (based on ZIP codes):**
- High income (>$100K): 34 customers (72%)
- Upper-middle ($75K-$100K): 11 customers (23%)
- Middle (<$75K): 2 customers (5%)

**Purchase Behavior (Pre-Churn):**
- **Average Annual Spend:** $6,116
- **Purchase Frequency:** Every 2.3 months
- **Product Preference:** 68% Cardio, 27% Strength, 5% Accessories
- **Loyalty:** 87% had active service issues (high engagement)

**Customer Profile:**
High-value, urban professionals (age 35-44) with disposable income who invested heavily in home Cardio equipment. They engaged with customer service (filed tickets) but received poor resolution. Result: Complete churn.

**Network Monitor:** Still 0 cloud API calls. Full demographic analysis completed locally.

---

James has the complete picture. Time for the confidential report.

**QUERY 6: GENERATE CONFIDENTIAL CHURN REPORT**

James: *"Generate a confidential customer churn report for executive review. Include PII for retention campaign targeting."*

Strategic Planning (local): Retrieves champion case from "Executive_Reporting_Patterns"

Knowledge (local): Accesses "Business Metrics Definitions" collection

```
üîß Tool: TDA_FinalReport (local generation)
   Format: Confidential Executive Report
   Security: Internal distribution only
   PII Included: Yes (for retention targeting)
```

Response:

---

**CONFIDENTIAL CUSTOMER CHURN ANALYSIS**
*Q4 2024 Premium Customer Retention Crisis*
*Prepared by: James Anderson, Compliance Officer*
*Date: December 7, 2024*
*Classification: INTERNAL USE ONLY - Contains PII*

**EXECUTIVE SUMMARY**

**Churn Event:** 47 premium customers (combined LTV: $287,450) completely stopped purchasing in Q4 2024.

**Root Cause:** Poor customer service resolution on Cardio equipment issues.

**Financial Impact:**
- **Immediate Loss:** $287K lifetime value at risk
- **Annual Revenue Impact:** $143K (47 customers √ó avg $3,050 annual spend)
- **Retention Campaign ROI:** $115K recovery potential ($40K investment, 80% success rate)

**CUSTOMER PROFILE**

**Demographics:**
- **Age:** 35-44 (49% majority), high-earning professionals
- **Location:** Urban (Seattle 49%, Portland 26%, SF 17%)
- **Income:** $100K+ annual (72% of group)
- **Product Focus:** Cardio equipment (68% of purchases)

**Behavioral Indicators:**
- High engagement: 87% filed service tickets
- Loyalty demonstrated: Average 2.3 month purchase frequency (pre-churn)
- Service dissatisfaction: 17 unresolved tickets (27% failure rate)

**RETENTION CAMPAIGN TARGET LIST**

**Tier 1: High-Priority (LTV >$7,000) - 12 customers**

1. **Jennifer Martinez** | jennifer.martinez@email.com
   - LTV: $8,450 | Address: 1425 Oak Street, Seattle, WA
   - Issue: ProRunner 3000 belt malfunction (unresolved)
   - Offer: Free replacement + $200 credit

2. **Michael Chen** | m.chen@techcorp.com
   - LTV: $7,230 | Address: 892 Pine Ave, Portland, OR
   - Issue: Display screen failure (part never arrived)
   - Offer: Immediate replacement + 2-year extended warranty

3. **Sarah Johnson** | s.johnson@gmail.com
   - LTV: $6,890 | Address: 2341 Maple Dr, San Francisco, CA
   - Issue: Motor overheating (closed unresolved)
   - Offer: Equipment upgrade + free maintenance plan

*(Continues for all 12 Tier 1 customers...)*

**Tier 2: Medium-Priority (LTV $6,000-$7,000) - 18 customers**
*(Full list with contact details, addresses, specific issues, personalized offers...)*

**Tier 3: Standard Priority (LTV $5,000-$6,000) - 17 customers**
*(Full list with contact details, addresses, specific issues, personalized offers...)*

**RETENTION CAMPAIGN STRATEGY**

**Phase 1: Immediate Outreach (Week 1)**
- Personal phone calls from VP of Customer Success
- Acknowledge service failures, sincere apology
- Offer resolution: Equipment replacement or upgrade

**Phase 2: Incentive Offers (Week 2)**
- Tier 1: $200 credit + extended warranty
- Tier 2: $150 credit + maintenance plan
- Tier 3: $100 credit + priority support

**Phase 3: Follow-Up (Week 3-4)**
- Track response rates
- Adjust offers based on feedback
- Document lessons learned

**FINANCIAL PROJECTIONS**

**Campaign Investment:**
- Equipment replacements/upgrades: $28,000
- Account credits: $6,400
- Staff time (outreach): $5,600
**Total: $40,000**

**Expected Recovery (80% success rate):**
- Customers retained: 38 of 47
- Annual revenue preserved: $115,000
- 3-year LTV recovered: $230,000

**ROI: 5.75x** ($230K recovered / $40K invested)

**COMPLIANCE & PRIVACY**

‚úÖ **Data Handling:** All PII processed on-premises (Uderia @LOCAL profile)
‚úÖ **Regulatory Compliance:** GDPR, CCPA, SOC 2 compliant
‚úÖ **Access Control:** Report distribution limited to C-suite and retention team
‚úÖ **Audit Trail:** All data access logged for compliance review

**Zero cloud API calls during analysis** - Complete data sovereignty maintained.

**RECOMMENDATIONS**

1. **Immediate:** Launch retention campaign targeting all 47 customers
2. **Short-term:** Improve service ticket resolution process (target 90% resolution rate)
3. **Long-term:** Implement proactive quality monitoring for Cardio equipment line

**NEXT STEPS**

- Approve $40K retention budget (Dec 8)
- Begin Tier 1 outreach (Dec 9)
- Weekly progress reviews (Fridays)

---

**Confidential Annexes:**
- Full customer contact list with PII (47 records)
- Service ticket transcripts (63 tickets)
- Retention offer scripts (3 tiers)

---

Done.

James saves report. Marks: **CONFIDENTIAL - INTERNAL ONLY**.

Network monitor final reading: **0 cloud API calls**. **0 bytes transmitted externally**.

All PII‚Äînames, emails, addresses, service issues‚Äîprocessed entirely on-premises.

**WHAT JUST HAPPENED:**

Traditional cloud-based approach:
- **BLOCKED:** Regulatory prohibition on sending PII to cloud providers
- **Alternative 1:** Manual analysis (2 weeks, incomplete insights)
- **Alternative 2:** Custom on-prem solution ($300K, 6 months development)

Uderia @LOCAL approach:
- **Same intelligence:** Claude-level reasoning (Llama 3.1 70B)
- **Same tools:** All MCP tools, strategic planning, RAG knowledge
- **Same transparency:** Execution logs, tool traces
- **Same output:** Executive-ready report with actionable insights
- **Zero cloud:** 100% on-premises execution
- **Time:** 18 minutes (vs 2 weeks or 6 months)

**THE SOVEREIGNTY PROOF:**

Network monitoring throughout session:
- Cloud API calls: **0**
- Data transmitted externally: **0 bytes**
- LLM inference: **Local only** (192.168.1.50)
- Database access: **Direct JDBC** (internal network)
- Vector store: **On-premises ChromaDB**

**THE POWER:**
- **No cloud compromise** (identical capabilities locally)
- **Full PII access** (names, emails, addresses queried freely)
- **Strategic planning** (champion cases retrieved from local RAG)
- **Business intelligence** (complete churn analysis with demographics)
- **Executive reporting** (confidential report with retention targeting)
- **Regulatory compliance** (GDPR, CCPA, SOC 2 guaranteed)

47 customers analyzed. $287K lifetime value at risk. $40K retention campaign designed. 5.75x ROI projected.

James went from "regulatory blocked" to "confidential report with PII-based targeting" in 18 minutes.

One compliance officer. One profile. Zero cloud exposure.

This is @LOCAL. This is Uderia. This is data sovereignty without compromise.

**The promise kept:** Cloud-level intelligence. Local execution. Zero trade-offs.
```

**Screen Capture Plan:**
- [ ] Profile switch: @ANALYST ‚Üí @LOCAL
- [ ] Profile indicator showing "Ollama Llama 3.1 70B (Local)"
- [ ] Network monitor: Zero outbound traffic to cloud APIs
- [ ] Customer PII queries with full names, emails visible
- [ ] Same MCP tools, same strategic planning, same transparency
- [ ] Slightly slower inference (local model) but complete functionality
- [ ] Confidential report generation
- [ ] Sovereignty guarantee banner

**Supporting Materials:**
- Customer PII dataset (anonymized for demo)
- Compliance documentation
- Network traffic monitoring proof

---

### 10.5 "God Mode" Power User Journey - Multi-Profile Orchestration
**Duration:** 6-7 minutes  
**Persona:** Elena, Head of Analytics orchestrating complete business intelligence workflow  
**Profiles Used:** @QUALITY ‚Üí @DBA ‚Üí @ANALYST ‚Üí @LOCAL ‚Üí @PROD (all profiles in sequence)

**Learning Objectives:**
- Demonstrate the ultimate "god mode" workflow
- Show seamless profile switching
- Illustrate team orchestration by one person
- Prove compounding value of specialized personas

**Scenario Narrative:**
Elena needs to deliver a comprehensive competitive analysis involving data validation, query optimization, business insights, sensitive customer data, and production deployment‚Äîall in one afternoon.

**Workflow Orchestration:**

**Phase 1: Data Validation (@QUALITY)**
- "Validate data quality for Q4 sales and customer data"
- Identifies 2 anomalies, flags for correction
- Confirms 99.2% data completeness

**Phase 2: Query Optimization (@DBA)**
- "Optimize the query for customer lifetime value calculation"
- Reviews schema, identifies missing index
- Generates optimized SQL (45s ‚Üí 2.1s execution time)

**Phase 3: Business Intelligence (@ANALYST)**
- "Analyze customer lifetime value by segment with visualizations"
- Generates executive dashboard with 4 charts
- Provides actionable recommendations

**Phase 4: Sensitive Data Analysis (@LOCAL)**
- "Identify high-value churned customers by name for retention campaign"
- Handles PII locally (names, emails, purchase history)
- Generates confidential retention target list

**Phase 5: Production Deployment (@PROD)**
- "Operationalize this entire workflow via REST API"
- Creates Airflow DAG with all optimized queries
- Schedules weekly execution

**Key Topics to Cover:**
- [ ] Seamless profile switching (5 switches in one session)
- [ ] Context retention across profile changes
- [ ] Each profile contributing specialized expertise
- [ ] Zero friction, zero handoffs
- [ ] Traditional approach: 5 people, 3 days, multiple meetings
- [ ] Uderia approach: 1 person, 2 hours, god mode activated

**Narration Script:**
```
Meet Elena. Head of Analytics. Executive mandate delivered this morning.

"Deliver comprehensive customer lifetime value analysis by end of day. Include data validation, optimized queries, executive insights, PII-based retention targeting, and production deployment."

Traditional approach: Assemble the team.
- Quality analyst validates data ‚Üí 4 hours
- DBA optimizes queries ‚Üí 3 hours  
- Business analyst generates insights ‚Üí 4 hours
- Compliance reviews PII analysis ‚Üí 2 hours
- Data engineer deploys to production ‚Üí 3 hours
- Coordination meetings ‚Üí 2 hours
**Total: 5 people, 3 days, 18 hours of work**

Uderia approach: Elena. One person. God mode. 2 hours.

**THE ORCHESTRATION:**

Elena logs into Uderia. Profile dropdown shows all 6 options.

This workflow needs 5 of them.

Time: 2:00 PM. Deadline: 5:00 PM. Let's go.

---

## PHASE 1: DATA VALIDATION (@QUALITY) - 2:00 PM

Profile selector ‚Üí **@QUALITY** (Claude Sonnet + validation tools)

Profile indicator: "@QUALITY Active"

Elena types: *"Validate data quality for Q4 2024 sales and customer data. Flag any issues that could affect lifetime value calculations."*

```
üîß Tool: qlty_validateData
   Tables: Sales, Customers, SaleDetails
   Period: Q4 2024
   
üìä Automated checks: Referential integrity, NULL values, outliers
```

Response (45 seconds):

**Data Quality Validation: Q4 2024**

**Overall Score: 99.2/100** ‚úÖ

**Issues Detected (2):**

**1. NULL TotalAmount (CRITICAL)** üî¥
- Table: Sales
- Count: 5 rows (SaleIDs: 234789, 234790, 234812, 235001, 235002)
- Impact: Missing revenue data for LTV calculations
- **Action Required:** Populate from SaleDetails before analysis

**2. Price Inconsistencies (WARNING)** üü°
- Table: SaleDetails
- Count: 12 rows with UnitPrice ‚â† Products.Price
- Impact: $876 revenue discrepancy
- **Action Advised:** Verify promotional pricing or correct records

**Data Completeness:** 99.8% (acceptable for analysis)

**Recommendation:** Proceed with analysis, but flag 5 NULL records for exclusion.

---

Elena notes the issues. Data quality: confirmed. Time: 45 seconds.

Quick switch to optimize performance.

---

## PHASE 2: QUERY OPTIMIZATION (@DBA) - 2:01 PM

Profile dropdown ‚Üí **@DBA** (Claude Sonnet + database tools)

Capabilities reload. Watch: Profile icon changes, tool list updates.

Profile indicator: "@DBA Active"

Elena pastes her query:

```sql
SELECT 
    c.CustomerID,
    c.FirstName || ' ' || c.LastName as CustomerName,
    COUNT(s.SaleID) as PurchaseCount,
    SUM(s.TotalAmount) as LifetimeValue,
    AVG(s.TotalAmount) as AvgOrderValue,
    MAX(s.SaleDate) as LastPurchase
FROM Customers c
LEFT JOIN Sales s ON c.CustomerID = s.CustomerID
WHERE s.TotalAmount IS NOT NULL  -- Exclude nulls flagged by @QUALITY
GROUP BY c.CustomerID, c.FirstName, c.LastName
ORDER BY LifetimeValue DESC;
```

Elena: *"Analyze this query for performance. Optimize if needed."*

```
üîß Tool: base_readQuery (EXPLAIN mode)
   Analyzing execution plan...
   
‚ö†Ô∏è Bottleneck: Full table scan on Sales (237K rows)
   Missing index on CustomerID (FK not indexed)
```

Response (50 seconds):

**Performance Analysis**

**Current Execution:** 12.3 seconds
**Rows Scanned:** 237,450 (Sales) + 8,945 (Customers)

**Optimization:**

```sql
-- Create index (run ONCE)
CREATE INDEX idx_sales_customerid ON Sales (CustomerID);
COLLECT STATISTICS ON Sales COLUMN (CustomerID);

-- Optimized query (same logic, uses index)
SELECT 
    c.CustomerID,
    c.FirstName || ' ' || c.LastName as CustomerName,
    COUNT(s.SaleID) as PurchaseCount,
    SUM(s.TotalAmount) as LifetimeValue,
    AVG(s.TotalAmount) as AvgOrderValue,
    MAX(s.SaleDate) as LastPurchase
FROM Customers c
LEFT JOIN Sales s ON c.CustomerID = s.CustomerID
WHERE s.TotalAmount IS NOT NULL
GROUP BY c.CustomerID, c.FirstName, c.LastName
ORDER BY LifetimeValue DESC;
```

**Optimized Execution:** 1.8 seconds (85% faster)

**Index created.** Query ready for production.

---

Elena saves optimized query. Performance: locked in. Time: 50 seconds.

Now: business insights.

---

## PHASE 3: BUSINESS INTELLIGENCE (@ANALYST) - 2:02 PM

Profile dropdown ‚Üí **@ANALYST** (Gemini Flash + business tools)

Capabilities reload. Visualization tools appear.

Profile indicator: "@ANALYST Active"

Elena: *"Using the optimized query, analyze customer lifetime value by segment. Create executive dashboard with visualizations."*

Strategic Planning retrieves: "Customer_Segmentation_Analysis" (17 champion cases)

Knowledge: "Business Metrics Definitions" collection

```
üîß Tool: base_readQuery (optimized query)
   Execution: 1.8 seconds
   
üìä Tool: TDA_Charting (4 visualizations)
   - Segment distribution (pie chart)
   - LTV by segment (bar chart)
   - Purchase frequency (line graph)
   - Top 20 customers (table)
```

Response (65 seconds):

**Customer Lifetime Value Analysis**

**Total Customers:** 8,945
**Total LTV:** $18.4M
**Average LTV:** $2,057

**Segmentation:**

[Pie Chart: Customer Segments]

**1. Premium (LTV >$5,000)** - 847 customers (9%)
- Total LTV: $7.2M (39% of revenue)
- Avg LTV: $8,502
- Avg Purchases: 6.8 per customer

**2. Standard (LTV $2,000-$5,000)** - 2,134 customers (24%)
- Total LTV: $6.8M (37% of revenue)
- Avg LTV: $3,186
- Avg Purchases: 3.2 per customer

**3. Emerging (LTV $500-$2,000)** - 3,456 customers (39%)
- Total LTV: $3.6M (20% of revenue)
- Avg LTV: $1,041
- Avg Purchases: 1.8 per customer

**4. Entry (LTV <$500)** - 2,508 customers (28%)
- Total LTV: $0.8M (4% of revenue)
- Avg LTV: $319
- Avg Purchases: 1.1 per customer

[Bar Chart: LTV by Segment]

**Key Insights:**

üéØ **Premium segment:** 9% of customers generate 39% of revenue (high concentration)
‚ö†Ô∏è **Churn risk:** 47 premium customers churned in Q4 (identified earlier)
üí° **Opportunity:** Emerging segment (3,456 customers) has upsell potential to Standard

**Recommendations:**
1. **Retention:** Launch premium customer retention program (focus on 47 churned)
2. **Growth:** Target Emerging segment with personalized upsell campaigns
3. **Prevention:** Monitor Premium segment service tickets proactively

---

Elena has executive insights. Visualizations ready. Time: 65 seconds.

But she needs specific names for retention targeting. That's sensitive PII.

Switch to sovereign execution.

---

## PHASE 4: SENSITIVE DATA ANALYSIS (@LOCAL) - 2:04 PM

Profile dropdown ‚Üí **@LOCAL** (Ollama Llama 3.1 70B - On-Premises)

Capabilities reload. Network monitor activates.

Profile indicator: "@LOCAL Active - 100% On-Premises"

**System Banner:**
```
üîí SOVEREIGNTY MODE ACTIVE
‚õî Zero cloud API calls | All data stays local
```

Elena: *"From the 847 premium customers, identify the 47 who churned in Q4. Include full names, emails, and addresses for retention campaign."*

Context retained from earlier phases. @LOCAL knows about the customer segments.

```
üîß Tool: base_readQuery (local execution)
   Query: Premium customers with NO Q4 2024 purchases
   PII fields: FirstName, LastName, Email, Address, City, State
   
üîí Sovereignty: All PII processed on-premises, zero cloud transmission
```

Response (70 seconds - slightly slower, local inference):

**Churned Premium Customers - Retention Target List**

**Total Identified:** 47 customers
**Combined LTV at Risk:** $287,450

**Top 10 High-Priority Targets:**

| CustomerID | Name | Email | Address | LTV | Last Purchase |
|------------|------|-------|---------|-----|---------------|
| 10234 | Jennifer Martinez | jennifer.martinez@email.com | 1425 Oak St, Seattle, WA | $8,450 | Sept 28, 2024 |
| 10567 | Michael Chen | m.chen@techcorp.com | 892 Pine Ave, Portland, OR | $7,230 | Sept 15, 2024 |
| 10892 | Sarah Johnson | s.johnson@gmail.com | 2341 Maple Dr, San Francisco, CA | $6,890 | Oct 3, 2024 |
| 11023 | David Rodriguez | david.r@outlook.com | 567 Elm St, Seattle, WA | $6,450 | Sept 22, 2024 |
| 11234 | Emily Thompson | e.thompson@company.com | 3421 Cedar Ln, Portland, OR | $5,980 | Oct 12, 2024 |
| ... | (42 more) | ... | ... | ... | ... |

**Network Monitor:** 0 cloud API calls. All PII stayed on-premises.

**Export:** Confidential retention list saved locally.

---

Elena has names and contact details. Compliance-safe. Time: 70 seconds.

Final phase: operationalize everything for production.

---

## PHASE 5: PRODUCTION DEPLOYMENT (@PROD) - 2:06 PM

Profile dropdown ‚Üí **@PROD** (Claude Sonnet + automation tools)

Capabilities reload. API documentation appears.

Profile indicator: "@PROD Active"

Elena: *"Create production-ready Airflow DAG that executes the complete workflow: data validation, optimized LTV calculation, segmentation analysis, and automated reporting. Schedule weekly."*

Strategic Planning: "Production_Automation_Patterns" repository (34 champion cases)

```
üîß Tool: TDA_FinalReport
   Format: Python Airflow DAG
   Schedule: Weekly (Mondays 6 AM)
   Profiles: @QUALITY ‚Üí @DBA ‚Üí @ANALYST (automated sequence)
```

Response (80 seconds):

**Airflow DAG: Customer LTV Analytics Pipeline**

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import os

def validate_data_quality():
    """Phase 1: Data validation with @QUALITY"""
    response = requests.post(
        'https://uderia.company.com/api/v1/query',
        headers={'Authorization': f'Bearer {os.environ["UDERIA_TOKEN"]}'},
        json={
            'profile': '@QUALITY',
            'query': 'Validate Q4 sales and customer data quality',
            'session_id': 'ltv_pipeline'
        }
    )
    
    results = response.json()
    quality_score = results['quality_score']
    
    if quality_score < 95:
        raise Exception(f"Data quality below threshold: {quality_score}%")
    
    print(f"‚úÖ Data quality validated: {quality_score}%")
    return results

def optimize_query():
    """Phase 2: Query optimization with @DBA"""
    optimized_sql = """
        SELECT c.CustomerID, c.FirstName || ' ' || c.LastName as CustomerName,
               COUNT(s.SaleID) as PurchaseCount,
               SUM(s.TotalAmount) as LifetimeValue,
               AVG(s.TotalAmount) as AvgOrderValue,
               MAX(s.SaleDate) as LastPurchase
        FROM Customers c
        LEFT JOIN Sales s ON c.CustomerID = s.CustomerID
        WHERE s.TotalAmount IS NOT NULL
        GROUP BY c.CustomerID, c.FirstName, c.LastName
        ORDER BY LifetimeValue DESC;
    """
    
    response = requests.post(
        'https://uderia.company.com/api/v1/query',
        headers={'Authorization': f'Bearer {os.environ["UDERIA_TOKEN"]}'},
        json={
            'profile': '@DBA',
            'sql': optimized_sql,
            'session_id': 'ltv_pipeline'
        }
    )
    
    results = response.json()
    print(f"‚úÖ Query executed: {results['execution_time']}")
    return results

def generate_insights():
    """Phase 3: Business intelligence with @ANALYST"""
    response = requests.post(
        'https://uderia.company.com/api/v1/query',
        headers={'Authorization': f'Bearer {os.environ["UDERIA_TOKEN"]}'},
        json={
            'profile': '@ANALYST',
            'query': 'Analyze customer LTV by segment with visualizations',
            'session_id': 'ltv_pipeline'
        }
    )
    
    results = response.json()
    
    # Save report to S3
    report_url = save_to_s3(results['report'])
    
    # Send Slack notification
    send_slack_notification(f"üìä Weekly LTV Report: {report_url}")
    
    print(f"‚úÖ Report generated and distributed")
    return results

default_args = {
    'owner': 'elena',
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'customer_ltv_analytics',
    default_args=default_args,
    description='Automated customer lifetime value analysis pipeline',
    schedule_interval='0 6 * * 1',  # Weekly: Mondays at 6 AM
    start_date=datetime(2024, 12, 7),
    catchup=False
)

# Task dependencies
task1 = PythonOperator(
    task_id='validate_data_quality',
    python_callable=validate_data_quality,
    dag=dag
)

task2 = PythonOperator(
    task_id='optimize_query',
    python_callable=optimize_query,
    dag=dag
)

task3 = PythonOperator(
    task_id='generate_insights',
    python_callable=generate_insights,
    dag=dag
)

# Pipeline: Quality ‚Üí Optimization ‚Üí Insights
task1 >> task2 >> task3
```

**Deployment Instructions:**
1. Save to `dags/customer_ltv_analytics.py`
2. Set environment variable: `UDERIA_TOKEN`
3. Deploy to Airflow
4. First run: Monday, Dec 9, 2024 at 6:00 AM

**Automated Workflow:**
- **Phase 1:** @QUALITY validates data (45s)
- **Phase 2:** @DBA executes optimized query (1.8s)
- **Phase 3:** @ANALYST generates report with visualizations (65s)
- **Total:** ~2 minutes execution weekly

**Output:** Executive report auto-distributed via Slack, saved to S3.

---

Done. Pipeline deployed.

Elena checks time: **2:08 PM**

**8 minutes total. 5 profiles orchestrated. Complete workflow deployed.**

Let's see what just happened.

---

## THE GOD MODE SUMMARY

Elena started at 2:00 PM. Finished at 2:08 PM.

**Profile orchestration:**

‚úÖ **@QUALITY** (45s) - Data validation, 99.2% quality score, 2 issues flagged
‚úÖ **@DBA** (50s) - Query optimization, 12.3s ‚Üí 1.8s (85% faster)
‚úÖ **@ANALYST** (65s) - Business intelligence, 4 visualizations, 3 recommendations
‚úÖ **@LOCAL** (70s) - PII analysis, 47 customers identified, zero cloud exposure
‚úÖ **@PROD** (80s) - Production deployment, Airflow DAG, weekly automation

**Total Time:** 8 minutes (including thinking time: ~12 minutes)

**Deliverables:**
1. ‚úÖ Data quality report (99.2% score)
2. ‚úÖ Optimized SQL query (85% performance gain)
3. ‚úÖ Executive dashboard (4 visualizations, segmentation analysis)
4. ‚úÖ Confidential retention list (47 customers with PII)
5. ‚úÖ Production pipeline (Airflow DAG, weekly execution)

**Traditional Approach:**

**Team Required:**
- Priya (Quality Analyst) - 4 hours
- Marcus (DBA) - 3 hours
- Sarah (Business Analyst) - 4 hours
- James (Compliance) - 2 hours
- Data Engineer (Production) - 3 hours

**Coordination:**
- Kickoff meeting - 30 min
- Mid-point sync - 45 min
- Final review - 45 min

**Total:** 5 people √ó 3 days = 18 hours of work + 2 hours meetings

**Bottlenecks:**
- Waiting for quality validation before optimization
- Waiting for optimization before analysis
- Waiting for compliance review before retention targeting
- Waiting for everything before deployment
- Back-and-forth iterations at each handoff

**Uderia God Mode:**

**Team Required:** Elena (1 person)

**Time:** 8 minutes execution + 4 minutes thinking = 12 minutes total

**Bottlenecks:** ZERO. Seamless profile switching with context retention.

**The Difference:**

| Dimension | Traditional | Uderia God Mode | Improvement |
|-----------|-------------|-----------------|-------------|
| **People** | 5 specialists | 1 person | 80% reduction |
| **Time** | 3 days (18 hours) | 12 minutes | **99.3%** faster |
| **Meetings** | 3 coordination sessions | 0 | 100% elimination |
| **Handoffs** | 4 handoffs (friction points) | 0 | Seamless flow |
| **Context Loss** | High (explain at each step) | 0 | Perfect retention |
| **Iterations** | Slow (days per cycle) | Instant | Real-time refinement |

**THE COMPOUNDING POWER:**

Each profile contributed specialized expertise:

üî¨ **@QUALITY:** Caught 2 data issues that would have corrupted analysis
‚öôÔ∏è **@DBA:** Delivered 85% performance gain (12.3s ‚Üí 1.8s)
üìä **@ANALYST:** Generated executive visualizations and actionable insights
üîí **@LOCAL:** Handled sensitive PII without regulatory risk
üöÄ **@PROD:** Operationalized entire workflow for ongoing automation

**Alone, each profile is powerful.**
**Together, orchestrated by one person, they're unstoppable.**

**THIS IS GOD MODE.**

One person commanding a team of specialized AI personas.

No coordination overhead.
No knowledge handoffs.
No waiting.
No friction.

Pure, compounded intelligence.

Elena went from "executive mandate" to "fully deployed production pipeline" in 12 minutes.

**Traditional team:** 5 people, 3 days, countless meetings.
**Elena with Uderia:** 1 person, 12 minutes, zero meetings.

This is the future.

Not replacing specialists.

**Empowering one person to orchestrate all of them.**

This is @QUALITY + @DBA + @ANALYST + @LOCAL + @PROD.

This is Uderia.

**This is God Mode.**
```

**Screen Capture Plan:**
- [ ] Profile dropdown showing all 5 profiles
- [ ] Phase 1: @QUALITY validation (3 min segment)
- [ ] Quick switch to @DBA (watch capabilities reload)
- [ ] Phase 2: @DBA optimization (4 min segment)
- [ ] Switch to @ANALYST (visualization tools appear)
- [ ] Phase 3: @ANALYST insights with charts (5 min segment)
- [ ] Switch to @LOCAL (network monitor shows zero cloud traffic)
- [ ] Phase 4: @LOCAL PII analysis (3 min segment)
- [ ] Switch to @PROD (API documentation visible)
- [ ] Phase 5: @PROD operationalization (4 min segment)
- [ ] Final montage: All 5 profiles contributed to one cohesive deliverable
- [ ] Side-by-side comparison: Traditional (5 people, 3 days) vs Uderia (1 person, 2 hours)

**Supporting Materials:**
- Complete workflow diagram showing all 5 phases
- Traditional vs Uderia comparison infographic
- "God mode" capability matrix
- Time/cost savings calculator

---

## üóÑÔ∏è Demo Environment Reference

### Database Schema: fitness_db

**Business Context:** Fitness equipment retail company with sales tracking and customer service management

**Tables Overview:**

1. **Customers** (Base table)
   - CustomerID, FirstName, LastName, Email, Phone
   - Address, City, State, ZipCode
   - RegistrationDate
   - **Use Cases:** Customer demographics, geographic analysis, customer lifetime value

2. **Products** (Inventory table)
   - ProductID, ProductName, ProductType, Brand
   - Price, StockQuantity
   - **Use Cases:** Product catalog queries, inventory tracking, pricing analysis

3. **Sales** (Transaction header)
   - SaleID, CustomerID, SaleDate, TotalAmount, SalesPersonID
   - **Use Cases:** Revenue trends, sales performance, temporal analysis

4. **SaleDetails** (Transaction line items)
   - SaleDetailID, SaleID, ProductID, Quantity, UnitPrice
   - **Use Cases:** Product-level revenue, quantity analysis, pricing validation

5. **ServiceTickets** (Customer support)
   - TicketID, CustomerID, ProductID, TicketDate
   - IssueDescription, Status, ResolutionDate
   - **Use Cases:** Product quality analysis, customer satisfaction, warranty tracking

**Key Relationships:**
- Sales ‚Üí Customers (one-to-many)
- SaleDetails ‚Üí Sales (one-to-many)
- SaleDetails ‚Üí Products (many-to-one)
- ServiceTickets ‚Üí Customers (many-to-one)
- ServiceTickets ‚Üí Products (many-to-one)

### MCP Tools Configuration

#### Core MCP Tools (Teradata Server)

**BASE Category - Database Operations:**
- `base_databaseList` - List all databases in the system
- `base_tableList` - List all tables in a database  
- `base_tableDDL` - Retrieve table DDL (CREATE TABLE statement)
- `base_readQuery` - Execute SELECT queries and return results
- `base_databaseBusinessDesc` (prompt) - Get business description of database
- `base_tableBusinessDesc` (prompt) - Get business description of table

**QUALITY Category - Data Validation:**
- `qlty_*` tools - Data profiling, validation, anomaly detection
- `qlty_databaseQuality` (prompt) - Comprehensive database quality assessment

**DBA Category - Administration:**
- `dba_databaseVersion` - Get Teradata system version
- `dba_databaseLineage` (prompt) - Data lineage analysis
- `dba_databaseHealthAssessment` (prompt) - System health check
- `dba_userActivityAnalysis` (prompt) - User activity tracking
- `dba_systemVoice` (prompt) - System status narrative
- `dba_tableArchive` (prompt) - Table archiving recommendations
- `dba_tableDropImpact` (prompt) - Impact analysis for dropping tables

#### Client-Side Tools (Built into Uderia)

**TDA Category - Platform Tools:**
- `TDA_FinalReport` - Format structured final report for UI
- `TDA_ComplexPromptReport` - Execute complex prompt workflows
- `TDA_LLMTask` - Delegate tasks to LLM for analysis
- `TDA_CurrentDate` - Get current date/time
- `TDA_SystemLog` - Log system messages to Live Status Panel
- `TDA_Charting` - Generate data visualizations (charts)

**MCP Tools by User Persona:**

**For Business Users (Modules 1-4, 6, 10.1, 10.2):**
- Primary: `base_readQuery`, `base_tableList`, `base_tableDDL`, `base_databaseList`
- Secondary: `TDA_FinalReport`, `TDA_LLMTask`, `TDA_Charting`
- Prompts: `base_databaseBusinessDesc`, `base_tableBusinessDesc`

**For Administrators (Modules 7, 8, 10.4):**
- Primary: `dba_databaseVersion`, all `dba_*` prompts
- Secondary: All consumption tracking endpoints (via REST API)
- Focus: Cost analytics, user management, system monitoring

**For DevOps/Advanced (Modules 9, 10.3):**
- All tools available: Full system access for integration demonstrations
- Focus: REST API automation, Airflow integration, profile switching

### Sample Queries by Persona

**Knowledge Worker Queries:**
- "What are the top 5 fitness products by revenue this month?"
- "Show me customer demographics for people who bought treadmills"
- "Which products have the highest return/service ticket rates?"
- "Compare sales performance across different states"
- "What's the average time to resolve service tickets by product type?"

**Content Developer Queries:**
- "Analyze the data quality of the Email field in Customers table"
- "Create a comprehensive sales pattern for Q4 holiday season"
- "What are the most common SQL patterns for revenue analysis?"

**Administrator Queries:**
- "Show me total LLM costs for the past 30 days"
- "Which users are consuming the most tokens?"
- "What are the top 5 most expensive queries this week?"
- "Display current consumption profile allocations"

**DevOps Engineer Queries:**
- "Execute this query asynchronously and poll for results"
- "Create an Airflow DAG for daily sales aggregation"
- "Test the same query across Claude and GPT-4 for comparison"

### Expected Demonstrations

**Module 1 (First Conversation):**
- Query: "Top 5 products by revenue"
- **MCP Tools Shown:** `base_readQuery` (executing SQL query)
- **Shows:** Strategic planning, SQL generation, table rendering
- Follow-up: "Customer demographics for top product"
- **Shows:** Context retention, multi-table joins, `TDA_FinalReport` formatting

**Module 3.2 (Capabilities Discovery):**
- **MCP Tools Tab:** Show `base_tableList`, `base_readQuery`, `base_tableDDL`, `dba_databaseVersion`
- **MCP Prompts Tab:** Show `base_databaseBusinessDesc`, `base_tableBusinessDesc`, categorized display
- **Demonstrate:** Adding new tool to profile, seeing it appear in capabilities

**Module 3.4 (System Customization):**
- **Demonstrate:** Disabling `base_tableDDL` tool and seeing queries fail without it
- **Demonstrate:** Re-enabling tool and query succeeding
- **Show:** Dynamic capability management (enable/disable specific tools/prompts)

**Module 4.2 (RAG Self-Learning):**
- **Show:** RAG case containing `base_readQuery` tool execution
- **Demonstrate:** Similar query retrieving past `base_readQuery` pattern from RAG
- **MCP Context:** How RAG captures tool names, arguments, and results

**Module 2 (REST API):**
- Convert above conversational query to cURL command
- Show session management and result retrieval

**Module 3 (Transparency):**
- Complex multi-phase query requiring recursive planning
- Demonstrate self-correction when initial approach fails

**Module 4 (Efficiency):**
- Show RAG case capture after successful query
- Demonstrate plan hydration with follow-up question
- Display token savings from RAG-guided planning

**Module 6 (Marketplace):**
- Create template from successful revenue analysis query
- Publish to marketplace as "Retail Revenue Patterns"
- Show another user deploying and using the pattern

**Module 10 (User Journeys):**
- Knowledge Worker: Full exploratory analysis workflow
- Administrator: Cost monitoring and user management
- DevOps: Production deployment with Airflow integration

---

## üìù Tutorial Creation Workflow

### For Each Section:

1. **Preparation Phase**
   - [ ] Review learning objectives
   - [ ] Prepare demo environment/data
   - [ ] Test all screen captures
   - [ ] Write and refine narration script

2. **Recording Phase**
   - [ ] Record screen capture (Tight.studio)
   - [ ] Record narration (voice-over or live)
   - [ ] Capture B-roll (UI elements, diagrams)
   - [ ] Ensure 2-5 minute target duration

3. **Post-Production Phase**
   - [ ] Edit for clarity and pacing
   - [ ] Add callouts/highlights
   - [ ] Insert transition slides
   - [ ] Review for technical accuracy

4. **Quality Assurance**
   - [ ] Verify learning objectives achieved
   - [ ] Test with sample audience
   - [ ] Gather feedback
   - [ ] Iterate if needed

---

## üéØ Success Metrics

**Tutorial Effectiveness Indicators:**
- [ ] Viewers can successfully complete learning objectives
- [ ] Clear differentiation from competitors established
- [ ] All six core principles comprehensively covered
- [ ] Actionable takeaways in every section
- [ ] Professional production quality (Tight.studio)

**Key Messaging Achieved:**
- [ ] "Two-in-one" conversation ‚Üí API paradigm clear
- [ ] "Hybrid intelligence" cloud + local positioning understood
- [ ] "Self-learning RAG" continuous improvement demonstrated
- [ ] "Marketplace" collective intelligence ecosystem showcased
- [ ] "Complete transparency" black box elimination proven
- [ ] "Financial governance" cost control demonstrated

---

## üìö Reference Materials

**Primary Sources:**
- README.md (comprehensive feature documentation)
- www.uderia.com (value proposition and messaging)
- docs/ directory (technical deep-dives)
- Source code (implementation details)

**Visual Assets Needed:**
- Architecture diagrams
- UI screenshots (all major views)
- Workflow comparison diagrams
- Cost savings visualizations
- Marketplace ecosystem graphics

**Demo Environment Requirements:**
- Configured MCP servers (database, API)
- Sample data for realistic queries
- Multiple LLM providers configured
- Pre-populated RAG collections
- Multiple user accounts for multi-user demos

---

## üöÄ Next Steps

1. **Review & Refine:** Validate this structure with stakeholders
2. **Script Writing:** Fill in narration placeholders section-by-section
3. **Environment Setup:** Prepare demo environment and sample data
4. **Pilot Recording:** Create 2-3 sample sections for feedback
5. **Iteration:** Refine based on pilot feedback
6. **Full Production:** Record all sections systematically
7. **Launch:** Publish to Tight.studio and promote

---

**Document Status:** Structure Complete - Ready for Narration Scripting  
**Next Action Required:** Begin filling in narration scripts starting with Module 1

