{
  "document_id": "80982fdb-019d-4c66-b8f2-535d80264d7a",
  "content": "© 2020 Teradata. All rights reserved.\nSQL Fundamentals\nRainer Geissendoerfer\nrainer.geissendoerfer@uderia.com\n12/01/2025\n© Teradata. All rights reserved.  However, you may combine your original work with publicly-available, Teradata-owned documentation to make a Custom Document; provided you limit\naccess to the Custom Document to you (and, if you are acting on behalf of a company, to that company for that company’s internal use).  TERADATA DISCLAIMS ALL\nREPRESENTATIONS, CONDITIONS, AND WARRANTIES OF ANY KIND (EXPRESS, IMPLIED, OR STATUTORY) WITH RESPECT TO THIS CUSTOM DOCUMENT. YOU AND PERSONS\nACCESSING THIS CUSTOM DOCUMENT BEAR THE ENTIRE RISK OF ANY RELIANCE ON THIS CUSTOM DOCUMENT, INCLUDING AS TO QUALITY, ACCURACY, AND RESULTS.\nIntroduction to SQL Fundamentals\nUsing SQL  Fundamentals\nTeradata Vantage™ - SQL Fundamentals describes basic SQL syntax, including Teradata extensions to ANSI/ISO SQL, SQL data deﬁnition, control, and manipulation, data handling,\nand basic query processing.\nUse this document with the other documents in the SQL document set.\nHow Do I Get Started?\nFamiliarize yourself with database objects. See Database Objects. Then, determine your needs and go to the related content.\nChanges and Additions\nDate Description\nJune 2022Minor edits.\nJuly 2021Minor edits.\nDatabase Objects\nThis section describes the database objects that you can use to store, manage, and access data in Teradata Vantage™, and provides guidelines to help determine the best object to\nuse for a speciﬁc purpose.\nSQL Fundamentals\nPage 2 of 99Databases and Users\nA database is a container with an allotment of space in which users can create and maintain database objects, including tables, views, triggers, indexes, stored procedures, user-\ndeﬁned functions, and macros.\nA user is similar to a database, except that a user has a password and can log on to the system, whereas a database cannot.\nThe primary database user is DBC, which owns all system disk space and contains the Data Dictionary tables and other system objects and information.\nDefining Databases and Users\nBefore you can create a database or user, you must have sufﬁcient privileges granted to you.\nTo create a database, use the CREATE DATABASE statement. You can specify the name of the database, the amount of storage to allocate, and other attributes.\nTo create a user, use the CREATE USER statement. The statement authorizes a new user identiﬁcation (user name) for the database and speciﬁes a passwo rd for user authentication.\nBecause the system creates a database for each user, the CREATE USER statement is very similar to the CREATE DATABASE statement.\nDifference Between Users and Databases\nFormally speaking, the difference between a user and a database is that a user has a password and a database does not. Users can also have default attributes such as time zone,\ndate form, character set, role, and proﬁle, while databases cannot.\nYou might infer from this that databases are passive objects, while users are active objects. That is only true in the sense that databases cannot execute SQL statements. However, a\nquery, macro, or stored procedure can execute using the privileges of the database.\nTables\nA table is referred to in set theory terminology as a relation, from which the expression relational database is derived.\nEvery relational table consists of one row of column headings (more commonly referred to as column names) and zero or more unique rows of data values.\nFormally speaking, each row represents what set theory calls a tuple. Each column represents what set theory calls an attribute.\nThe number of rows (or tuples) in a table is referred to as its cardinality and the number of columns (or attributes) is referred to as its degree or arity.\nDefining Tables\nUse the CREATE TABLE statement to deﬁne base tables. This SQL statement speciﬁes a table name, one or more column names, and the attributes of each column. The CREATE\nTABLE statement can also specify data block size, percent free space, and other physical attributes of the table.\nThe CREATE/MODIFY USER and CREATE/MODIFY DATABASE statements provide options for creating permanent journal tables.\nDefining Indexes for a Table\nAn index is a physical mechanism used to store and access the rows of a table. When you deﬁne a table, you can deﬁne a primary index or primary AMP index and one or more\nsecondary indexes.\nIf you deﬁne a table and you do not specify a PRIMARY INDEX clause, PRIMARY AMP [INDEX], NO PRIMARY INDEX clause, PRIMARY KEY constraint, UNIQUE constraint, or\nPARTITION BY, the default behavior is for the system to create the table using the ﬁrst column as the nonunique primary index. (If you prefer that the system creates a table without a\nprimary or a primary AMP index, use DBS Control to change the value of the PrimaryIndexDefault General ﬁeld.\nIf the PARTITION BY clause is speciﬁed without specifying a PRIMARY INDEX, PRIMARY AMP [INDEX], or NO PRIMARY INDEX clause, the default is NO PRIMARY INDEX regardless\nof the setting of this ﬁeld or whether a PRIMARY KEY or UNIQUE clause is speciﬁed.\nRelated Information\nFor details on indexes, see Indexes. For details on NoPI tables, see No Primary Index (NoPI) Tables.\nFor details about creating a table without a primary or a primary AMP index, and using DBS Control to change the value of the PrimaryIndexDefault General ﬁeld, see Teradata\nVantage™ - Database Utilities, B035-1102.\nDuplicate Rows in Tables\nA table deﬁned not to permit duplicate rows is called a SET table because its properties are based on set theory, where set is deﬁned as an unordered group of unique elements with\nno duplicates.\nA table deﬁned to permit duplicate rows is called a MULTISET table because its properties are based on a multiset, or bag, model, where bag and multiset are deﬁned as an\nunordered group of elements that may be duplicates.\nTemporary Tables\nTemporary tables are useful for temporary storage of data. Vantage supports these types of temporary tables.\nType Usage\nGlobal temporary A global temporary table has a persistent table deﬁnition that is stored in the Data Dictionary. Any number of sessions can materialize and populate their\nown local copies that are retained until session logoff.\nSQL Fundamentals\nPage 3 of 99Type Usage\nGlobal temporary tables are useful for storing temporary, intermediate results from multiple queries into working tables that are frequently used by\napplications.\nGlobal temporary tables are identical to ANSI global temporary tables.\nVolatile Like global temporary tables, the contents of volatile tables are only retained for the duration of a session. However, volatile tables do not have persistent\ndeﬁnitions. To populate a volatile table, a session must ﬁrst create the deﬁnition.\nGlobal temporary traceGlobal temporary trace tables are useful for debugging external routines (UDFs, UDMs, and external stored procedures). During execution, external\nroutines can write trace output to columns in a global temporary trace table.\nLike global temporary tables, global temporary trace tables have persistent deﬁnitions, but do not retain rows across sessions.\nMaterialized instances of a global temporary table share the following characteristics with volatile tables:\nPrivate to the session that created them.\nContents cannot be shared by other sessions.\nOptionally emptied at the end of each transaction using the ON COMMIT PRESERVE/DELETE rows option in the CREATE TABLE statement.\nActivity optionally logged in the transient journal using the LOG/NO LOG option in the CREATE TABLE statement.\nDropped automatically when a session ends.\nGlobal Temporary Tables\nGlobal temporary tables allow you to deﬁne a table template in the database schema, providing large savings for applications that require well known temporary table deﬁnitions.\nThe deﬁnition for a global temporary table is persistent and stored in the Data Dictionary. Space usage is charged to logoin user temporary space. Each user session can materialize\nas many as 2000 global temporary tables at a time.\nHow Global T emporary T ables W ork\nTo create the base deﬁnition for a global temporary table, use the CREATE TABLE statement and specify the keywords GLOBAL TEMPORARY to describe the table type. Although\nspace usage for materialized global temporary tables is charged to temporary space, creating the global temporary table deﬁnition requires an adequate amount of permanent space.\nAfter creation, the table exists only as a deﬁnition. It has no rows and no physical instantiation.\nWhen an application in a session accesses a table with the same name as the deﬁned base table, and the table has not already been materialized in that session, then that table is\nmaterialized as a real relation using the stored deﬁnition. Because that initial invocation is generally due to an INSERT statement, a temporary table—in the strictest sense—is usually\npopulated immediately upon its materialization.\nThere are only two occasions when an empty global temporary table is materialized:\nA CREATE INDEX statement is issued on the table.\nA COLLECT STATISTICS statement is issued on the table.\nThe following table summarizes this information.\nStatement Issued on Not-Y et-Materialized Global T emporary T able Affect\nINSERT Local instance of global temporary table is materialized and populated with data.\nCREATE INDEX … ON TEMPORARY\nCOLLECT STATISTICS ... ON TEMPORARYLocal instance of global temporary table is materialized but not populated with data.\nIssuing a SELECT, UPDATE, or DELETE on a global temporary table that is not materialized produces the same result as issuing a SELECT, UPDATE, or DELETE on an empty global\ntemporary table that is materialized.\nExample: Using Global T emporary T ables\nSuppose there are four sessions, Session 1, Session 2, Session 3, and Session 4 and two users, User_1 and User_2. Consider the scenario in the following two tables.\nStep Session Action Result\n1 1 The DBA creates a global temporary table deﬁnition in the database\nscheme named globdb.gt1 according to the following CREATE TABLE\nstatement:\nCREATE GLOBAL TEMPORARY TABLE globdb.gt1,\nLOG\n(f1 INT NOT NULL PRIMARY KEY,\nf2 DATE,\nf3 FLOAT)\nON COMMIT PRESERVE ROWS;The global temporary table deﬁnition is created and stored in the\ndatabase schema.\n2 1 User_1 logs on an SQL session and references globdb.gt1 using the\nfollowing INSERT statement:Session 1 creates a local instance of the global temporary table deﬁnition\nglobdb.gt1 (also called a materialized temporary table).\nSQL Fundamentals\nPage 4 of 99Step Session Action Result\nINSERT globdb.gt1 (1, 980101, 11.1); Immediately upon materialization, the table is populated with a single row\nwith the following values:\nf1=1\nf2=980101\nf3=11.1\nTherefore, the contents of the local instance of the global temporary table\ndeﬁnition is not empty when it is created.\nAn INSERT/DELETE/UPDATE statement that references globdb.gt1 in\nSession 1 maps to this local instance of the table.\n3 2 User_2 logs on an SQL session and issues the following SELECT\nstatement.\nSELECT * FROM globdb.gt1;No rows are returned because Session 2 has not yet materialized a local\ninstance of globdb.gt1.\n4 2 User_2 issues the following INSERT statement:\nINSERT globdb.gt1 (2, 980202, 22.2);Session 2 creates a local instance of the global temporary table deﬁnition\nglobdb.gt1.\nThe table is populated, immediately upon materialization, with a single\nrow having the following values.\nf1=2\nf2=980202\nf3=22.2\nFrom this point on, any INSERT/DELETE/UPDATE statement that\nreferences globdb.gt1 in Session 2 maps to this local instance of the\ntable.\n5 2 User_2 logs on again and issues the following SELECT statement:\nSELECT * FROM globdb.gt1;A single row containing the data (2, 980202, 22.2) is returned to the\napplication.\n6 1 User_1 logs off from Session 1. The local instance of globdb.gt1 for Session 1 is dropped.\n7 2 User_2 logs off from Session 2. The local instance of globdb.gt1 for Session 2 is dropped.\nUser_1 and User_2 continue their work, logging onto two additional sessions as described in the following table.\nStep Session Action Result\n1 3 User_1 logs on another SQL session 3 and issues the following\nSELECT statement:\nSELECT * FROM globdb.gt1;No rows are returned because Session 3 has not yet materialized a local\ninstance of globdb.gt1.\n2 3 User_1 issues the following INSERT statement:\nINSERT globdb.gt1 (3, 980303, 33.3);Session 3 creates a local instance of the global temporary table deﬁnition\nglobdb.gt1.\nThe table is populated, immediately upon materialization, with a single row\nhaving the following values.\nf1=3\nf2=980303\nf3=33.3\nFrom this point on, any INSERT/DELETE/UPDATE statement that references\nglobdb.gt1 in Session 3 maps to this local instance of the table.\n3 3 User_1 again issues the following SELECT statement:\nSELECT * FROM globdb.gt1;A single row containing the data (3, 980303, 33.3) is returned to the application.\n4 4 User_2 logs on Session 4 and issues the following CREATE INDEX\nstatement:\nCREATE INDEX (f2) ON TEMPORARY globdb.gt1;An empty local global temporary table named globdb.gt1 is created for\nSession 4.\nThis is one case where a local instance of a global temporary table is\nmaterialized without data. The other case is a COLLECT STATISTICS statement\n—in this case, the following statement:\nSQL Fundamentals\nPage 5 of 99Step Session Action Result\nCOLLECT STATISTICS ON TEMPORARY globdb.gt1;\n5 4 User_2 issues the following SELECT statement:\nSELECT * FROM globdb.gt1;No rows are returned because the local instance of globdb.gt1 for Session 4 is\nempty.\n6 4 User_2 issues the following SHOW TABLE statement:\nSHOW TABLE globdb.gt1;CREATE SET GLOBAL TEMPORARY TABLE globdb.gt1, FALLBACK, LOG\n(\nf1 INTEGER NOT NULL,\nf2 DATE FORMAT 'YYYY-MM-DD',\nf3 FLOAT)\nUNIQUE PRIMARY INDEX (f1)\nON COMMIT PRESERVE ROWS;\n7 4 User_2 issues the following SHOW TEMPORARY TABLE statement:\nSHOW TEMPORARY TABLE globdb.gt1;CREATE SET GLOBAL TEMPORARY TABLE globdb.gt1, FALLBACK, LOG\n(\nf1 INTEGER NOT NULL,\nf2 DATE FORMAT 'YYYY-MM-DD',\nf3 FLOAT)\nUNIQUE PRIMARY INDEX (f1)\nINDEX (f2)\nON COMMIT PRESERVE ROWS;\nNote that this report indicates the new index f2 that has been created for the\nlocal instance of the temporary table.\nWith few exceptions, materialized temporary tables have the same properties as permanent tables.\nAfter a global temporary table deﬁnition is materialized in a session, all further references to the table are made to the materialized table. No additional copies of the base deﬁnition are\nmaterialized for the session. This global temporary table is deﬁned for exclusive use by the session whose application materialized it.\nMaterialized global temporary tables differ from permanent tables in the following ways:\nThey are empty when ﬁrst materialized.\nTheir contents cannot be shared by another session.\nThe contents can optionally be emptied at the end of each transaction.\nThe materialized table is dropped automatically at the end of each session.\nLimitations\nYou cannot use the following CREATE TABLE options for global temporary tables:\nPARTITION BYEND\nWITH DATA\nPermanent journaling\nReferential integrity constraints\nThis means that a temporary table cannot be the referencing or referenced table in a referential integrity constraint.\nReferences to global temporary tables are not permitted in FastLoad, MultiLoad, or FastExport.\nThe Table Rebuild utility (with the exception of online archiving) operates on base global temporary tables only. Online archiving does not operate on temporary tables.\nNon-ANSI Extensions\nTransient journaling options on the global temporary table deﬁnition are permitted using the CREATE TABLE statement.\nYou can modify the transient journaling and ON COMMIT options for base global temporary tables using the ALTER TABLE statement.\nPrivileges Required\nTo materialize a global temporary table, you must have the appropriate privilege on the base global temporary table or on the containing database or user as required by the statement\nthat materializes the table.\nNo access logging is performed on materialized global temporary tables, so no access log entries are generated.\nVolatile Tables\nNeither the deﬁnition nor contents of a volatile table persist across a system restart. You must use CREATE TABLE with the VOLATILE keyword to create a new volatile table each time\nyou start a session in which it is needed.\nSQL Fundamentals\nPage 6 of 99You can create volatile tables as you need them. Being able to create a table quickly provides you with the ability to build scratch tables whenever you need them. Any volatile tables\nyou create are dropped automatically as soon as your session logs off.\nVolatile tables are always created in the logon user space, regardless of the current default database setting. That is, the database name for the table is the login user name. Space\nusage is charged to login user spool space. Each user session can materialize as many as 1000 volatile tables at a time.\nLimitations\nThe following CREATE TABLE options are not permitted for volatile tables:\nPermanent journaling\nReferential integrity constraints\nA volatile table cannot be the referencing or referenced table in a referential integrity constraint.\nCheck constraints\nCompressed columns\nDEFAULT clause\nTITLE clause\nNamed indexes\nColumn partitioning\nReferences to volatile tables are not permitted in FastLoad or MultiLoad.\nNon-ANSI Extensions\nVolatile tables are not deﬁned in ANSI.\nPrivileges Required\nTo create a volatile table, you do not need any privileges.\nNo access logging is performed on volatile tables, so no access log entries are generated.\nVolatile T able Maintenance among Multiple Sessions\nVolatile tables are private to a session. You can log on multiple sessions and create volatile tables with the same name in each session.\nHowever, when you create a volatile table, the name must be unique among all global and permanent temporary table names in the database that has the name of the login user.\nFor example, suppose you log on to Session 1 and Session 2. Assume the default database name is your login user name. Consider the following scenario.\nStage Session 1 Session 2 Result\n1 Create a volatile table named VT1. Create a volatile table named VT1. Each session creates its own copy of volatile table VT1 using your logon\nuser name as the database.\n2 Create a permanent table with an unqualiﬁed\ntable name of VT2.— Session 1 creates a permanent table named VT2 using your logon user\nname as the database.\n3 — Create a volatile table named VT2. Session 2 receives a CREATE TABLE error, because there is already a\npermanent table with that name.\n4 Create a volatile table named VT3. — Session 1 creates a volatile table named VT3 using your logon user name\nas the database.\n5 — Create a permanent table with an\nunqualiﬁed table name of VT3.Session 2 creates a permanent table named VT3 using your logon user\nname as the database.\nBecause a volatile table is known only to the session that creates it, a\npermanent table with the same name as the volatile table VT3 in Session 1\ncan be created as a permanent table in Session 2.\n6 Insert into VT3. —\nSession 1 references volatile table VT3.\nVolatile tables take precedence over permanent tables in the same\ndatabase in a session.\nBecause Session 1 has a volatile table VT3, a reference to VT3 in Session 1\nis mapped to the volatile table VT3 until it is dropped. Select from VT3.\nHowever, in Session 2, references to VT3 remain mapped to the permanent\ntable named VT3.\n7 — Create volatile table VT3. Session 2 receives a CREATE TABLE error for attempting to create the\nvolatile table VT3 because of the existence of that permanent table.\nSQL Fundamentals\nPage 7 of 99Stage Session 1 Session 2 Result\n8 — Insert into VT3. Session 2 references permanent table VT3.\n9 Drop VT3. — Session 2 drops volatile table VT3.\n10 Select from VT3. — Session 1 references the permanent table VT3.\nQueue Tables\nVantage supports queue tables, which are similar to ordinary base tables, with the additional unique property of behaving like an asynchronous ﬁrst-in-ﬁrst-out (FIFO) queue. Queue\ntables are useful for applications that want to submit queries that wait for data to be inserted into queue tables without polling.\nWhen you create a queue table, you must deﬁne a TIMESTAMP column with a default value of CURRENT_TIMESTAMP. The values in the column indicate the time the rows were\ninserted into the queue table, unless different, user-supplied values are inserted.\nYou can then use a SELECT AND CONSUME statement, which operates like a FIFO pop:\nData is returned from the row with the oldest timestamp in the speciﬁed queue table.\nThe row is deleted from the queue table, guaranteeing that the row is processed only once.\nIf no rows are available, the transaction enters a delay state until one of the following occurs:\nA row is inserted into the queue table.\nThe transaction aborts, either as a result of direct user intervention, such as the ABORT statement, or indirect user intervention, such as a DROP TABLE statement on the queue\ntable.\nTo perform a peek on a queue table, use a SELECT statement.\nError Logging Tables\nYou can create an error logging table that you associate with a permanent base table when you want Vantage to log information about the following:\nInsert errors that occur during an SQL INSERT ... SELECT operation on the permanent base table\nUpdate and insert errors that occur during an SQL MERGE operation on the permanent base table\nTo enable error logging for an INSERT ... SELECT or MERGE statement, specify the LOGGING ERRORS option.\nIF a MERGE operation or\nINSERT … SELECT operation generates\nerrors that … AND the request … THEN the error logging table contains …\nthe error logging facilities can handle completes\nan error row for each error that the operation generated.\na marker row that you can use to determine the number of error rows for\nthe request.\nThe presence of the marker row means the request completed successfully.\naborts and rolls back when referential integrity (RI)\nor unique secondary index (USI) violations are\ndetected during index maintenancean error row for each USI or RI violation that was detected.\nan error row for each error that was detected prior to index maintenance.\nThe absence of a marker row in the error logging table means the request was\naborted.\nreach the error limit speciﬁed by the\nLOGGING ERRORS optionaborts and rolls back an error row for each error that the operation generated, including the error that\ncaused the error limit to be reached.\nthe error logging facilities cannot handle aborts and rolls back an error row for each error that the operation generated until the error that the\nerror logging facilities could not handle.\nYou can use the information in the error logging table to determine how to recover from the errors, such as which data rows to delete or correct and whether to rerun the request.\nNo Primary Index (NoPI) Tables\nFor better performance when bulk loading data using Teradata FastLoad or through SQL sessions (in particular, using the INSERT statement from TPump with the ArraySupport option\nenabled), you can create a No Primary Index (NoPI) table to use as a staging table to load your data. Without a primary index (PI), the system can store rows on any AMP that is\ndesired, appending the rows to the end of the table.\nBy avoiding the data redistribution normally associated with loading data into staging tables that have a PI, NoPI tables provide a performance beneﬁt to applications that load data\ninto a staging table, transform or standardize the data, and then store the converted data into another staging table.\nApplications can also beneﬁt by using NoPI tables in the following ways:\nAs a log ﬁle\nAs a sandbox table to store data until an appropriate indexing method is determined\nA query that accesses the data in a NoPI table results in a full-table scan unless you deﬁne a secondary index on the NoPI table and use the columns that are indexed in the query.\nTemporal Tables\nSQL Fundamentals\nPage 8 of 99Temporal tables store and maintain information with respect to time.\nAnalytic Result T able\nThe primary use of an Analytical Result Table (ART) is to associate Unbounded Array Framework (UAF) function results with a name label, enabling you to easily retrieve the result data\nand pass the result to another function. An ART can have multiple layers. Each layer has its own dedicated row composition for the series or matrix. The primary layer of ART can be\nretrieved using a SELECT statement. Other layers can only be retrieved using the TD_EXTRACT_RESULTS function.\nAn ART is a table generated by the INTO ART or INTO VOLATILE ART clause when executing a UAF function. The INTO ART clause creates a permanent table. The INTO VOLATILE\nART clause creates a temporary table. You can assign a name to the table and to some of its columns, but all other aspects of the table are set by Teradata software. User\nmanipulation of ART tables, such as update, insert and delete, is not allowed.\nRelated Information\nIndexes, see Indexes. For details on NoPI tables, see No Primary Index (NoPI) Tables.\nRules for duplicate rows in a table, see CREATE TABLE in Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nFor an explanation of the features not available for global temporary base tables, see CREATE TABLE in Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and\nExamples, B035-1144.\nThe result of an INSERT operation that would create a duplicate row, see INSERT in Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nThe result of an INSERT using a SELECT subquery that would create a duplicate row, see INSERT in Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nCreating a NoPI table, see the NO PRIMARY INDEX clause for the CREATE TABLE statement in Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples,\nB035-1144.\nLoading data into staging tables from FastLoad, see Teradata® FastLoad Reference, B035-2411.\nLoading data into staging tables from TPump, see Teradata® Parallel Data Pump Reference, B035-3021.\nPrimary indexes, see Primary Indexes.\nSecondary indexes, see Secondary Indexes.\nIndividual characteristics of global temporary and volatile tables, see Global Temporary Tables and Volatile Tables.\nVolatile table limitations, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nCreating a queue table, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nThe SELECT AND CONSUME statement, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nHow to create an error logging table, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nSpecifying error handling for INSERT ... SELECT and MERGE statements, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nTemporal tables, see Teradata Vantage™ - Temporal Table Support, B035-1182 and Teradata Vantage™ - ANSI Temporal Table Support, B035-1186.\nColumns\nA column is a structural component of a table and has a name and a declared type. Each row in a table has exactly one value for each column. Each value in a row is a value in the\ndeclared type of the column. The declared type includes nulls and values of the declared type.\nDefining Columns\nThe column deﬁnition clause of the CREATE TABLE statement deﬁnes the table column elements.\nA name and a data type must be speciﬁed for each column deﬁned for a table.\nHere is an example that creates a table called employee with three columns:\nCREATE TABLE employee\n  (deptno INTEGER\n  ,name CHARACTER(23)\n  ,hiredate DATE);\nEach column can be further deﬁned with one or more optional attribute deﬁnitions. The following attributes are also elements of the SQL column deﬁnition clause:\nData type attribute declaration, such as NOT NULL, FORMAT, TITLE, and CHARACTER SET\nCOMPRESS column storage attributes clause\nDEFAULT and WITH DEFAULT default value control clauses\nPRIMARY KEY, UNIQUE, REFERENCES, and CHECK column constraint attributes clauses\nHere is an example that deﬁnes attributes for the columns in the employee table:\nCREATE TABLE employee\n  (deptno INTEGER NOT NULL\n  ,name CHARACTER(23) CHARACTER SET LATIN\n  ,hiredate DATE DEFAULT CURRENT_DATE);\nSystem-Derived and System-Generated Columns\nIn addition to the table columns that you deﬁne, tables contain columns that Vantage generates or derives dynamically.\nColumn Description\nIdentity A column that was speciﬁed with the GENERATED ALWAYS AS IDENTITY or GENERATED BY DEFAULT AS IDENTITY option in the table\ndeﬁnition.\nObject Identiﬁer (OID) For a table that has LOB columns, OID columns store pointers to subtables that store the actual LOB data.\nSQL Fundamentals\nPage 9 of 99Column Description\nPARTITION For a table that is deﬁned with a partitioned primary index (PPI), the PARTITION column provides the partition number of the combined\npartitioning expression associated with a row, where the combined partitioning expression is derived from the partitioning expressions deﬁned\nfor each level of the PPI. This is zero for a table that does not have a PPI.\nPARTITION#L1 through\nPARTITION#L62For tables that are deﬁned with a multilevel PPI, these columns provide the partition number associated with the corresponding level. These are\nzero for a table that does not have a PPI and zero if the level is greater than the number of partitions.\nROWID Contains the row identiﬁer value that uniquely identiﬁes the row.\nRestrictions apply to using the system-derived and system-generated columns in SQL statements. For example, you can use the keywords PARTITION and PARTITION#L1 through\nPARTITION#L15 in a query where a table column can be referenced, but you can only use the keyword ROWID in a CREATE JOIN INDEX statement.\nRelated Information\nData types, see Data Types.\nCREATE TABLE and the column deﬁnition clause, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nSystem-derived and system-generated columns, see Database Design PPIs, see Partitioned and Nonpartitioned Primary Indexes.\nRow identiﬁers, see Row Hash and RowID.\nData Types\nEvery data value belongs to an SQL data type. For example, when you deﬁne a column in a CREATE TABLE statement, you must specify the data type of the column. The database\nsupports the following categories of data types.\nData T ype Category Description Data T ype Examples\nARRAY/VARRAY An ARRAY data type is used for storing and accessing multidimensional data. The ARRAY data type\ncan store many values of the same speciﬁc data type in a sequential or matrix-like format.One-dimensional (1-D) ARRAY\nMultidimensional (n-D) ARRAY\nByte Byte data types store raw data as logical bit streams. These data types are stored in the client\nsystem format and are not translated by the database. The data is transmitted directly from the\nmemory of the client system.BYTE\nVARBYTE\nBLOB (Binary Large Object)\nCharacter Character data types represent characters that belong to a given character set. These data types\nrepresent character data.CHAR\nVARCHAR\nCLOB (Character Large Object)\nDateTime DateTime data types represent date, time, and timestamp values.\nDATE\nTIME\nTIMESTAMP\nTIME WITH TIME ZONE\nTIMESTAMP WITH TIME ZONE\nGeospatial Geospatial data types represent geographic information and provides a way for applications that\nmanage, analyze, and display geographic information to interface with the database.ST_Geometry\nMBR\nInterval Interval data types represent a span of time. For example, an interval value can represent a time\nspan that includes a number of years, months, days, hours, minutes, or seconds.INTERVAL YEAR\nINTERVAL YEAR TO MONTH\nINTERVAL MONTH\nINTERVAL DAY\nINTERVAL DAY TO HOUR\nINTERVAL DAY TO MINUTE\nINTERVAL DAY TO SECOND\nINTERVAL HOUR\nINTERVAL HOUR TO MINUTE\nINTERVAL HOUR TO SECOND\nINTERVAL MINUTE\nINTERVAL MINUTE TO SECOND\nINTERVAL SECOND\nJSON The JSON data type represents data that is in JSON (JavaScript Object Notation) format. JSON\nNumeric Numeric data types represent a numeric value that is an exact numeric number (integer or decimal)\nor an approximate numeric number (ﬂoating point).BYTEINT\nSMALLINT\nINTEGER\nBIGINT\nDECIMAL/NUMERIC\nSQL Fundamentals\nPage 10 of 99Data T ype Category Description Data T ype Examples\nFLOAT/REAL/DOUBLE PRECISION\nNUMBER\nParameter Parameter data types are data types that can be used only with input or result parameters in a\nfunction, method, stored procedure, or external stored procedure.TD_ANYTYPE\nVARIANT_TYPE\nPeriod A Period data type represents a time period, where a period is a set of contiguous time granules that\nextends from a beginning bound up to but not including an ending bound.PERIOD(DATE)\nPERIOD(TIME)\nPERIOD(TIME WITH TIME ZONE)\nPERIOD(TIMESTAMP)\nPERIOD(TIMESTAMP WITH TIME ZONE)\nUDT UDT (User-Deﬁned Type) data types are custom data types that you deﬁne to model the structure\nand behavior of the data used by your applications.Distinct\nStructured\nXML The XML data type represents XML content. The data is stored in a compact binary form that\npreserves the information set of the XML document, including the hierarchy information and type\ninformation derived from XML validation.XML\nRelated Information\nFor detailed information on data types and the related functions, procedures, methods, and operators that operate on these types, see the following documents:\nTeradata Vantage™ - Data Types and Literals, B035-1143.\nTeradata Vantage™ - Geospatial Data Types, B035-1181.\nTeradata Vantage™ - JSON Data Type, B035-1150.\nTeradata Vantage™ - XML Data Type, B035-1140.\nTeradata Vantage™ - SQL Functions, Expressions, and Predicates, B035-1145.\nKeys\nA primary key is a column, or combination of columns, in a table that uniquely identiﬁes each row in the table. The values deﬁning a primary key for a table:\nMust be unique\nCannot change\nCannot be null\nA foreign key is a column, or combination of columns, in a table that is also the primary key in one or more additional tables in the same database. Foreign keys provide a mechanism\nto link related tables based on key values.\nKeys and Referential Integrity\nThe database uses primary and foreign keys to maintain referential integrity.\nEffect on Row Distribution\nBecause the database uses a unique primary or secondary index to enforce a primary key, the primary key can affect how the database distributes and retrieves rows.\nPrimary Keys and Primary Indexes\nThe following table summarizes the differences between keys and indexes, using the primary key and primary index for purposes of comparison.\nPrimary Key Primary Index\nImportant element of logical data model.Not used in logical data model.\nUsed to maintain referential integrity. Used to distribute and retrieve data.\nMust be unique to identify each row. Can be unique or nonunique.\nValues cannot change. Values can change.\nCannot be null. Can be null.\nDoes not imply access path. Deﬁnes the most common access path.\nNot required for physical table deﬁnition.Required for physical table deﬁnition.\nRelated Information\nKeys and referential integrity, see Referential Integrity.\nEffect on row distribution, see Primary Indexes and Secondary Indexes.\nIndexes\nSQL Fundamentals\nPage 11 of 99An index is a mechanism that the SQL query optimizer can use to make table access more efﬁcient. Indexes enhance data access by providing a more-or-less direct path to stored\ndata to avoid performing full table scans to locate the small number of rows you typically want to retrieve or update.\nThe database parallel architecture makes indexing an aid to better performance, not a crutch necessary to ensure adequate performance. Full table scans are not something to be\nfeared in the database environment. This means that the sorts of unplanned, ad hoc queries that characterize the data warehouse process, and that often are not supported by\nindexes, perform very effectively for the database using full table scans.\nClassic Indexes and Teradata Indexes\nThe classic index for a relational database is itself a ﬁle made up of rows having these parts:\nA (possibly unique) data ﬁeld in the referenced table.\nA pointer to the location of that row in the base table (if the index is unique) or a pointer to all possible locations of rows with that data ﬁeld value (if the index is nonunique).\nBecause Vantage is a massively parallel architecture, it requires a more efﬁcient means of distributing and retrieving its data. One such method is hashing. All Teradata indexes are\nbased on row hash values rather than raw table column values, even though secondary, hash, and join indexes can be stored in order of their values to make them more useful for\nsatisfying range conditions.\nSelectivity of Indexes\nAn index that retrieves many rows is said to have weak selectivity.\nAn index that retrieves few rows is said to be strongly selective.\nThe more strongly selective an index is, the more useful it is. In some cases, it is possible to link together several weakly selective nonunique secondary indexes by bit mapping them.\nThe result is effectively a strongly selective index and a dramatic reduction in the number of table rows that must be accessed.\nRow Hash and RowID\nDatabase table rows are self-indexing with respect to their primary index and so require no additional storage space. When a row is inserted into a table, Vantage stores the 32-bit row\nhash value of the primary index with it.\nBecause row hash values are not necessarily unique, Vantage also generates a unique 32-bit numeric value (called the Uniqueness Value) that it appends to the row hash value,\nforming a unique RowID. This RowID makes each row in a table uniquely identiﬁable and ensures that hash collisions do not occur.\nIf a table is deﬁned with a partitioned primary index (PPI), the RowID also includes the combined partition number to which the row is assigned, where the combined partition number is\nderived from the partition numbers for each level of the PPI.\nFor tables with no PPI … For tables with a PPI …\nThe ﬁrst row having a speciﬁc row hash value is always assigned a uniqueness value\nof 1, which becomes the current high uniqueness value. Each time another row\nhaving the same row hash value is inserted, the current high value increments by 1,\nand that value is assigned to the row.The ﬁrst row having a speciﬁc combined partition number and row hash value is always\nassigned a uniqueness value of 1, which becomes the highest current uniqueness value. Each\ntime another row having the same combined partition number and row hash value is inserted,\nthe current high value increments by 1, and that value is assigned to the row.\nTable rows having the same row hash value are stored on disk sorted in the\nascending order of RowID.\nUniqueness values are not reused except for the special case in which the highest\nvalued row within a row hash is deleted from a table.Table rows having the same combined partition number and row hash value are stored on disk\nsorted in the ascending order of RowID.\nUniqueness values are not reused except for the special case in which the highest valued row\nwithin a combined partition number and row hash is deleted from a table.\nA RowID for a row might change, for instance, when a primary index or partitioning column is changed, or when there is complex update of the table.\nIndex Hash Mapping\nRows are distributed across the AMPS using a hashing algorithm that computes a row hash value based on the primary index. The row hash is a 32-bit value. Depending on the system\nsetting for the hash bucket size, either the higher-order 16 bits or the higher-order 20 bits of a hash value determine an associated hash bucket.\nNormally, the hash bucket size is 20 bits for new systems. If you are upgrading from an older release, the hash bucket size may be 16 bits. If a 20-bit hash bucket size is more\nappropriate for the size of a system, you can use the DBS Control Utility to change the system setting for the hash bucket size.\nIF the hash bucket size is … THEN the number of hash buckets is …\n16 bits 65536\n20 bits 1048576\nThe hash buckets are distributed as evenly as possible among the AMPs on a system.\nVantage maintains a hash map—an index of which hash buckets live on which AMPs—that it uses to determine whether rows belong to an AMP based on their row hash values. Row\nassignment is performed in a manner that ensures as equal a distribution as possible among all the AMPs on a system.\nRelated Information\nFor details about hash bucket size, see the information about the DBS Control Utility in Teradata Vantage™ - Database Utilities, B035-1102.\nAdvantages of Indexes\nThe intent of indexes is to lessen the time it takes to retrieve rows from a database. The faster the retrieval, the better.\nSQL Fundamentals\nPage 12 of 99Disadvantages of Indexes\nThey must be updated every time a row is updated, deleted, or added to a table.\nThis is only a consideration for indexes other than the primary index in the Vantage environment. The more indexes you have deﬁned for a table, the bigger the potential update\ndownside becomes.\nBecause of this, secondary, join, and hash indexes are rarely appropriate for OLTP situations.\nAll secondary indexes are stored in subtables, and join and hash indexes are stored in separate tables, exerting a burden on system storage space.\nWhen FALLBACK is deﬁned for a table, a further storage space burden is created because secondary index subtables are always duplicated whenever FALLBACK is deﬁned\nfor a table. An additional burden on system storage space is exerted when FALLBACK is deﬁned for join indexes or hash indexes or both.\nFor this reason, it is extremely important to use the EXPLAIN modiﬁer to determine optimum data manipulation statement syntax and index usage before putting statements and\nindexes to work in a production environment.\nTeradata Index Types\nPrimary index\nIn general, all database tables require a primary index because the system distributes tables on their primary indexes. Primary indexes can be:\nUnique or nonunique.\nPartitioned or nonpartitioned.\nSecondary index\nSecondary indexes can be unique or nonunique.\nJoin index (JI)\nHash index\nUnique Indexes\nA unique index, like a primary key, has a unique value for each row in a table.\nTeradata deﬁnes two different unique indexes:\nUnique primary index (UPI)\nUPIs provide optimal data distribution and are typically assigned to the primary key for a table. When a NUPI makes better sense for a table, then the primary key is frequently\nassigned to be a USI.\nUnique secondary index (USI)\nUSIs guarantee that each complete index value is unique, while ensuring that data access based on it is always a two-AMP operation.\nNonunique Indexes\nA nonunique index does not require its values to be unique. There are occasions when a nonunique index is the best choice as the primary index for a table.\nNUSIs are also very useful for many decision support situations.\nPartitioned and Nonpartitioned Primary Indexes\nPrimary indexes can be partitioned or nonpartitioned.\nA nonpartitioned primary index (NPPI) is the traditional primary index by which rows are assigned to AMPs.\nA partitioned primary index (PPI) allows rows to be partitioned, based on some set of columns, on the AMP to which they are distributed, and ordered by the hash of the primary index\ncolumns within the partition.\nA PPI can improve query performance through partition elimination. A PPI provides a useful alternative to an NPPI for executing range queries against a table, while still providing\nefﬁcient access, join, and aggregation strategies on the primary index.\nA multilevel PPI allows each partition at a level to be subpartitioned based on a partitioning expression, where the maximum number of levels is 15.\nA multilevel PPI provides multiple access paths to the rows in the base table and can improve query performance through partition elimination at each of the various levels or\ncombination of levels.\nA PPI can only be deﬁned as unique if all the partitioning columns are included in the set of primary index columns.\nJoin Indexes\nA join index is an indexing structure containing columns from one or more base tables and is generally used to resolve queries and eliminate the need to access and join the base\ntables it represents.\nJoin indexes can be deﬁned in the following general ways.\nSimple or aggregate\nSingle-table or multitable\nHash-ordered or value-ordered\nComplete or sparse\nSQL Fundamentals\nPage 13 of 99See Join Indexes.\nHash Indexes\nHash indexes are used for the same purposes as are single-table join indexes, and are less complicated to deﬁne. However, a join index offers more choices.\nFor more information, see Hash Indexes.\nCreating Indexes For a Table\nUse the CREATE TABLE statement to deﬁne a primary index and one or more secondary indexes. (You can also deﬁne secondary indexes using the CREATE INDEX statement.) You\ncan deﬁne the primary index (and any secondary index) as unique, depending on whether duplicate values are to be allowed in the indexed column set. A partitioned primary index\ncannot be deﬁned as unique if one or more partitioning columns are not included in the primary index.\nTo create hash or join indexes, use the CREATE HASH INDEX and CREATE JOIN INDEX statements, respectively.\nDetermining the Usefulness of Indexes\nThe selection of indexes to support a query is not under user control. You cannot provide the Teradata query optimizer with pragmas or hints, nor can you specify resource options or\ncontrol index locking.\nThe only references made to indexes in the SQL language concern their deﬁnition and not their use. Teradata SQL data manipulation language statements do not provide for any\nspeciﬁcation of indexes.\nThere are several implications of this behavior.\nTo ensure that the optimizer has access to current information about how to best optimize any query or update made to the database, you must use the COLLECT STATISTICS\nstatement to collect statistics regularly on all indexed columns, all frequently joined columns, and all columns frequently speciﬁed in query predicates.\nTo ensure that your queries access your data in the most efﬁcient manner possible, use the EXPLAIN request modiﬁer to try out various candidate queries or updates and to\nnote which indexes are used by the optimizer in their execution (if any) as well as to examine the relative cost of the operation.\nRelated Information\nLinking weakly selective secondary indexes into a strongly selective unit using bit mapping, see NUSI Bit Mapping.\nPPIs, see Partitioned and Nonpartitioned Primary Indexes.\nJoin indexes, see Join Indexes.\nUsing the EXPLAIN request modiﬁer, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nAdditional performance-related information about how to use the access and join plan Teradata Vantage™ - Database Design, B035-1094 reports produced by EXPLAIN to\noptimize the performance of your databases, see Teradata Vantage™ - Database Design, B035-1094.\nCollecting and maintaining accurate database statistics, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nPrimary Indexes\nThe primary index for a table controls the distribution and retrieval of the data for that table across the AMPs. Both distribution and retrieval of the data is controlled using the database\nhashing algorithm.\nIf the primary index is deﬁned as a partitioned primary index (PPI), the data is partitioned, based on some set of columns, on each AMP, and ordered by the hash of the primary index\ncolumns within the partition.\nData accessed based on a primary index is always a one-AMP operation because a row and its index are stored on the same AMP. This is true whether the primary index is unique or\nnonunique, and whether it is partitioned or nonpartitioned.\nPrimary Index Assignment\nIn general, most database tables require a primary index. (Some tables in the Data Dictionary do not have primary indexes and a global temporary trace table is not allowed to have a\nprimary index.) To create a primary index, use the CREATE TABLE statement.\nIf you do not assign a primary index explicitly when you create a table, the database assigns a primary index, based on the following rules.\nPrimary Index Primary Key Unique Column Constraint The database selects the …\nno YES no primary key column set to be a UPI.\nno no YES ﬁrst column or columns having a UNIQUE constraint to be a UPI.\nno YES YES primary key column set to be a UPI.\nno no no ﬁrst column deﬁned for the table to be a NUPI.\nIf the data type of the ﬁrst column in the table is a LOB, then the CREATE TABLE operation\naborts and the system returns an error message.\nIn general, the best practice is to specify a primary index instead of having the database select a default primary index.\nUniform Distribution of Data and Optimal Access Considerations\nWhen choosing the primary index for a table, there are two essential factors to keep in mind: uniform distribution of the data and optimal access.\nWith respect to uniform data distribution, consider the following factors:\nSQL Fundamentals\nPage 14 of 99The more distinct the primary index values, the better.\nRows having the same primary index value are distributed to the same AMP.\nParallel processing is more efﬁcient when table rows are distributed evenly across the AMPs.\nWith respect to optimal data access, consider the following factors:\nChoose the primary index on the most frequently used access path.\nFor example\nIf rows are generally accessed by a range query, consider deﬁning a PPI on the table that creates a useful set of partitions.\nIf the table is frequently joined with a speciﬁc set of tables, consider deﬁning the primary index on the column set that is typically used as the join condition.\nPrimary index operations must provide the full primary index value.\nPrimary index retrievals on a single value are always one-AMP operations.\nAlthough it is true that the columns you choose to be the primary index for a table are often the same columns that deﬁne the primary key, it is also true that primary indexes often\ncomprise ﬁelds that are neither unique nor components of the primary key for the table.\nUnique and Nonunique Primary Index Considerations\nIn addition to uniform distribution of data and optimal access considerations, other guidelines and performance considerations apply to selecting a unique or a nonunique column set\nas the primary index for a table.\nOther considerations can include:\nPrimary and other alternate key column sets\nThe value range seen when using predicates in a WHERE clause\nWhether access can involve multiple rows or a spool ﬁle or both\nPartitioning Considerations\nThe decision to deﬁne a single-level or multilevel Partitioned Primary Index (PPI) for a table depends on how its rows are most frequently accessed. PPIs are designed to optimize\nrange queries while also providing efﬁcient primary index join strategies and may be appropriate for other classes of queries. Performance of such queries is improved by accessing\nonly the rows of the qualiﬁed partitions.\nA PPI increases query efﬁciency by avoiding full table scans without the overhead and maintenance costs of secondary indexes.\nThe most important factors for PPIs are accessibility and maximization of partition elimination. In all cases, it is critical for parallel efﬁciency to deﬁne a primary index that distributes the\nrows of the table fairly evenly across the AMPs.\nRestrictions\nRestrictions apply to the columns you choose to be the primary index for a table. For partitioned primary indexes, further restrictions apply to the columns you choose to be partitioning\ncolumns. Here are some of the restrictions:\nA primary index column or partitioning column cannot be a column that has a CLOB, BLOB, ST_Geometry, MBR, or Period data type.\nPartitioning expressions for single-level PPIs may only use the following general forms:\nColumn (must be INTEGER or a data type that casts to INTEGER)\nExpressions based on one or more columns, where the expression evaluates to INTEGER or data type that casts to INTEGER\nThe CASE_N and RANGE_N functions\nPartitioning expressions of a multilevel PPI may only specify CASE_N and RANGE_N functions.\nYou cannot compress columns that are members of the primary index column set or are partitioning columns.\nOther restrictions apply to the partitioning expression for PPIs. For example:\nComparison of character data where the server character set is KANJI1 or KANJISJIS is not allowed.\nThe expression for the RANGE_N test value must evaluate to BYTEINT, SMALLINT, INTEGER, DATE, CHAR, VARCHAR, GRAPHIC or VARGRAPHIC.\nNondeterministic partitioning expressions are not allowed, including cases that might not report errors, such as casting TIMESTAMP to DATE, because of the potential for wrong\nresults.\nPrimary Index Properties\nDeﬁned with the CREATE TABLE data deﬁnition statement.\nCREATE INDEX is used only to create secondary indexes.\nModiﬁed with the ALTER TABLE data deﬁnition statement.\nSome modiﬁcations, such as partitioning and primary index columns, require an empty table.\nAutomatically assigned by CREATE TABLE if you do not explicitly deﬁne a primary index. However, the best practice is to always specify the primary index, because the default\nmay not be appropriate for the table.\nCan be composed of as many as 64 columns.\nA maximum of one can be deﬁned per table.\nCan be partitioned or nonpartitioned.\nPartitioned primary indexes are not automatically assigned. You must explicitly deﬁne a partitioned primary index.\nCan be unique or nonunique.\nSQL Fundamentals\nPage 15 of 99Note that a partitioned primary index can only be unique if all the partitioning columns are also included as primary index columns. If the primary index does not include all the\npartitioning columns, uniqueness on the primary index columns may be enforced with a unique secondary index on the same columns as the primary index.\nDeﬁned as nonunique if the primary index is not deﬁned explicitly as unique or if the primary index is speciﬁed for a single column SET table.\nControls data distribution and retrieval using the Teradata hashing algorithm.\nImproves performance when used correctly in the WHERE clause of an SQL data manipulation statement to perform the following actions.\nSingle-AMP retrievals.\nJoins between tables with identical primary indexes, the optimal scenario.\nPartition elimination when the primary index is partitioned.\nRelated Information\nUsing primary indexes to enhance the performance of your databases, see Teradata Vantage™ - Database Design, B035-1094.\nDistribution and retrieval of the data controlled using the database hashing algorithm, see Row Hash and RowID.\nPartitioning considerations, including information on column partitioning, called Teradata Columnar, see Teradata Vantage™ - Database Design, B035-1094, Teradata Vantage™\n- SQL Data Deﬁnition Language Syntax and Examples, B035-1144, and Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nDetails on all the restrictions that apply to primary indexes and partitioned primary indexes, see CREATE TABLE in Teradata Vantage™ - SQL Data Deﬁnition Language Syntax\nand Examples, B035-1144.\nCriteria for selecting a primary index, see Teradata Vantage™ - Database Design, B035-1094.\nSecondary Indexes\nSecondary indexes are never required for database tables, but they can often improve system performance.\nYou create secondary indexes explicitly using the CREATE TABLE and CREATE INDEX statements. The database can implicitly create unique secondary indexes; for example, when\nyou use a CREATE TABLE statement that speciﬁes a primary index, the database implicitly creates unique secondary indexes on column sets that you specify using PRIMARY KEY or\nUNIQUE constraints.\nCreating a secondary index causes the database to build a separate internal subtable to contain the index rows, thus adding another set of rows that requires updating each time a\ntable row is inserted, deleted, or updated.\nNonunique secondary indexes (NUSIs) can be speciﬁed as either hash-ordered or value-ordered. Value-ordered NUSIs are limited to a single numeric-valued (including DATE) sort key\nwhose size is four or fewer bytes.\nSecondary index subtables are also duplicated whenever a table is deﬁned with FALLBACK.\nAfter the table is created and usage patterns have developed, additional secondary indexes can be deﬁned with the CREATE INDEX statement.\nUnique and Nonunique Secondary Indexes\nVantage processes USIs and NUSIs very differently.\nConsider the following statements that deﬁne a USI and a NUSI.\nSecondary Index Statement\nUSI\nCREATE UNIQUE INDEX (customer_number)  ON customer_table;\nNUSI\nCREATE INDEX (customer_name)  ON customer_table;\nThe following table highlights differences in the build process for the preceding statements.\nUSI Build Process NUSI Build Process\nEach AMP accesses its subset of the base table rows. Each AMP accesses its subset of the base table rows.\nEach AMP copies the secondary index value and appends the RowID for the base table\nrow.Each AMP builds a spool ﬁle containing each secondary index value found followed by\nthe RowID for the row it came from.\nEach AMP creates a Row Hash on the secondary index value and puts all three values\nonto the BYNET.For hash-ordered NUSIs, each AMP sorts the RowIDs for each secondary index value into\nascending order.\nFor value-ordered NUSIs, the rows are sorted by NUSI value order.\nThe appropriate AMP receives the data and creates a row in the index subtable.\nIf the AMP receives a row with a duplicate index value, an error is reported.For hash-ordered NUSIs, each AMP creates a row hash value for each secondary index\nvalue on a local basis and creates a row in its portion of the index subtable.\nFor value-ordered NUSIs, storage is based on NUSI value rather than the row hash value\nfor the secondary index.\nEach row contains one or more RowIDs for the index value.\nConsider the following statements that access a USI and a NUSI.\nSQL Fundamentals\nPage 16 of 99Secondary Index Statement\nUSI\nSELECT * FROM customer_table  WHERE customer_number=12;\nNUSI\nSELECT * FROM customer_table  WHERE customer_name = 'SMITH';\nThe following table identiﬁes differences for the access process of the preceding statements.\nUSI Access Process NUSI Access Process\nThe supplied index value hashes to the corresponding secondary index row.A message containing the secondary index value is broadcast to every AMP.\nThe retrieved base table RowID is used to access the speciﬁc data row. For a hash-ordered NUSI, each AMP creates a local row hash and uses it to access its portion of the\nindex subtable to see if a corresponding row exists.\nValue-ordered NUSI index subtable values are scanned only for the range of values speciﬁed by the\nquery.\nThe process is complete.\nThis is typically a two-AMP operation.If an index row is found, the AMP uses the RowID or value order list to access the corresponding\nbase table rows.\n  The process is complete.\nThis is always an all-AMP operation, with the exception of a NUSI that is deﬁned on the same\ncolumns as the primary index.\nThe NUSI is not used if the estimated number of rows to be read in the base table is equal to or greater than the estimated number of data blocks in the base table; in this case, a full\ntable scan is done, or, if appropriate, partition scans are done.\nNUSIs and Covering\nThe Optimizer aggressively pursues NUSIs when they cover a query. Covered columns can be speciﬁed anywhere in the query, including the select list, the WHERE clause, aggregate\nfunctions, GROUP BY clauses, expressions, and so on. Presence of a WHERE condition on each indexed column is not a prerequisite for using a NUSI to cover a query.\nValue-Ordered NUSIs\nValue-ordered NUSIs are very efﬁcient for range conditions, and more so when strongly selective or when combined with covering. Because the NUSI rows are sorted by data value, it\nis possible to search only a portion of the index subtable for a given range of key values.\nValue-ordered NUSIs have the following limitations.\nThe sort key is limited to a single numeric or DATE column.\nThe sort key column must be four or fewer bytes.\nThe following query is an example of the sort of SELECT statement for which value-ordered NUSIs were designed.\n   SELECT *\n   FROM Orders\n   WHERE o_date BETWEEN DATE '1998-10-01' AND DATE '1998-10-07';\nMultiple Secondary Indexes and Composites\nDatabase designers frequently deﬁne multiple secondary indexes on a table.\nFor example, the following statements deﬁne two secondary indexes on the EMPLOYEE table:\n   CREATE INDEX (department_number) ON EMPLOYEE;\n   CREATE INDEX (job_code) ON EMPLOYEE;\nThe WHERE clause in the following query speciﬁes the columns that have the secondary indexes deﬁned on them:\n   SELECT last_name, first_name, salary_amount\n   FROM employee\n   WHERE department_number = 500\n   AND job_code = 2147;\nWhether the Optimizer chooses to include one, all, or none of the secondary indexes in its query plan depends entirely on their individual and composite selectivity.\nRelated Information\nFor more information on multiple and composite secondary index access, and other aspects of index selection, see Teradata Vantage™ - Database Design, B035-1094.\nNUSI Bit Mapping\nBit mapping is a technique used by the Optimizer to effectively link several weakly selective indexes in a way that creates a result that drastically reduces the number of base rows that\nmust be accessed to retrieve the desired data. The process determines common rowIDs among multiple NUSI values by means of the logical intersection operation.\nSQL Fundamentals\nPage 17 of 99Bit mapping is signiﬁcantly faster than the three-part process of copying, sorting, and comparing rowID lists. Additionally, the technique dramatically reduces the number of base table\nI/Os required to retrieve the requested rows.\nRelated Information\nWhen Vantage performs NUSI bit mapping, see Teradata Vantage™ - Database Design, B035-1094.\nHow NUSI bit maps are computed, see Teradata Vantage™ - Database Design, B035-1094.\nUsing the EXPLAIN modiﬁer to determine if bit mapping is being used for your indexes, see Teradata Vantage™ - Database Design, B035-1094 or Teradata Vantage™ - SQL\nData Manipulation Language, B035-1146.\nSecondary Index Properties\nCan enhance the speed of data retrieval.\nBecause of this, secondary indexes are most useful in decision support applications.\nDo not affect data distribution.\nCan be a maximum of 32 deﬁned per table.\nCan be composed of as many as 64 columns.\nFor a value-ordered NUSI, only a single numeric or DATE column of four or fewer bytes may be speciﬁed for the sort key.\nFor a hash-ordered covering index, only a single column may be speciﬁed for the hash ordering.\nCan be created or dropped dynamically as data usage changes or if they are found not to be useful for optimizing data retrieval performance.\nRequire additional disk space to store subtables.\nRequire additional I/Os on inserts and deletes.\nBecause of this, secondary indexes might not be as useful in OLTP applications.\nShould not be deﬁned on columns whose values change frequently.\nShould not include columns that do not enhance selectivity.\nShould not use composite secondary indexes when multiple single column indexes and bit mapping might be used instead.\nComposite secondary index is useful if it reduces the number of rows that must be accessed.\nThe Optimizer does not use composite secondary indexes unless there are explicit values for each column in the index.\nMost efﬁcient for selecting a small number of rows.\nCan be unique or nonunique.\nNUSIs can be hash-ordered or value-ordered, and can optionally include covering columns.\nCannot be partitioned, but can be deﬁned on a table with a partitioned primary index.\nUSI and NUSI Properties\nUSI NUSI\nGuarantee that each complete index value is unique.\nAny access using the index is a two-AMP operation.Useful for locating rows having a speciﬁc value in the index.\nCan be hash-ordered or value-ordered.\nValue-ordered NUSIs are particularly useful for enhancing the performance of range queries.\nCan include covering columns.\nAny access using the index is an all-AMP operation.\nRelated Information\nFor more information on CREATE TABLE and CREATE INDEX, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nJoin Indexes\nJoin indexes are not indexes in the usual sense of the word. They are ﬁle structures designed to permit queries (join queries in the case of multitable join indexes) to be resolved by\naccessing the index instead of having to access and join their underlying base tables.\nYou can use join indexes to:\nDeﬁne a prejoin table on frequently joined columns (with optional aggregation) without denormalizing the database.\nCreate a full or partial replication of a base table with a primary index on a foreign key column table to facilitate joins of very large tables by hashing their rows to the same AMP\nas the large table.\nDeﬁne a summary table without denormalizing the database.\nYou can deﬁne a join index on one or several tables.\nDepending on how the index is deﬁned, join indexes can also be useful for queries where the index structure contains only some of the columns referenced in the statement. This\nsituation is referred to as a partial cover of the query.\nUnlike traditional indexes, join indexes do not implicitly store pointers to their associated base table rows. Instead, they are generally used as a quick access method that eliminates the\nneed to access and join the base tables they represent. They substitute for rather than point to base table rows. The only exception to this is the case where an index partially covers a\nquery. If the index is deﬁned using either the ROWID keyword or the UPI or USI of its base table as one of its columns, then it can be used to join with the base table to cover the query.\nDefining Join Indexes\nSQL Fundamentals\nPage 18 of 99To create a join index, use the CREATE JOIN INDEX statement.\nFor example, suppose that a common task is to look up customer orders by customer number and date. You might create a join index like the following, linking the customer table, the\norder table, and the order detail table:\n   CREATE JOIN INDEX cust_ord2\n   AS SELECT cust.customerid,cust.loc,ord.ordid,item,qty,odate\n   FROM cust, ord, orditm\n   WHERE cust.customerid = ord.customerid\n   AND ord.ordid = orditm.ordid;\nMultitable Join Indexes\nA multitable join index stores and maintains the joined rows of two or more tables and, optionally, aggregates selected columns.\nMultitable join indexes are for join queries that are performed frequently enough to justify deﬁning a prejoin on the joined columns.\nA multitable join index is useful for queries where the index structure contains all the columns referenced by one or more joins, thereby allowing the index to cover that part of the query,\nmaking it possible to retrieve the requested data from the index rather than accessing its underlying base tables. For obvious reasons, an index with this property is often referred to as\na covering index.\nSingle-Table Join Indexes\nSingle-table join indexes are very useful for resolving joins on large tables without having to redistribute the joined rows across the AMPs.\nSingle-table join indexes facilitate joins by hashing a frequently joined subset of base table columns to the same AMP as the table rows to which they are frequently joined. This\nenhanced geography eliminates BYNET trafﬁc as well as often providing a smaller sized row to be read and joined.\nAggregate Join Indexes\nWhen query performance is of utmost importance, aggregate join indexes offer an extremely efﬁcient, cost-effective method of resolving queries that frequently specify the same\naggregate operations on the same column or columns. When aggregate join indexes are available, the system does not have to repeat aggregate calculations for every query.\nYou can deﬁne an aggregate join index on two or more tables, or on a single table. A single-table aggregate join index includes a summary table with:\nA subset of columns from a base table\nAdditional columns for the aggregate summaries of the base table columns\nSparse Join Indexes\nYou can create join indexes that limit the number of rows in the index to only those that are accessed when, for example, a frequently run query references only a small, well known\nsubset of the rows of a large base table. By using a constant expression to ﬁlter the rows included in the join index, you can create what is known as a sparse index.\nAny join index, whether simple or aggregate, multitable or single-table, can be sparse.\nTo create a sparse index, use the WHERE clause in the CREATE JOIN INDEX statement.\nEffects of Join Indexes\nLoad Utilities\nMultiLoad and FastLoad utilities cannot be used to load or unload data into base tables that have a join index deﬁned on them because join indexes are not maintained during\nthe execution of these utilities. If an error occurs because of a join index, take these steps:\nEnsure that any queries that use the join index are not running.\nDrop the join index. (The system defers completion of this step until there are no more queries running that use the join index.)\nLoad the data into the base table.\nRecreate the join index.\nThe TPump utility, which performs standard SQL row inserts and updates, can be used to load or unload data into base tables with join indexes because it properly\nmaintains join indexes during execution. However, in some cases, performance may improve by dropping join indexes on the table prior to the load and recreating them\nafter the load.\nPermanent Journal Recovery\nUsing a permanent journal to recover a base table (that is, ROLLBACK or ROLLFORWARD) with an associated join index deﬁned is permitted. The join index is not\nautomatically rebuilt during the recovery process. Instead, it is marked as nonvalid and it must be dropped and recreated before it can be used again in the execution of\nqueries.\nJoin Indexes and Base Tables\nIn most respects, a join index is similar to a base table. For example, you can do the following things to a join index:\nCreate nonunique secondary indexes on its columns.\nCreate a unique primary index on its columns, provided it is a non-compressed and nonvalue-ordered single-table join index.\nExecute COLLECT STATISTICS, DROP STATISTICS, HELP, and SHOW statements.\nPartition its primary index, if it is a noncompressed join index.\nUnlike base tables, you cannot do the following things with join indexes:\nSQL Fundamentals\nPage 19 of 99Query or update join index rows explicitly.\nStore and maintain arbitrary query results such as expressions.\nYou can maintain aggregates or sparse indexes if you deﬁne the join index to do so.\nRelated Information\nCreating or dropping join indexes, or displaying the attributes of the columns deﬁned by a join index, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and\nExamples, B035-1144.\nUsing join indexes to enhance the performance of your databases, see Teradata Vantage™ - Database Design, B035-1094 or Teradata Vantage™ - SQL Data Deﬁnition\nLanguage Syntax and Examples, B035-1144.\nDatabase design considerations for join indexes, see Teradata Vantage™ - Database Design, B035-1094.\nImproving join index performance, see Teradata Vantage™ - Database Design, B035-1094.\nHash Indexes\nHash indexes are used for the same purposes as are single-table join indexes, and are less complicated to deﬁne. However, a join index offers more choices.\nHash Index Single-T able Join Index\nColumn list cannot contain aggregate or ordered analytical functions. Column list can contain aggregate functions.\nCannot have a unique primary index. A non-compressed and nonvalue-ordered single-table join index can have a unique primary\nindex.\nCannot have a secondary index. Can have a secondary index.\nSupports transparently added, system-deﬁned columns that point to the underlying\nbase table rows.Does not implicitly add underlying base table row pointers.\nPointers to underlying base table rows can be created explicitly by deﬁning one element of the\ncolumn list using the ROWID keyword or the UPI or USI of the base table.\nCannot be deﬁned on a NoPI table. Can be deﬁned on a NoPI table.\nHash indexes are useful for creating a full or partial replication of a base table with a primary index on a foreign key column to facilitate joins of very large tables by hashing them to the\nsame AMP.\nYou can deﬁne a hash index on one table only. The functionality of hash indexes is a subset to that of single-table join indexes.\nHash and Single-Table Join Indexes\nThe reasons for using hash indexes are similar to those for using single-table join indexes. Not only can hash indexes optionally be speciﬁed to be distributed in such a way that their\nrows are AMP-local with their associated base table rows, they also implicitly provide an alternate direct access path to those base table rows. This facility makes hash indexes\nsomewhat similar to secondary indexes in function. Hash indexes are also useful for covering queries so that the base table need not be accessed at all.\nThe following list summarizes the similarities of hash and single-table join indexes:\nPrimary function of both is to improve query performance.\nBoth are maintained automatically by the system when the relevant columns of their base table are updated by a DELETE, INSERT, UPDATE, or MERGE statement.\nBoth can be the object of any of the following SQL statements:\nCOLLECT STATISTICS\nDROP STATISTICS\nHELP INDEX\nSHOW\nBoth receive their space allocation from permanent space and are stored in distinct tables.\nThe storage organization for both supports a compressed format to reduce storage space, but for a hash index, Vantage makes this decision.\nBoth can be FALLBACK protected.\nNeither can be queried or directly updated.\nNeither can store an arbitrary query result.\nBoth share the same restrictions for use with the MultiLoad and FastLoad utilities.\nA hash index implicitly deﬁnes a direct access path to base table rows. A join index may be explicitly speciﬁed to deﬁne a direct access path to base table rows.\nEffects of Hash Indexes\nHash indexes affect Vantage functions and features the same way join indexes affect Vantage functions and features.\nQueries Using a Hash Index\nIn most respects, a hash index is similar to a base table. For example, you can perform COLLECT STATISTICS, DROP STATISTICS, HELP, and SHOW statements on a hash index.\nUnlike base tables, you cannot do the following things with hash indexes:\nQuery or update hash index rows explicitly.\nStore and maintain arbitrary query results such as expressions.\nCreate explicit unique indexes on its columns.\nPartition the primary index of the hash index.\nRelated Information\nSQL Fundamentals\nPage 20 of 99For more information about:\nHash indexes, see Effects of Hash Indexes.\nUsing CREATE HASH INDEX to create a hash index, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nUsing DROP HASH INDEX to drop a hash index, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nUsing HELP HASH INDEX to display the data types of the columns deﬁned by a hash index, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples,\nB035-1144.\nDatabase design considerations for hash indexes, see Teradata Vantage™ - Database Design, B035-1094.\nConsult the following documents for more detailed information on using hash indexes to enhance the performance of your databases:\nTeradata Vantage™ - Database Design, B035-1094\nTeradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144\nReferential Integrity\nReferential integrity (RI) is deﬁned as all the following notions.\nThe concept of relationships between tables, based on the deﬁnition of a primary key (or UNIQUE alternate key) and a foreign key.\nA mechanism that provides for speciﬁcation of columns within a referencing table that are foreign keys for columns in some other referenced table.\nReferenced columns must be deﬁned as Primary key columns or Unique columns.\nA reliable mechanism for preventing accidental database corruption when performing inserts, updates, and deletes.\nReferential integrity requires that a row having a value that is not null for a referencing column cannot exist in a table if an equal value does not exist in a referenced column.\nReferential Integrity Enforcement\nThe database supports two forms of declarative SQL for enforcing referential integrity:\nA standard method that enforces RI on a row-by-row basis\nA batch method that enforces RI on a statement basis\nBoth methods offer the same measure of integrity enforcement, but perform it in different ways.\nA third form, sometimes informally referred to as soft RI, is related to these because it provides a declarative deﬁnition for a referential relationship, but it does not enforce that\nrelationship. Enforcement of the declared referential relationship is left to the user by any appropriate method.\nReferencing (Child) Table\nThe referencing table is referred to as the child table, and the speciﬁed child table columns are the referencing columns.\nReferencing columns must have the same numbers and types of columns, data types, and sensitivity as the referenced table keys. Column-level constraints are not compared and, for\nstandard referential integrity, compression is not allowed on either referenced or referencing columns.\nReferenced (Parent) Table\nA child table must have a parent, and the referenced table is referred to as the parent table.\nThe parent key columns in the parent table are the referenced columns.\nFor standard and batch RI, the referenced columns must be one of the following unique indexes:\nA unique primary index (UPI), deﬁned as NOT NULL\nA unique secondary index (USI), deﬁned as NOT NULL\nSoft RI does not require any index on the referenced columns.\nTerminology\nTerm Deﬁnition\nChild Table A table where the referential constraints are deﬁned.\nChild table and referencing table are synonyms.\nParent Table The table referenced by a child table.\nParent table and referenced table are synonyms.\nPrimary Key\nUNIQUE Alternate KeyA unique identiﬁer for a row of a table.\nForeign Key A column set in the child table that is also the primary key (or a UNIQUE alternate key) in the parent table.\nForeign keys can consist of as many as 64 different columns.\nReferential Constraint A constraint deﬁned on a column set or a table to ensure referential integrity.\nSQL Fundamentals\nPage 21 of 99Term Deﬁnition\nFor example, consider the following table deﬁnition:\nCREATE TABLE A\n(A1 CHAR(10) REFERENCES B (B1), /* 1 */\n A2 INTEGER\nFOREIGN KEY (A1,A2) REFERENCES C /* 2 */\nPRIMARY INDEX (A1));\nThis CREATE TABLE statement speciﬁes the following referential integrity constraints.\nConstraint 1 is deﬁned at the column level. Implicit foreign key A1 references the parent key B1 in table B.\nConstraint 2 is deﬁned at the table level.\nExplicit composite foreign key (A1, A2) implicitly references the UPI (or a USI) of parent table C, which must be two columns, the ﬁrst typed\nCHAR(10) and the second typed INTEGER.\nBoth parent table columns must also be deﬁned as NOT NULL.\nWhy Referential Integrity Is Important\nConsider the employee and payroll tables for any business.\nWith referential integrity constraints, the two tables work together as one. When one table gets updated, the other table also gets updated.\nThe following case depicts a useful referential integrity scenario.\nLooking for a better career, Mr. Clark Johnson leaves his company. Clark Johnson is deleted from the employee table.\nThe payroll table, however, does not get updated because the payroll clerk simply forgets to do so. Consequently, Mr. Clark Johnson keeps getting paid.\nWith good database design, referential integrity relationship would have been deﬁned on these tables. They would have been linked and, depending on the deﬁned constraints, the\ndeletion of Clark Johnson from the employee table could not be performed unless it was accompanied by the deletion of Clark Johnson from the payroll table.\nBesides data integrity and data consistency, referential integrity also has the beneﬁts listed in the following table.\nBeneﬁt Description\nIncreases development productivity It is not necessary to code SQL statements to enforce referential constraints.\nVantage automatically enforces referential integrity.\nRequires fewer programs to be writtenAll update activities are programmed to ensure that referential constraints are not violated.\nVantage enforces referential integrity in all environments.\nNo additional programs are required.\nImproves performance Vantage chooses the most efﬁcient method to enforce the referential constraints.\nVantage can optimize queries based on the fact that there is referential integrity.\nRules for Assigning Columns as FOREIGN KEYS\nThe FOREIGN KEY columns in the referencing table must be identical in deﬁnition with the keys in the referenced table. Corresponding columns must have the same data type and\ncase sensitivity.\nFor standard referential integrity, the COMPRESS option is not permitted on either the referenced or referencing column(s).\nColumn level constraints are not compared.\nA one-column FOREIGN KEY cannot reference a single column in a multicolumn primary or unique key—the foreign and primary/unique key must contain the same number of\ncolumns.\nCircular References\nReferences can be deﬁned as circular in that Table A can reference Table B, which can reference Table A. At least one set of FOREIGN KEYS must be deﬁned on nullable columns.\nIf the FOREIGN KEYS in Table A are on columns deﬁned as nullable, then rows could be inserted into Table A with nulls for the FOREIGN KEY columns. When the appropriate rows\nexist in Table B, the nulls of the FOREIGN KEY columns in Table A could then be updated to contain values that are not null that match the Table B values.\nReferences to the Table Itself\nFOREIGN KEY references can also be to the same table that contains the FOREIGN KEY.\nThe referenced columns must be different columns than the FOREIGN KEY, and both the referenced and referencing columns must subscribe to the referential integrity rules.\nCREATE and ALTER TABLE Syntax\nReferential integrity affects the syntax and semantics of CREATE TABLE and ALTER TABLE.\nSQL Fundamentals\nPage 22 of 99Maintaining Foreign Keys\nDeﬁnition of a FOREIGN KEY requires that Vantage maintain the integrity deﬁned between the referenced and referencing table.\nVantage maintains the integrity of foreign keys as explained in the following table.\nFor this data manipulation activity … The system veriﬁes that …\nA row is inserted into a referencing table and foreign key columns are\ndeﬁned to be NOT NULL.a row exists in the referenced table with the same values as those in the foreign key columns.\nIf such a row does not exist, then an error is returned.\nIf the foreign key contains multiple columns, and if any one column value of the foreign key is null, then\nnone of the foreign key values are validated.\nThe values in foreign key columns are altered to be NOT NULL. a row exists in the referenced table that contains values equal to the altered values of all of the foreign key\ncolumns.\nIf such a row does not exist, then an error is returned.\nA row is deleted from a referenced table. no rows exist in referencing tables with foreign key values equal to those of the row to be deleted.\nIf such rows exist, then an error is returned.\nBefore a referenced column in a referenced table is updated. no rows exist in a referencing table with foreign key values equal to those of the referenced columns.\nIf such rows exist, then an error is returned.\nBefore the structure of columns deﬁned as foreign keys or referenced\nby foreign keys is altered.the change would not violate the rules for deﬁnition of a foreign key constraint.\nAn ALTER TABLE or DROP INDEX statement attempting to change such a columns structure returns an\nerror.\nA table referenced by another is dropped. the referencing table has dropped its foreign key reference to the referenced table.\nAn ALTER TABLE statement adds a foreign key reference to a table.\nThe same processes occur whether the reference is deﬁned for\nstandard or for soft referential integrity.all of the values in the foreign key columns are validated against columns in the referenced table.\nWhen the system parses ALTER TABLE, it deﬁnes an error table that:\nHas the same columns and primary index as the target table of the ALTER TABLE statement.\nHas a name that is the same as the target table name sufﬁxed with the reference index number.\nA reference index number is assigned to each foreign key constraint for a table.\nTo determine the number, use one of the following system views: RI_Child_TablesV,\nRI_Distinct_ChildrenV, RI_Distinct_ParentsV, RI_Parent_TablesV.\nIs created under the same user or database as the table being altered.\nIf a table already exists with the same name as that generated for the error table then an error is\nreturned to the ALTER TABLE statement.\nRows in the referencing table that contain values in the foreign key columns that cannot be found in any row\nof the referenced table are copied into the error table (the base data of the target table is not modiﬁed).\nIt is your responsibility to:\nCorrect data values in the referenced or referencing tables so that full referential integrity exists\nbetween the two tables.\nUse the rows in the error table to deﬁne which corrections to make.\nMaintain the error table.\nReferential Integrity and the FastLoad and MultiLoad Utilities\nForeign key references are not supported for any table that is the target table for a FastLoad or MultiLoad.\nRelated Information\nFor more details about ALTER TABLE and CREATE TABLE, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nViews\nA view can be compared to a window through which you can see selected portions of a database. Views retrieve portions of one or more tables or other views.\nViews and Tables\nSQL Fundamentals\nPage 23 of 99Views look like tables to a user, but they are virtual, not physical, tables. They display data in columns and rows and, in general, can be used as if they were physical tables. However,\nonly the column deﬁnitions for a view are stored: views are not physical tables.\nA view does not contain data: it is a virtual table whose deﬁnition is stored in the Data Dictionary. The view is not materialized until it is referenced by a statement. Some operations that\nare permitted for the manipulation of tables are not valid for views, and other operations are restricted, depending on the view deﬁnition.\nDefining a View\nThe CREATE VIEW statement deﬁnes a view. The statement names the view and its columns, deﬁnes a SELECT on one or more columns from one or more underlying tables and/or\nviews, and can include conditional expressions and aggregate operators to limit the row retrieval.\nUsing Views\nThe primary reason to use views is to simplify end user access to the database. Views provide a constant vantage point from which to examine and manipulate the database. Their\nperspective is altered neither by adding or nor by dropping columns from its component base tables unless those columns are part of the view deﬁnition.\nFrom an administrative perspective, views are useful for providing an easily maintained level of security and authorization. For example, users in a Human Resources department can\naccess tables containing sensitive payroll information without being able to see salary and bonus columns. Views also provide administrators with an ability to control read and update\nprivileges on the database with little effort.\nRestrictions\nSome operations that are permitted on base tables are not permitted on views—so metimes for obvious reasons and sometimes not.\nThe following set of rules outlines the restrictions on how views can be created and used.\nYou cannot create an index on a view.\nA view deﬁnition cannot contain an ORDER BY clause.\nAny derived columns in a view must explicitly specify view column names, for example by using an AS clause or by providing a column list immediately after the view name.\nYou cannot update tables from a view under the following circumstances:\nThe view is deﬁned as a join view (deﬁned on more than one table)\nThe view contains derived columns.\nThe view deﬁnition contains a DISTINCT clause.\nThe view deﬁnition contains a GROUP BY clause.\nThe view deﬁnes the same column more than once.\nTriggers\nTriggers are active database objects associated with a subject table. A trigger essentially consists of a stored SQL statement or a block of SQL statements.\nTriggers execute when an INSERT, UPDATE, DELETE, or MERGE modiﬁes a speciﬁed column or columns in the subject table.\nTypically, a stored trigger performs an UPDATE, INSERT, DELETE, MERGE, or other SQL operation on one or more tables, which may possibly include the subject table.\nTriggers in the database conform to the ANSI/ISO SQL:2008 standard, and also provide some additional features.\nTriggers have two types of granularity:\nRow triggers ﬁre once for each row of the subject table that is changed by the triggering event and that satisﬁes any qualifying condition included in the row trigger deﬁnition.\nStatement triggers ﬁre once upon the execution of the triggering statement.\nYou can create, alter, and drop triggers.\nIF you want to … THEN use …\ndeﬁne a trigger CREATE TRIGGER.\nenable a trigger\ndisable a trigger\nchange the creation timestamp for a\ntriggerALTER TRIGGER.\nDisabling a trigger stops the trigger from functioning, but leaves the trigger deﬁnition in place as an object. This allows utility\noperations on a table that are not permitted on tables with enabled triggers.\nEnabling a trigger restores its active state.\nremove a trigger from the system permanentlyDROP TRIGGER.\nRelated Information\nFor details on creating, dropping, and altering triggers, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nProcess Flow\nNote that this is a logical ﬂow, not a physical re-enactment of how Vantage processes a trigger.\n1. The triggering event occurs on the subject table.\n2. A determination is made as to whether triggers deﬁned on the subject table are to become active upon a triggering event.\n3. Qualiﬁed triggers are examined to determine the trigger action time, whether they are deﬁned to ﬁre before or after the triggering event.\n4. When multiple triggers qualify, then they ﬁre normally in the ANSI-speciﬁed order of creation timestamp.\nSQL Fundamentals\nPage 24 of 99To override the creation timestamp and specify a different execution order of triggers, you can use the ORDER clause, a Teradata extension.\nEven if triggers are created without the ORDER clause, you can redeﬁne the order of execution by changing the trigger creation timestamp using the ALTER TRIGGER\nstatement.\n5. The triggered SQL statements (triggered action) execute.\nIf the trigger deﬁnition uses a REFERENCING clause to specify that old, new, or both old and new data for the triggered action is to be collected under a correlation name (an\nalias), then that information is stored in transition tables or transition rows:\nOLD [ROW] values under old_transition_variable_name, NEW [ROW] values under new_transition_variable_name, or both.\nOLD TABLE set of rows under old_transition_table_name, NEW TABLE set of rows under new_transition_table_name, or both.\nOLD_NEW_TABLE set of rows under old_new_table_name, with old values as old_transition_variable_name and new values as new_transition_variable_name.\n6. The trigger passes control to the next trigger, if deﬁned, in a cascaded sequence. The sequence can include recursive triggers.\nOtherwise, control passes to the next statement in the application.\n7. If any of the actions involved in the triggering event or the triggered actions abort, then all of the actions are aborted.\nRestrictions\nMost Teradata load utilities cannot access a table that has an active trigger.\nAn application that uses triggers can use ALTER TRIGGER to disable the trigger and enable the load. The application must be sure that loading a table with disabled triggers does not\nresult in a mismatch in a user deﬁned relationship with a table referenced in the triggered action.\nOther restrictions on triggers include:\nBEFORE statement triggers are not allowed.\nBEFORE triggers cannot have data-changing statements as triggered action (triggered SQL statements).\nBEFORE triggers cannot access OLD TABLE, NEW TABLE, or OLD_NEW_TABLE.\nTriggers and hash indexes are mutually exclusive. You cannot deﬁne triggers on a table on which a hash index is already deﬁned.\nA positioned (updatable cursor) UPDATE or DELETE is not allowed to ﬁre a trigger. An attempt to do so generates an error.\nYou cannot deﬁne triggers on an error logging table.\nArchiving Triggers\nTriggers are archived and restored as part of a database archive and restoration. Individual triggers can be archived or restored using the ARCHIVE or RESTORE statements of\nTeradata DSA.\nRelated Information\nFor more information about:\nGuidelines for creating triggers, conditions that cause triggers to ﬁre, trigger action that occurs when a trigger ﬁres, the trigger action time, or when to use row triggers and\nwhen to use statement triggers, see CREATE TRIGGER in Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nTemporarily disabling triggers, enabling triggers, or changing the creation timestamp of a trigger, ALTER TRIGGER in Teradata Vantage™ - SQL Data Deﬁnition Language\nSyntax and Examples, B035-1144.\nPermanently removing triggers from the system, see DROP TRIGGER in Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nMacros\nA macro consists of one or more statements that can be executed by performing a single statement. Each time the macro is performed, one or more rows of data can be returned.\nA frequently used SQL statement or series of statements can be incorporated into a macro and deﬁned using the SQL CREATE MACRO statement.\nThe statements in the macro are performed using the EXECUTE statement.\nA macro can include an EXECUTE statement that executes another macro.\nPerforming a macro is similar to performing a multistatement request.\nRelated Information\nFor more information about:\nThe SQL CREATE MACRO statement, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nThe EXECUTE statement, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nSingle-User and Multiuser Macros\nYou can create a macro for your own use, or grant execution authorization to others.\nFor example, your macro might enable a user in another department to perform operations on the data in Vantage. When executing the macro, a user need not be aware of the\ndatabase being accessed, the tables affected, or even the results.\nContents of a Macro\nWith the exception of CREATE AUTHORIZATION and REPLACE AUTHORIZATION, a data deﬁnition statement is allowed in macro if it is the only SQL statement in that macro.\nA data deﬁnition statement is not resolved until the macro is executed, at which time unqualiﬁed database object references are fully resolved using the default database of the user\nsubmitting the EXECUTE statement. If this is not the desired result, you must fully qualify all object references in a data deﬁnition statement in the macro body.\nSQL Fundamentals\nPage 25 of 99A macro can contain parameters that are substituted with data values each time the macro is executed. It also can include a USING modiﬁer, which allows the parameters to be ﬁlled\nwith data from an external source such as a disk ﬁle. A COLON character preﬁxes references to a parameter name in the macro. Parameters cannot be used for data object names.\nExecuting a Macro\nRegardless of the number of statements in a macro, Vantage treats it as a single request.\nWhen you execute a macro, either all its statements are processed successfully or none are processed. If a macro fails, it is aborted, any updates are backed out, and the database is\nreturned to its original state.\nWays to Execute SQL Macros in Embedded SQL\nIF the macro … THEN use …\nis a single statement, and that statement\nreturns no datathe EXEC statement to specify static execution of the macro\n-or-\nthe PREPARE and EXECUTE statements to specify dynamic execution.\nUse DESCRIBE to verify that the single statement of the macro is not a data returning statement.\nconsists of multiple statements\nreturns dataa cursor, either static or dynamic.\nThe type of cursor used depends on the speciﬁc macro and on the needs of the application.\nStatic SQL Macro Execution in Embedded SQL\nStatic SQL macro execution is associated with a macro cursor using the macro form of the DECLARE CURSOR statement.\nWhen you perform a static macro, you must use the EXEC form to distinguish it from the dynamic SQL statement EXECUTE.\nDynamic SQL Macro Execution in Embedded SQL\nDeﬁne dynamic macro execution using the PREPARE statement with the statement string containing an EXEC macro_name statement rather than a single-statement request.\nThe dynamic request is then associated with a dynamic cursor.\nDropping, Replacing, Renaming, and Retrieving Information about a Macro\nIF you want to … THEN use the following statement …\ndrop a macro DROP MACRO\nredeﬁne an existing macro REPLACE MACRO\nrename a macro RENAME MACRO\nget the attributes for a macro HELP MACRO\nget the data deﬁnition statement most recently used to create, replace, or modify a\nmacroSHOW MACRO\nRelated Information\nFor more information about:\nDynamic SQL macro execution in embedded SQL, see the information about DECLARE CURSOR (Macro Form) in Teradata Vantage™ - SQL Stored Procedures and\nEmbedded SQL, B035-1148.\nDropping, replacing, renaming, and retrieving information about a macro, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nPerforming a macro, which is similar to performing a multistatement request, see Multistatement Requests.\nStored Procedures\nStored procedures are called Persistent Stored Modules in the ANSI/ISO SQL:2008 standard. They are written in SQL and consist of a set of control and condition handling statements\nthat make SQL a computationally complete programming language.\nThese features provide a server-based procedural interface to the database for application programmers.\nTeradata stored procedure facilities are a subset of and conform to the ANSI/ISO SQL:2008 standards for semantics.\nElements of Stored Procedures\nThe set of statements constituting the main tasks of the stored procedure is called the stored procedure body, which can consist of a single statement or a compound statement, or\nblock.\nA single statement stored procedure body can contain one control statement, such as LOOP or WHILE, or one SQL DDL, DML, or DCL statement, including dynamic SQL. Some\nstatements are not allowed, including:\nAny declaration (local variable, cursor, condition, or condition handler) statement\nSQL Fundamentals\nPage 26 of 99A cursor statement (OPEN, FETCH, or CLOSE)\nA compound statement stored procedure body consists of a BEGIN-END statement enclosing a set of declarations and statements, including:\nLocal variable declarations\nCursor declarations\nCondition declarations\nCondition handler declaration statements\nControl statements\nSQL DML, DDL, and DCL statements supported by stored procedures, including dynamic SQL\nMultistatement requests (including dynamic multistatement requests) delimited by the keywords BEGIN REQUEST and END REQUEST\nCompound statements can also be nested.\nCreating Stored Procedures\nYou can create a stored procedure using either of the following:\nSQL CREATE PROCEDURE or REPLACE PROCEDURE statement in Teradata Studio Express.\nCOMPILE command with the BTEQ utility.\nThe procedures are stored in the user database space as objects and are executed on the server.\nThe stored procedure deﬁnitions in the next examples are designed only to demonstrate the usage of the feature. They are not recommended for use.\nExample: Deﬁning a Stored Procedure for New Employees\nAssume you want to deﬁne a stored procedure NewProc to add new employees to the Employee table and retrieve the name of the department to which the employee belongs.\nYou can also report an error, in case the row that you are trying to insert already exists, and handle that error condition.\nThe CREATE PROCEDURE statement looks like this:\nCREATE PROCEDURE NewProc (IN name CHAR(12),\n                          IN number INTEGER,\n                          IN dept INTEGER,\n                          OUT dname CHAR(10))\nBEGIN\n   INSERT INTO Employee (EmpName, EmpNo, DeptNo )\n      VALUES (name, number, dept);\n   SELECT DeptName\n      INTO dname FROM Department\n         WHERE DeptNo = dept;\nEND;\nThis stored procedure deﬁnes parameters that must be ﬁlled in each time it is called.\nRelated Information\nFor information on CREATE PROCEDURE and REPLACE PROCEDURE, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nModifying Stored Procedures\nTo modify a stored procedure deﬁnition, use the REPLACE PROCEDURE statement.\nExample: Inserting Salary Information into the Employee T able\nAssume you want to change the previous example to insert salary information to the Employee table for new employees.\nThe REPLACE PROCEDURE statement looks like this:\nREPLACE PROCEDURE NewProc (IN name CHAR(12),\n                           IN number INTEGER,\n                           IN dept INTEGER,\n                           IN salary DECIMAL(10,2),\n                           OUT dname CHAR(10))\nBEGIN\n   INSERT INTO Employee (EmpName, EmpNo, DeptNo, Salary_Amount)\n      VALUES (name, number, dept, salary);\n   SELECT DeptName\n      INTO dname FROM Department\n         WHERE DeptNo = dept;\nEND;\nExecuting Stored Procedures\nSQL Fundamentals\nPage 27 of 99If you have sufﬁcient privileges, you can execute a stored procedure from any supporting client utility or interface using the SQL CALL statement. You can also execute a stored\nprocedure from an external stored procedure written in C, C++, or Java.\nYou have to specify arguments for all the parameters contained in the stored procedure.\nHere is an example of a CALL statement to execute the procedure created in Creating Stored Procedures:\nCALL NewProc ('Jonathan', 1066, 34, dname);\nOutput From Stored Procedures\nStored procedures can return output values in the INOUT or OUT arguments of the CALL statement. They can also return result sets, the results of SELECT statements that are\nexecuted when the stored procedure opens result set cursors. To return result sets, the CREATE PROCEDURE or REPLACE PROCEDURE statement for the stored procedure must\nspecify the DYNAMIC RESULT SET clause.\nRelated Information\nFor details on how to write a stored procedure that returns result sets, see Teradata Vantage™ - SQL Stored Procedures and Embedded SQL, B035-1148.\nRecompiling Stored Procedures\nThe ALTER PROCEDURE statement enables recompilation of stored procedures without having to execute SHOW PROCEDURE and REPLACE PROCEDURE statements.\nThis statement provides the following beneﬁts:\nStored procedures created in earlier database releases can be recompiled to derive the beneﬁts of new features and performance improvements.\nRecompilation is also useful for cross-platform archive and restoration of stored procedures.\nALTER PROCEDURE allows changes in the following compile-time attributes of a stored procedure: SPL option or Warnings option.\nDeleting, Renaming, and Retrieving Information about a Stored Procedure\nIF you want to … THEN use the following statement …\ndelete a stored procedure from a database DROP PROCEDURE\nrename a stored procedure RENAME PROCEDURE\nget information about the parameters speciﬁed in a stored procedure and their attributes HELP PROCEDURE\nget the data deﬁnition statement most recently used to create, replace, or modify a stored\nprocedureSHOW PROCEDURE\nArchiving Procedures\nStored procedures are archived and restored as part of a database archive and restoration. Individual stored procedures can be archived or restored using the ARCHIVE or RESTORE\nstatements of the DSA utility.\nRelated Information\nFor more information about:\nStored procedure control and condition handling statements, see Teradata Vantage™ - SQL Stored Procedures and Embedded SQL, B035-1148.\nInvoking stored procedures, see the CALL statement in Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nCreating, replacing, dropping, or renaming stored procedures, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nControlling and tracking privileges for stored procedures, see Teradata Vantage™ - SQL Data Control Language, B035-1149 or Teradata Vantage™ - SQL Stored Procedures\nand Embedded SQL, B035-1148.\nControl statements, parameters, local variables, and labels, see Teradata Vantage™ - Database Administration, B035-1093.\nUsing CALL to execute stored procedures, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nExecuting stored procedures from an external stored procedure, see Teradata Vantage™ - SQL External Routine Programming, B035-1147.\nExternal Stored Procedures\nExternal stored procedures are written in the C, C++, or Java programming language, installed on the database, and then executed like stored procedures.\nUsage\nHere is a synopsis of the steps you take to develop, compile, install, and use external stored procedures:\n1. Write, test, and debug the C, C++, or Java code for the procedure.\n2. If you are using Java, place the class or classes for the external stored procedure in an archive ﬁle (JAR or ZIP) and call the SQLJ.INSTALL_JAR external stored procedure to\nregister the archive ﬁle with the database.\n3. Use CREATE PROCEDURE or REPLACE PROCEDURE for external stored procedures to create a database object for the external stored procedure.\n4. Use GRANT to grant privileges to users who are authorized to use the external stored procedure.\n5. Invoke the procedure using the CALL statement.\nExecuting SQL From External Stored Procedures\nTo execute SQL, a C or C++ external stored procedure can use CLIv2 and a Java external stored procedure can use JDBC.\nSQL Fundamentals\nPage 28 of 99A C or C++ external stored procedure can also call the FNC_CallSP library function to call a stored procedure that contains SQL.\nDifferences Between Stored Procedures and External Stored Procedures\nUsing external stored procedures is very similar to using stored procedures, except for the following:\nInvoking an external stored procedure from a client application does not affect the nesting limit for stored procedures.\nTo install an external stored procedure on a database, you must have the CREATE EXTERNAL PROCEDURE privilege on the database.\nThe CREATE PROCEDURE statement for external stored procedures is different from the CREATE PROCEDURE statement for stored procedures. In addition to syntax\ndifferences, you do not have to use the COMPILE command in BTEQ.\nRelated Information\nFor more information about:\nExternal stored procedure programming, see Teradata Vantage™ - SQL External Routine Programming, B035-1147.\nInvoking external stored procedures, see the CALL statement in Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nInstalling external stored procedures on the server, see the CREATE/REPLACE PROCEDURE statement in Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and\nExamples, B035-1144.\nUser-Defined Functions\nSQL provides a set of useful functions, but they might not satisfy all of the particular requirements you have to process your data.\nThe database supports two types of user-deﬁned functions (UDFs) that allow you to extend SQL by writing your own functions:\nSQL UDFs\nExternal UDFs\nSQL UDFs\nSQL UDFs allow you to encapsulate regular SQL expressions in functions and then use them like standard SQL functions.\nRather than coding commonly used SQL expressions repeatedly in queries, you can objectize the SQL expressions through SQL UDFs.\nMoving complex SQL expressions from queries to SQL UDFs makes the queries more readable and can reduce the client/server network trafﬁc.\nExternal UDFs\nExternal UDFs allow you to write your own functions in the C, C++, or Java programming language, install them on the database, and then use them like standard SQL functions.\nYou can also install external UDF objects or packages from third-party vendors.\nTeradata supports three types of external UDFs.\nUDF T ype Description\nAggregate Aggregate functions produce summary results. They differ from scalar functions in that they take grouped sets of relational data, make a pass over each\ngroup, and return one result for the group. Some examples of standard SQL aggregate functions are AVG, SUM, MAX, and MIN.\nScalar Scalar functions take input parameters and return a single value result. Examples of standard SQL scalar functions are CHARACTER_LENGTH, POSITION, and\nTRIM.\nTable A table function is invoked in the FROM clause of a SELECT statement and returns a table to the statement.\nUsage\nTo create and use an SQL UDF, follow these steps:\n1. Use CREATE FUNCTION or REPLACE FUNCTION to deﬁne the UDF.\n2. Use GRANT to grant privileges to users who are authorized to use the UDF.\n3. Call the function.\nHere is a synopsis of the steps you take to develop, compile, install, and use an external UDF:\n1. Write, test, and debug the C, C++, or Javacode for the UDF.\n2. If you are using Java, place the class or classes for the UDF in an archive ﬁle (JAR or ZIP) and call the SQLJ.INSTALL_JAR external stored procedure to register the archive ﬁle\nwith the database.\n3. Use CREATE FUNCTION or REPLACE FUNCTION to create a database object for the UDF.\n4. Use GRANT to grant privileges to users who are authorized to use the UDF.\n5. Call the function.\nRelated Information\nFor more information about:\nWriting, testing, and debugging source code for an external UDF, see Teradata Vantage™ - SQL External Routine Programming, B035-1147.\nData deﬁnition statements related to UDFs, including CREATE FUNCTION and REPLACE FUNCTION, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and\nExamples, B035-1144.\nInvoking a table function in the FROM clause of a SELECT statement, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nSQL Fundamentals\nPage 29 of 99Profiles\nProﬁles deﬁne values for the following system parameters:\nDefault database\nSpool space\nTemporary space\nDefault account and alternate accounts\nPassword security attributes\nOptimizer cost proﬁle\nOptimizer cost proﬁles are not intended for use on production systems. The Cost Proﬁle parameter is for use only under the direction of Teradata Support Center personnel.\nQuery band\nAn administrator can deﬁne a proﬁle and assign it to a group of users who share the same settings.\nAdvantages of Using Profiles\nSimplify system administration.\nAdministrators can create a proﬁle that contains system parameters\nand assign the proﬁle to a group of users. To change a parameter, the administrator updates the proﬁle instead of each individual user.\nControl password security.\nA proﬁle can deﬁne password attributes such as the number of:\nDays before a password expires\nDays before a password can be used again\nMinutes to lock out a user after a certain number of failed logon attempts\nAdministrators can assign the proﬁle to an individual user or to a group of users.\nUsage\nThe following steps describe how to use proﬁles to manage a common set of parameters for a group of users.\n1. Deﬁne a user proﬁle.\nA CREATE PROFILE statement deﬁnes a proﬁle, and lets you set:\nAccount identiﬁers to charge for the space used and a default account identiﬁer\nDefault database\nSpace to allocate for spool ﬁles and temporary tables\nOptimizer cost proﬁle to activate at the session and request scope level\nOptimizer cost proﬁles are not intended for use on production systems. The Cost Proﬁle parameter is for use only under the direction of Teradata Support Center\npersonnel.\nNumber of incorrect logon attempts to allow before locking a user and the number of minutes before unlocking a locked user\nA query band to enforce query band name/value pairs\nNumber of days before a password expires and the number of days before a passwo rd can be used again\nMinimum and maximum number of characters in a passwo rd string\nThe characters allowed in a password string, including whether to:\nAllow digits and special characters\nRequire at least one numeric character\nRequire at least one alpha character\nRequire at least one special character\nRestrict the password string from containing the user name\nRequire a mixture of uppercase and lowercase characters\nRestrict certain words from being a signiﬁcant part of a passwo rd string\n2. Assign the proﬁle to users.\nUse the CREATE USER or MODIFY USER statement to assign a proﬁle to a user. Proﬁle settings override the values set for the user.\n3. If necessary, change any of the system parameters for a proﬁle.\nUse the MODIFY PROFILE statement to change a proﬁle.\nRelated Information\nFor more information about:\nThe syntax and usage of proﬁles, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nPasswords and security, see Teradata Vantage™ - Analytics Database Security Administration, B035-1100.\nOptimizer cost proﬁles, see Teradata Vantage™ - SQL Request and Transaction Processing, B035-1142.\nRoles\nRoles deﬁne privileges on database objects. A user who is assigned a role can access all the objects that the role has privileges to.\nSQL Fundamentals\nPage 30 of 99Roles simplify management of user privileges. A database administrator can create different roles for different job functions and responsibilities, grant speciﬁc privileges on database\nobjects to the roles, and then grant membership to the roles to users.\nAdvantages of Using Roles\nSimplify privilege administration.\nA database administrator can grant privileges on database objects to a role and have the privileges automatically applied to all users assigned to that role.\nWhen a user’s function within an organization changes, changing the user’s role is far easier than deleting old privileges and granting new privileges to go along with the new\nfunction.\nReduce Data Dictionary disk space.\nMaintaining privileges on a role level rather than on an individual level makes the size of the DBC.AccessRights table much smaller. Instead of inserting one row per user per\nprivilege on a database object, Vantage inserts one row per role per privilege in DBC.AccessRights, and one row per role member in DBC.RoleGrants.\nUsage\nThe following steps describe how to manage user privileges using roles.\n1. Deﬁne a role.\nA CREATE ROLE statement deﬁnes a role. A newly created role does not have any associated privileges.\n2. Add privileges to the role.\nUse the GRANT statement to grant privileges to roles on databases, tables, views, macros, columns, triggers, stored procedures, join indexes, hash indexes, and UDFs.\n3. Grant the role to users or other roles.\nUse the GRANT statement to grant a role to users or other roles.\n4. Assign default roles to users.\nUse the DEFAULT ROLE option of the CREATE USER or MODIFY USER statement to specify the default role for a user, where:\nDEFAULT ROLE = … Speciﬁes …\nrole_name the name of one role to assign as the default role for a user.\nNONE that the user does not have a default role.\nNULL\nALL the default role to be all roles that are directly or indirectly granted to the user.\nAt logon time, the default role of the user becomes the current role for the session.\nPrivilege validation uses the active roles for a user, which include the current role and all nested roles.\n5. If necessary, change the current role for a session.\nUse the SET ROLE statement to change the current role for a session.\nManaging role-based privileges requires sufﬁcient privileges. For example, the CREATE ROLE statement is only authorized to users who have the CREATE ROLE system privilege.\nRelated Information\nFor information on the syntax and usage of roles, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nUser-Defined Types\nSQL provides a set of predeﬁned data types, such as INTEGER and VARCHAR, that you can use to store the data that your application uses, but they might not satisfy all of the\nrequirements you have to model your data.\nUser-deﬁned types (UDTs) allow you to extend SQL by creating your own data types and then using them like predeﬁned data types.\nUDT Types\nVantage supports distinct and structured UDTs.\nUDT T ype Description Example\nDistinct A UDT that is based on a single predeﬁned data type, such as INTEGER or\nVARCHAR.A distinct UDT named euro that is based on a DECIMAL(8,2) data type can\nstore monetary data.\nStructured A UDT that is a collection of one or more ﬁelds called attributes, each of which\nis deﬁned as a predeﬁned data type or other UDT (which allows nesting).A structured UDT named circle can consist of x-coordinate, y-coordinate,\nand radius attributes.\nDistinct and structured UDTs can deﬁne methods that operate on the UDT. For example, a distinct UDT named euro can deﬁne a method that converts the value to a US dollar amount.\nSimilarly, a structured UDT named circle can deﬁne a method that computes the area of the circle using the radius attribute.\nSQL Fundamentals\nPage 31 of 99Vantage also supports a form of structured UDT called dynamic UDT. Instead of using a CREATE TYPE statement to deﬁne the UDT, like you use to deﬁne a distinct or structured type,\nyou use the NEW VARIANT_TYPE expression to construct an instance of a dynamic UDT and deﬁne the attributes of the UDT at run time.\nUnlike distinct and structured UDTs, which can appear almost anywhere that you can specify predeﬁned types, you can only specify a dynamic UDT as the data type of (up to eight)\ninput parameters to external UDFs. The beneﬁt of dynamic UDTs is that they signiﬁcantly increase the number of input arguments that you can pass in to external UDFs.\nUsing a Distinct UDT\nHere is a synopsis of the steps you take to develop and use a distinct UDT:\n1. Use the CREATE TYPE statement to create a distinct UDT that is based on a predeﬁned data type, such as INTEGER or VARCHAR.\nVantage automatically generates functionality for the UDT that allows you to import and export the UDT between the client and server, use the UDT in a table, perform\ncomparison operations between two UDTs, and perform data type conversions between the UDT and the predeﬁned data type on which the deﬁnition is based.\n2. If the UDT deﬁnes methods, write, test, and debug the C or C++ code for the methods, and then use CREATE METHOD or REPLACE METHOD to identify the location of the\nsource code and install it on the server.\nThe methods are compiled, linked to the dynamic linked library (DLL or SO) associated with the SYSUDTLIB database, and distributed to all database nodes in the system.\n3. Use GRANT to grant privileges to users who are authorized to use the UDT.\n4. Use the UDT as the data type of a column in a table deﬁnition.\nUsing a Structured UDT\nHere is a synopsis of the steps you take to develop and use a structured UDT (that is not a dynamic UDT):\n1. Use the CREATE TYPE statement to create a structured UDT and specify attributes, constructor methods, and instance methods.\nVantage automatically generates the following functionality:\nA default constructor function that you can use to construct a new instance of the structured UDT and initialize the attributes to NULL\nObserver methods for each attribute that you can use to get the attribute values\nMutator methods for each attribute that you can use to set the attribute values\n2. Follow these steps to implement, install, and register cast functionality for the UDT (Vantage does not automatically generate cast functionality for structured UDTs):\na. Write, test, and debug C or C++ code that implements cast functionality that allows you to perform data type conversions between the UDT and other data types,\nincluding other UDTs.\nb. Identify the location of the source code and install it on the server:\nIF you write the source code as a … THEN use one of the following statements …\nmethod CREATE METHOD or REPLACE METHOD\nfunction CREATE FUNCTION or REPLACE FUNCTION\nThe source code is compiled, linked to the dynamic linked library (DLL or SO) associated with the SYSUDTLIB database, and distributed to all database nodes in the\nsystem.\nc. Use the CREATE CAST or REPLACE CAST statement to register the method or function as a cast routine for the UDT.\nd. Repeat Steps a through c for all methods or functions that provide cast functionality.\n3. Follow these steps to implement, install, and register ordering functionality for the UDT (Vantage does not automatically generate ordering functionality for structured UDTs):\na. Write, test, and debug C or C++ code that implements ordering functionality that allows you to perform comparison operations between two UDTs.\nb. Identify the location of the source code and install it on the server:\nIF you write the source code as a … THEN use one of the following statements …\nmethod CREATE METHOD or REPLACE METHOD\nfunction CREATE FUNCTION or REPLACE FUNCTION\nThe source code is compiled, linked to the dynamic linked library (DLL or SO) associated with the SYSUDTLIB database, and distributed to all database nodes in the\nsystem.\nc. Use the CREATE ORDERING or REPLACE ORDERING statement to register the method or function as an ordering routine for the UDT.\n4. Follow these steps to implement, install, and register transform functionality for the UDT (Vantage does not automatically generate transform functionality for structured UDTs):\na. Write, test, and debug C or C++ code that implements transform functionality that allows you to import and export the UDT between the client and server.\nb. Identify the location of the source code and install it on the server:\nIF the source code implements transform functionality for … THEN …\nimporting the UDT to the server you must write the source code as a C or C++ UDF and use CREATE FUNCTION or\nREPLACE FUNCTION to identify the location of the source code and install it on the\nserver.\nexporting the UDT to the server\nIF you write the source code as a …THEN use one of the following statements\nto identify the location of the source code\nand install it on the server …\nmethod CREATE METHOD or REPLACE METHOD\nfunction CREATE FUNCTION or REPLACE\nFUNCTION\nSQL Fundamentals\nPage 32 of 99The source code is compiled, linked to the dynamic linked library (DLL or SO) associated with the SYSUDTLIB database, and distributed to all database nodes in the\nsystem.\nc. Repeat Steps a through b.\nIF you took Steps a through b to implement and install this transform\nfunctionality …THEN repeat Steps a through b to implement and install this transform\nfunctionality …\nimporting the UDT to the server exporting the UDT from the server\nexporting the UDT from the server importing the UDT to the server\nd. Use the CREATE TRANSFORM or REPLACE TRANSFORM statement to register the transform routines for the UDT.\n5. If the UDT deﬁnes constructor methods or instance methods, write, test, and debug the C or C++ code for the methods, and then use CREATE METHOD or REPLACE\nMETHOD to identify the location of the source code and install it on the server.\nThe methods are compiled, linked to the dynamic linked library (DLL or SO) associated with the SYSUDTLIB database, and distributed to all database nodes in the system.\n6. Use GRANT to grant privileges to users who are authorized to use the UDT.\n7. Use the UDT as the data type of a column in a table deﬁnition.\nUsing a Dynamic UDT\nFollow these steps to use a dynamic UDT as the data type of an input parameter to an external UDF:\n1. In the CREATE FUNCTION or REPLACE FUNCTION statement for the UDF, specify the data type of up to eight input parameters as VARIANT_TYPE.\n2. Write, test, and debug the C or C++ source code for the UDF.\n3. Call the UDF, using the NEW VARIANT_TYPE expression to construct instances of dynamic UDT arguments and deﬁne up to 128 attributes for each UDT.\nUDT Indexing\nA user can declare a primary or secondary index on a UDT column when issuing a CREATE TABLE, CREATE INDEX, CREATE JOIN INDEX or CREATE HASH INDEX statement.\nOnce the index has been created, the user can issue DML statements containing UDT predicate and/or UDT join term expressions.\nRelated Information\nFor more information about:\nCREATE TYPE, CREATE METHOD and REPLACE METHOD, CREATE FUNCTION and REPLACE FUNCTION, CREATE CAST and REPLACE CAST, CREATE ORDERING and\nREPLACE ORDERING, CREATE TRANSFORM and REPLACE, and TRANSFORM, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nUDT expressions, including NEW and NEW VARIANT_TYPE, see Teradata Vantage™ - SQL Operators and User-Deﬁned Functions, B035-1210.\nWriting, testing, and debugging source code for a constructor method or instance method, see Teradata Vantage™ - SQL External Routine Programming, B035-1147.\nWriting, testing, and debugging source code for a UDF that uses UDT types, see Teradata Vantage™ - SQL External Routine Programming, B035-1147\nBasic SQL Syntax\nThis section explains basic SQL syntax and the lexicon for Teradata SQL, a single, uniﬁed, nonprocedural language that provides the capabilities for querying the database.\nSQL Statement Structure\nSyntax\nThe following diagram indicates the basic structure of an SQL statement.\nstatement_keyword\n  { expressions  |\n    functions  |\n    keywords  |\n    clauses |\n    phrases\n  } [[,]...] [;]\nstatement_keyword\nThe name of the statement.\nexpressions\nLiterals, name references, or operations using names and literals.\nfunctions\nThe name of a function and its arguments, if any.\nkeywords\nSpecial values introducing clauses or phrases or representing special objects, such as NULL. Most keywords are reserved words and cannot be used in names.\nclauses\nSubordinate statement qualiﬁers.\nphrases\nData attribute phrases.\n;\nThe Teradata SQL statement separator and request terminator.\nThe semicolon separates statements in a multistatement request and terminates a request when it is the last nonblank character on an input line in BTEQ.\nSQL Fundamentals\nPage 33 of 99The request terminator is required for a request deﬁned in the body of a macro.\nTypical SQL  Statement\nA typical SQL statement consists of a statement keyword, one or more column names, a database name, a table name, and one or more optional clauses introduced by keywords. For\nexample, in the following single-statement request, the statement keyword is SELECT:\nSELECT deptno, name, salary\nFROM personnel.employee\nWHERE deptno IN(100, 500)\nORDER BY deptno, name ;\nThe select list for this statement is made up of object names:\nDeptno, name, and salary (the column names)\nPersonnel (the database name)\nEmployee (the table name)\nThe search condition, or WHERE clause, is introduced by the keyword WHERE.\nWHERE deptno IN(100, 500)\nThe sort order, or ORDER BY, clause is introduced by the keywords ORDER BY.\nORDER BY deptno, name\nRelated Information\nFor more information about the elements that appear in an SQL statement:\nStatement_keyword, see Keywords.\nKeywords, see Keywords.\nObject names, see Object Names.\nExpressions, see Expressions.\nFunctions, see Functions.\nSeparators, see Separators.\nTerminators, see Terminators.\nFor the distinction between statement and request, see SQL Statements and SQL Requests.\nKeywords\nKeywords are words that have special meanings in SQL statements.\nStatement Keyword\nThe statement keyword, the ﬁrst keyword in an SQL statement, is usually a verb. For example, in the INSERT statement, the ﬁrst keyword is INSERT.\nOther Keywords\nOther keywords appear throughout a statement as qualiﬁers (for example, DISTINCT, PERMANENT), or as words that introduce clauses (for example, IN, AS, AND, TO, WHERE).\nBy convention, keywords appear entirely in uppercase letters. However, SQL keywords are not case-sensitive and can be in uppercase or lowercase letters.\nFor example, SQL interprets the following SELECT statements identically:\nSelect Salary from Employee where EmpNo = 10005;\nSELECT Salary FROM Employee WHERE EmpNo = 10005;\nselect Salary FRom Employee WherE EmpNo = 10005;\nAll keywords must be from the ASCII repertoire. Full-width letters are not valid regardless of the character set being used.\nRelated Information\nFor information on restricted keywords, see Restricted Words.\nCharacter Sets\nWhen making a request to the database, an SQL statement can be represented using any client character set that is available on the client and enabled in the database, including\nuser-deﬁned character sets.\nCharacter Data\nThe system converts the SQL into UNICODE, and then converts any character literals from UNICODE to the data type of any column into which they are inserted.\nSQL Fundamentals\nPage 34 of 99If you enter data by way of a USING clause, the system stores the data based on the session character set, either LATIN, UNICODE, or KANJI1. As with any character data, if inserted\ninto a character column, the USING data is implicitly translated to the character set of the receiving column.\nIf you create a character column, for example in a CREATE TABLE statement, and no character set is explicitly designated for the column, the column is created using the default\ncharacter set.\nSetting the Default Server Character Set\nYou can set the default server character set for:\nA user in a CREATE or MODIFY USER\nProﬁle members in a CREATE or MODIFY PROFILE statement\nUpon upgrade to Teradata Database 14.10 and above from a pre-14.10 release, the server default character set is initialized based on the language support mode; LATIN for standard\nmode or UNICODE for Japanese mode.\nRelated Information\nFor more information about:\nAvailable character sets and enabling character set options, see International Character Set Support, B035-1125.\nNotation used for characters is described in Notation Conventions.\nObject Names\nDatabase objects (for example, databases, tables, and columns) that you specify in an SQL statement must conform to system object naming rules or the SQL statement is not valid.\nPass Through Characters may not be used in object names.\nGeneral Rules for Constructing Object Names\nObject names must be unique within the scope of the object name.\nWithin the database system:\nNo two Database names can have the same name.\nCreating a user always creates a database with the same name as the user, so a user can not be named the same as an existing database.\nNo two proﬁles can share a name.\nNo two roles can share a name.\nA Proﬁle can have the same name as a Role, but no Role or Proﬁle can have the same name as a Database\nWithin a named Database:\nTables, views, stored procedures, join or hash indexes, triggers, user-deﬁned functions, and macros must have unique names\nWithin a Table or View:\nNo two columns can have the same name.\nNo two named constraints can have the same name.\nNo two secondary indexes can have the same name.\nWithin a macro or stored procedure:\nNo two parameters can have the same name.\nNames are optional for CHECK constraints, REFERENCE constraints, and INDEX objects.\nObject names are also subject to the name validation rules enabled on the system.\nUsing QUOT ATION MARKS Characters with Object Names\nEnclosing an object name in QUOTATION MARKS characters (U+0022) enables the use of characters, spaces, symbols, and other special characters that may not otherwise be\nallowed.\nFor example, an object name containing a white space character must be enclosed in quotation marks:\n\"object name\"\nWhen used as part of an object name, rather than when used to delineate an object name, quotation marks must be represented as a sequence of two QUOTATION MARKS\ncharacters (U+0022). Each pair of quotation marks is counted as one character when calculating the name size limit.\nQUOTATION MARKS characters that delineate object names are not stored in Dictionary tables, and when querying a Dictionary view, results that contain such names are displayed\nwithout the double quotation marks. However, if you include the name in a subsequent database request, you must add the double quotation marks, or the request fails.\nCase Sensitivity of Object Names\nSQL Fundamentals\nPage 35 of 99Object names are not case-dependent. Any mix of uppercase and lowercase can be used when deﬁning or referencing object names in a request. Because of this, you cannot reuse a\nname that is required to be unique just by changing its case.\nFor example, the following statements are identical:\nSELECT Salary FROM Employee WHERE EmpNo = 10005;\nSELECT SALARY FROM EMPLOYEE WHERE EMPNO = 10005;\nSELECT salary FROM employee WHERE eMpNo = 10005;\nThe case of column names is signiﬁcant. The column name is the default title of an output column, so the system returns these names in the same case in which the names were\ndeﬁned.\nFor example, assume that the columns in the SalesReps table are deﬁned:\nCREATE TABLE SalesReps\n ( last_name VARCHAR(20) NOT NULL,\n   first_name VARCHAR(12) NOT NULL, ...\nIn response to a query that does not deﬁne a TITLE phrase, for example:\nSELECT Last_Name, First_Name\nFROM SalesReps\nORDER BY Last_Name;\nthe column names are returned exactly as deﬁned, that is, last_name, then ﬁrst_name.\nYou can use the TITLE phrase to specify the case, wording, and placement of an output column heading either in the column deﬁnition or in an SQL request.\nRestricted W ords in Object Names\nBe careful not to use restricted words in object names. Restricted words include both reserved and nonreserved words.\nIf you attempt to create a new object with an unquoted name that includes a reserved word, the request fails with an error.\nThe system does not reject object names that use nonreserved words, but in some contexts, the system can interpret such words using the nonreserved word semantics rather\nthan that of an object name.\nNew database releases can include new reserved and nonreserved words. Names that conﬂict with these new reserved words should be identiﬁed and renamed as part of preparation\nfor a Vantage software upgrade. Instead you can enclose names containing reserved or nonreserved words in quotation marks, but changing the words is preferred to avoid possible\ndeveloper errors.\nObject Name Processing and Storage\nObject names are stored in the Data Dictionary tables using the UNICODE server character set, and are processed internally as UNICODE strings. For backwards compatibility, object\nnames are available in HELP output and in compatibility views translated to LATIN or KANJI1 based on the Language Support Mode (Standard or Japanese).\nComparison and Normalization of Object Names\nIn comparing two names, the following rules apply:\nNames are case insensitive.\nObject names are considered identical when converted to NFC or NFD and compared as case insensitive. A fullwidth LATIN SMALL LETTER A (U+FF41) is not the same as a\nLATIN SMALL LETTER A (U+0061), but case pairs are equivalent.\nThe system converts all database object names to UNICODE for storage in the Data Dictionary. Names are normalized to UNICODE Normalization Form C (NFC) after\nconversion to upper case.\nTeradata supports all UNICODE normalization forms; NFC, NFD, NFCK, and NFDK. For information on use of the TRANSLATE function to specify alternate UNICODE normalization\nforms, for example, UNICODE_TO_UNICODE_NFD, see Teradata Vantage™ - SQL Functions, Expressions, and Predicates, B035-1145.\nUNICODE and Object Names\nAll object names are stored in the Data Dictionary in UNICODE. Teradata provides a ﬁle, UOBJNEXT.txt, which lists the UNICODE representation of all characters that are valid in an\nobject name. You can download the ﬁle here:\n1. Access Teradata Vantage™ - SQL Fundamentals, B035-1141 at https://docs.teradata.com/.\n2. In the left pane, select Attachments to download the Object_Name_Characters zip ﬁle.\nSome characters in this list can become unusable if the DBSControl NameValidationRule ﬁeld is conﬁgured to enforce a reduced character set.\nObject Name V alidation Options\nThe DBS Control ﬁeld NameValidationRule is used to deﬁne object name restrictions.\nValues of 2 and above place additional limits on the character repertoire allowed.\nThe NameValidationRule is enforced by the system at object creation. Characters outside the repertoire deﬁned by the rule are rejected by the system if the characters appear when\ncreating object names.\nSQL Fundamentals\nPage 36 of 99Characters Allowed in Object Names\nThe following table summarizes the use of characters in object names.\nParameter Description\nObject name length A maximum of 128 characters when expressed in UNICODE normalization form D.\nNameValidationRule can be used to apply additional character restrictions.\nCharacters allowed in unquoted object names\nAdditional characters may be allowed in\nquoted or Unicode delimited names.An object name not enclosed in quotation marks must be composed of an identiﬁer-start character followed by a sequence of\nidentiﬁer-start or identiﬁer extend characters, up to the maximum object name length limit.\nCharacters in object names not enclosed in quotation marks must also be in the session character set.\nIdentiﬁer start characters must be contained in the session character set and belong to one of the following Unicode General\nCategory classes:\nUpper-case letters [Lu]\nLower-case letters [Ll]\nTitle-case letters [Lt]\nModiﬁer letters [Lm]\nOther letters ([Lo]\nLetter numbers [Nl]\nOr be one of the following characters:\nNUMBER SIGN (U+0023)\nDOLLAR SIGN (U+0024)\nLOW LINE (U+005F)\nINVERTED EXCLAMATION MARK (U+00A1)\nOVERLINE (U+203E)\nEURO SIGN (U+20AC)\nKATAKANA-HIRAGANA VOICED SOUND MARK (U+309B)\nKATAKANA-HIRAGANA SEMI-VOICED SOUND MARK (U+309B)\nFULLWIDTH NUMBER SIGN (U+FF03)\nFULLWIDTH DOLLAR SIGN (U+FF04)\nFULLWIDTH LOW LINE (U+FF3F)\nIdentiﬁer-extender characters must be in the session character set and belong to one of the following Unicode General Category\nclasses:\nNon-spacing marks [Mn]\nSpacing combing marks [Mc]\nDecimal numbers [Nd]\nConnector punctuations [Pc]\nFormatting codes [Cf]\nThe MIDDLE DOT character (U+00B7) is also a valid identiﬁer-extender character.\nCharacters allowed only in object names that\nare enclosed in quotation marksA string literal is required for object names that:\nHave an identiﬁer-extender character as the ﬁrst character.\nInclude the white space character, SPACE (U+0020)\nAre Teradata keywords\nIn addition, object names that contain any character from the following classes must be enclosed in quotation marks, unless the\ncharacter explicitly appears in the list of allowed characters:\nOther, Control [Cc]\nOther, Not Assigned [Cn]\nNo characters in this category appear in UNICODE character repertoire.\nOther, Private Use [Co]\nOther, Surrogate [Cs]\nLetter, Cased [LC]\nMark, Enclosing [Me]\nNumber, Other [No]\nPunctuation, Dash [Pd]\nPunctuation, Close [Pe]\nPunctuation, Final quotation marks [Pf] (may behave like Ps or Pe depending on usage)\nPunctuation, Initial quotation marks [Pi] (may behave like Ps or Pe depending on usage)\nPunctuation, Other [Po]\nPunctuation, Open [Ps]\nSymbol, Currency [Sc]\nSymbol, Modiﬁer [Sk]\nSymbol, Math [Sm]\nSQL Fundamentals\nPage 37 of 99Parameter Description\nSymbol, Other [So]\nSeparator, Line [Zl]\nSeparator, Paragraph [Zp]\nSeparator, Space [Zs]\nWhen used as part of an object name, quotation marks must be represented as a sequence of two QUOTATION MARKS characters\n(U+0022). Each set of two quotation marks is counted as one character when calculating the name size limit.\nDisallowed characters The following characters cannot appear in an object name:\nNULL (U+0000)\nSUBSTITUTE character (U+001A)\nREPLACEMENT CHARACTER (U+FFFD)\nCompatibility ideographs (U+FA6C, U+FA6F, U+FAD0, FAD1, FAD5, FAD6, and FAD7)\nThe setting of the NameValidationRule ﬁeld may deﬁne additional character restrictions.\nOther considerations These additional restrictions apply:\nAn object name consisting entirely of white spaces is not allowed.\nA trailing white space is not considered part of an object name\nYou can use the NameValidationRule ﬁeld to restrict object name allowable characters to a subset of those normally allowed.\nRelated Information\nFor more information about:\nFor a list of what the system considers to be an object name for the purpose of name validation, see System Validated Object Names.\nUsing the TITLE phrase, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144 and Teradata Vantage™ - SQL Data Manipulation\nLanguage, B035-1146.\nUPPER Function or TRANSLATE and TRANSLATE_CHK functions, see Teradata Vantage™ - SQL Functions, Expressions, and Predicates, B035-1145.\nCase Sensitivity of Object Names, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nFor a list of reserved and nonreserved restricted words, see Restricted Words.\nWorking with Unicode Delimited Identifiers\nThe system allows the use of UNICODE delimited identiﬁers to enable speciﬁcation of object names and literals having characters that are not compatible with the session character\nset.\nSystem Use of Unicode Delimited Identiﬁers\nTeradata uses Unicode delimited identiﬁers for:\nReturning certain information as the result of running a console utility or executing a UDT, UDF, or stored procedure that contains an SQL TEXT element.\nReturning results of an EXPLAIN, HELP or SHOW request for names that include characters not in the repertoire of the session character set, or that are otherwise unprintable.\nFor example, the table name: \nIn an ASCII session, where the table name does not translate into the session character set, the table portion of the HELP output appears similar to the following:\nTable Name: ^Z^Z^Z^Z\nTable Dictionary Name: ^Z^Z^Z^Z\nTable SQL Name: U&\"\\FF83\\FF70\\FF8C\\FF9E\\FF99\"\nTable UEscape: \\ \nwhere:\nOutput Description\nTable Dictionary Name: ^Z^Z ^Z^Z\nEach ^Z represents an ASCII replacement character, 0x1A, used to replace the 4 untranslatable characters, \n .\nThe system uses ^ to represent unprintable control characters.\nTable SQL Name: U&\"\\FF83\\FF70\\FF8C \n\\FF9E\\FF99\"The SQL Name begins with U& to indicate that it is a UNICODE delimited identiﬁer, that is, it contains untranslatable characters.\nThe sequence \\FF83\\FF70\\FF8C\\FF9E\\FF99 is the set of UNICODE identiﬁers for the 4 untranslatable characters, preceded by the\ndefault delimiter character, in this case, \\.\nThe string is enclosed in quotation marks so that the delimiter characters it contains can be used in an SQL request.\nThe system does not return the UEscape clause, which normally closes a UNICODE delimited identiﬁer. If you want to use a\nUNICODE delimited identiﬁer in an SQL request, you need to add the UEscape clause.\nTable UEscape: \\ Indicates the delimiter character used to separate the UNICODE identiﬁers in the SQL Name.\nUsing an SQL  Name or SQL  Title V alue in an SQL  Request\nSQL Fundamentals\nPage 38 of 99You can use the SQL Name or SQL Title of an object returned as the result of a HELP request in an SQL request.\nFor example, you can use HELP DATABASE to look in the mydb database for the name of a table you want to drop:\nHELP DATABASE mydb;\n… Table SQL Name U&\"table_\\4E00_name\"\n   Table UEscape  \\ \nThe SQL Name returned by the HELP DATABASE statement is a UNICODE delimited identiﬁer, which includes an untranslatable character that the system expresses as \\4E00.\nIf you want to use a UNICODE delimited identiﬁer from the SQL Name as part of an SQL request, you must:\n1. Add the closing UEscape clause to the table name taken from the SQL Name ﬁeld.\n2. Specify the delimiter character ( \\ )\nFor example:\nDrop Table U&\"table_\\4E00_name\" UEscape '\\';\nIf the SQL Name is not a UNICODE delimited identiﬁer, you can use the name in an SQL request as it appears in the HELP output, without specifying the UEscape phrase and delimiter.\nDetermining the Delimiter Character\nYou can use one of the following delimiter characters in SQL Name and SQL title, depending on availability in the session character set:\nBACKSLASH (U+005C)\nTILDE (U+007E)\nThe YEN SIGN (U+00A5) or WON SIGN (U+20A9), depending on which is present in the session character set at 0x5C.\nNUMBER SIGN (U+0023), which is present in all supported session character sets.\nThe UEscape ﬁeld that follows each SQL Name ﬁeld must be used to identify the delimiter used.\nHexadecimal Representation of Object Names\nAll object names are stored in the Data Dictionary in UNICODE. However, in an international environment, some client users may ﬁnd that their client (session) character set cannot\nexpress an object name created on another client. In these cases, you can use a hexadecimal representation of the unavailable UNICODE characters in the object name to more easily\naccess the object.\nUsing Delimiters with Hexadecimal Identiﬁers\nWhen you reference a database object using a hexadecimal representation of the object name, you must express it as a name literal or enclose the name in UNICODE delimiters.\nThe best practice is to use UNICODE delimited identiﬁers because the hexadecimal value remains the same across all supported character sets, whereas using hexadecimal name\nliterals allows the name representation to vary depending on the character set.\nExample: Using UNICODE Delimited Identiﬁers\nConsider a table name of \n  where the translation to the UNICODE server character set is equivalent to the following Unicode delimited identiﬁer:\nU&\"#FF83#FF70#FF8C#FF9E#FF99\" UESCAPE '#'\nFrom a client where the client character set is KANJIEUC_0U, you can access the table using the following hexadecimal name literal:\n'80C380B080CC80DE80D9'XN\nFrom a client where the client character set is KANJISJIS_0S, you can access the table using the following hexadecimal name literal:\n'C3B0CCDED9'XN\nExample: Using Hexadecimal Name Literals\nAssume that a hexadecimal name literal that represents the name TAB1 using full width Latin characters from the KANJIEBCDIC5035_0I character set on a system enabled for\nJapanese language support:\nSELECT EmployeeID FROM \"0E42E342C142C242F10F\"XN;\nThe database converts the hexadecimal name literal from a KANJI1 string to a UNICODE string to ﬁnd the object name in the Data Dictionary.\nHere is an example of a Unicode delimited identiﬁer that represents the name TAB1 on a system enabled for Japanese language support:\nSELECT EmployeeID FROM U&\"#FF34#FF21#FF22#FF11\" UESCAPE '#';\nHexadecimal Name Literals\nHexadecimal name literals provide a means to create and reference object names by their internal UNICODE representation in the Data Dictionary, for example, because characters\nare not representable in the session character set.\nSQL Fundamentals\nPage 39 of 99Support for hexadecimal name literals may be discontinued in a future database release. Use Unicode delimited identiﬁers when you need characters not representable in the session\ncharacter set.\nANSI Compliance\nHexadecimal name literals are Teradata extensions to the ANSI/ISO SQL:2008 standard.\nMaximum Length\nA hexadecimal name literal can consist of a maximum of 60 hexadecimal digits.\nUsage\nA hexadecimal name literal is useful for specifying an object name containing characters that cannot generally be entered directly from a keyboard.\nObject names are stored and processed using the UNICODE server character set. Vantage converts a hexadecimal name literal to a UNICODE string to ﬁnd the object name in the\nData Dictionary.\nWith object naming, the following text ﬁles list the characters from the UNICODE server character set that are valid in object names and can appear in hexadecimal name literals. You\ncan download the ﬁles here:\n1. Access Teradata Vantage™ - SQL Fundamentals, B035-1141 at https://docs.teradata.com/.\n2. In the left pane, select Attachments to download the Object_Name_Characters zip ﬁle.\nFile Name Title\nUOBJNSTD.txt UNICODE in Object Names on Standard Language Support Systems\nUOBJNJAP.txt UNICODE in Object Names on Japanese Language Support Systems\nRestrictions\nThe minimal restrictions for object names are that the object name must not consist entirely of white space characters, and the following characters are not allowed in the name:\nNULL (U+0000)\nSUBSTITUTE character (U+001A)\nREPLACEMENT CHARACTER (U+FFFD)\nThe compatibility ideographs U+FA6C, U+FACF, U+FAD0, U+FAD1, U+FAD5, U+FAD6, U+FAD7\nExample: Querying the Session Character Set\nConsider a table with the name TAB1 (in full-width Latin characters) on a system enabled with Japanese language support.\nUse this query when the session character set is KANJIEBCDIC5035_0I to return all columns from the table:\nSELECT * FROM '0E42E342C142C242F10F'XN;\nUse this query when the session character set is KANJIEUC_0U to return all columns:\nSELECT * FROM '8273826082618250'XN;\nFinding the Internal Hexadecimal Representation for an Object Name\nYou can use the CHAR2HEXINT function to ﬁnd the internal hexadecimal representation for any object name. For example, the following table shows how to ﬁnd a database name.\nIF you want to ﬁnd the internal representation of … THEN use the CHAR2HEXINT function on the …\na database name that you can use to form a hexadecimal name literal DatabaseName column of the DBC.Databases view.\na database name that you can use to form a Unicode delimited identiﬁer DatabaseName column of the DBC.DatabasesV view.\na table, macro, or view name that you can use to form a hexadecimal name literalTableName column of the DBC.Tables view.\na table, macro, or view name that you can use to form a Unicode delimited identiﬁerTableName column of the DBC.TablesV view.\nExample: Finding Hexadecimal Name Literals in DBC.T ables [Deprecated]\nThis method is deprecated because it no longer works for all names. Instead, use the UESCAPE method shown in the next example.\nTo ﬁnd the internal hexadecimal representation for the table Dbase that in a form that converts to a hexadecimal name literal, select CHAR2HEXINT from DBC.Tables:\nSELECT CHAR2HEXINT(T.TableName) (FORMAT 'X(60)',\n   TITLE 'Internal Hex Representation'),T.TableName (TITLE 'Name')\nFROM DBC.Tables T\nWHERE T.TableKind = 'T' AND T.TableName = 'Dbase';\nThe system returns the result, for example:\nSQL Fundamentals\nPage 40 of 99Internal Hex Representation                                  Name\n------------------------------------------------------------ --------------\n446261736520202020202020202020202020202020202020202020202020 Dbase\nYou can use the result to reconstruct the object in the form of a hexadecimal name literal.\n1. Remove the excess space characters (202020...) from the result.\n2. Enclose the remaining numbers (those to the left of the space characters) in double quotation marks.\n3. Add XN at the end.\nThe hexadecimal name literal for the preceding example is:\n\"4462617365\"XN\nTo obtain the internal hexadecimal representation of the names of other objects types, modify the WHERE clause. For example, to obtain the internal hexadecimal representation of a\nview, modify the WHERE clause to TableKind = 'V'.\nSimilarly, to obtain the internal hexadecimal representation of a macro, modify the WHERE condition to TableKind = 'M'.\nExample: Finding Unicode Delimited Identiﬁers in DBC.T ablesV\nYou can also ﬁnd the internal hexadecimal representation for any database object in DBC.TablesV view. These hexadecimal representations convert to Unicode delimited identiﬁers.\nFor example, for the table Dbase:\nSELECT CHAR2HEXINT(T.TableName) (FORMAT 'X(60)',\n   TITLE 'Internal Hex Representation'),T.TableName (TITLE 'Name')\nFROM DBC.TablesV T\nWHERE T.TableKind = 'T' AND T.TableName = 'Dbase';\nThe system returns the result, for example:\nInternal Hex Representation                                  Name\n------------------------------------------------------------ --------------\n00440062006100730065                                         Dbase\nYou can use the result to reconstruct the object name as a Unicode delimited identiﬁer:\n1. Copy the hexadecimal representation from the result of the SELECT CHAR2HEXINT request, for example:\n00440062006100730065\n2. Add a delimiter character before each set of double zeros (00), for example:\nx0044x0062x0061x0073x0065\nYou can use most printable (7-bit) ASCII characters as delimiters.\n3. Convert the string to UNICODE delimiter format, as shown in Working with Unicode Delimited Identiﬁers, for example:\nU&\"x0044x0062x0061x0073x0065\" UESCAPE 'x'\nRelated Information\nFor more information about:\nCHAR2HEXINT, see Teradata Vantage™ - SQL Functions, Expressions, and Predicates, B035-1145.\nObject names and name validation, see General Rules for Constructing Object Names.\nClient character sets and object name restrictions, see International Character Set Support, B035-1125.\nDBC.TablesV view, see Teradata Vantage™ - Data Dictionary, B035-1092.\nTableKind column, see Teradata Vantage™ - Data Dictionary, B035-1092.\nReferencing Object Names in a Request\nWhen you reference an object name in a request, you may need to qualify the name. The topics that follow explain the rules for use of qualiﬁed and unqualiﬁed object names.\nThe following example shows unqualiﬁed, qualiﬁed and a fully qualiﬁed name references in a SELECT statement that accesses the Employee table in the personnel database:\nSELECT Name, DeptNo, JobTitle\nFROM Personnel.Employee\nWHERE Personnel.Employee.DeptNo = 100 ;\nRelated Information\nFor more information about the default database, see The Default Database.\nFully Qualified Object Names\nA fully qualiﬁed object name includes the names of all parent objects up to the level of the containing database. A common use case is a fully qualiﬁed column name, which consists of\na database name, table name, and column name.\nSQL Fundamentals\nPage 41 of 99[ [ database_name . ] table_name  ] column_name\ndatabase_name\nA qualifying name for the database in which the table and column being referenced is stored.\nDepending on the ambiguity of the reference, database_name can be required.\ntable_name\nA qualifying name for the table in which the column being referenced is stored.\nDepending on the ambiguity of the reference, table_name can be required.\ncolumn_name\nOne of the following:\nThe name of the column being referenced\nThe alias of the column being referenced\nThe keyword PARTITION\nName Resolution Rules and the Need to Fully Qualify a Name\nName resolution is performed statement by statement.\nAn ambiguous unqualiﬁed name returns an error to the requestor.\nWhen an INSERT statement contains a subquery, names are resolved in the subquery ﬁrst.\nNames in a view are resolved when the view is created.\nNames in a macro data manipulation statement are resolved when the macro is created.\nNames in a macro data deﬁnition statement are resolved when the macro is performed using the default database of the user submitting the EXECUTE statement.\nTherefore, you should fully qualify all names in a macro data deﬁnition statement, unless you speciﬁcally intend for the user’s default to be used.\nNames in stored procedure statements are resolved either when the procedure is created or when the procedure is executed, depending on whether the CREATE PROCEDURE\nstatement includes the SQL SECURITY clause and which option the clause speciﬁes.\nWhether unqualiﬁed object names acquire the database name of the creator, invoker, or owner of the stored procedure also depends on whether the CREATE PROCEDURE\nstatement includes the SQL SECURITY clause and which option the clause speciﬁes.\nUnqualified Object Names\nAn unqualiﬁed object name is a reference to an object such as a table, column, trigger, macro, or stored procedure that does not include parent objects. For example, the WHERE\nclause in the following statement uses “DeptNo” as an unqualiﬁed column name:\nSELECT *\nFROM Personnel.Employee\nWHERE DeptNo = 100 ;\nUnqualiﬁed Column Names\nYou can omit database and table name qualiﬁers when you reference columns if the reference is not ambiguous.\nFor example, the WHERE clause in the following statement:\nSELECT Name, DeptNo, JobTitle\nFROM Personnel.Employee\nWHERE Personnel.Employee.DeptNo = 100 ;\ncan be written as:\nWHERE DeptNo = 100 ;\nbecause the database name and table name can be derived from the Personnel.Employee reference in the FROM clause.\nOmitting Database Names\nWhen you omit the database name qualiﬁer, Vantage looks in the following databases to ﬁnd the unqualiﬁed object name:\nThe default database\nOther databases, if any, referenced by the SQL statement\nThe logon user database for a volatile table, if the unqualiﬁed object name is a table name\nThe SYSLIB database, if the unqualiﬁed object name is a C or C++ UDF that is not in the default database\nThe search must ﬁnd the name in only one of those databases. An ambiguous name error message results if the name exists in more than one of those databases.\nFor example, if your logon user database has no volatile tables named Employee and you have established Personnel as your default database, you can omit the Personnel database\nname qualiﬁer from the preceding sample query.\nUsing a Column Alias\nIn addition to referring to a column by name, an SQL query can reference a column by an alias. Column aliases are used for join indexes when two columns have the same name.\nHowever, an alias can be used for any column when a pseudonym is more descriptive or easier to use. Using an alias to name an expression allows a query to reference the\nexpression.\nSQL Fundamentals\nPage 42 of 99You can specify a column alias with or without the keyword AS on the ﬁrst reference to the column in the query. The following example creates and uses aliases for the ﬁrst two\ncolumns.\nSELECT departnumber AS d, employeename e, salary\nFROM personnel.employee\nWHERE d IN(100, 500)\nORDER BY d, e ;\nAlias names must meet the same requirements as names of other database objects.\nThe scope of alias names is conﬁned to the query.\nReferencing All Columns in a Table\nAn asterisk references all columns in a row simultaneously, for example, the following SELECT statement references all columns in the Employee table. A list of those fully qualiﬁed\ncolumn names follows the query.\nSELECT * FROM Employee;\nPersonnel.Employee.EmpNo\nPersonnel.Employee.Name\nPersonnel.Employee.DeptNo\nPersonnel.Employee.JobTitle\nPersonnel.Employee.Salary\nPersonnel.Employee.YrsExp\nPersonnel.Employee.DOB\nPersonnel.Employee.Gender\nPersonnel.Employee.Race\nPersonnel.Employee.MStat\nPersonnel.Employee.EdLev\nPersonnel.Employee.HCap\nExpressions\nAn expression, which speciﬁes a value, can consist of literals (or constants), name references, or operations using names and literals.\nScalar Expressions\nA scalar expression produces a single number, character string, byte string, date, time, timestamp, or interval.\nA value expression has exactly one declared type common to every possible result of evaluation. Implicit type conversion rules apply to expressions.\nQuery Expressions\nQuery expressions operate on table values and produce rows and tables of data. Query expressions can include a FROM clause, which operates on a table reference and returns a\nsingle-table value.\nZero-table SELECT statements do not require a FROM clause.\nLiterals\nLiterals, or constants, are values coded directly in the text of an SQL statement, view or macro deﬁnition text, or CHECK constraint deﬁnition text. In general, the system is able to\ndetermine the data type of a literal by its form.\nNumeric Literals\nA numeric literal (also referred to as a constant) is a character string of 1 to 40 characters selected from the following:\nDigits 0 through 9\nPlus sign\nMinus sign\nDecimal point\nThere are three types of numeric literals: integer, decimal, and ﬂoating point.\nType Description\nInteger Literal An integer literal declares literal strings of integer numbers. Integer literals consist of an optional sign followed by a sequence of up to 10 digits.\nA numeric literal that is outside the range of values of an INTEGER is considered a decimal literal.\nHexadecimal integer literals also represent integer values. A hexadecimal integer literal speciﬁes a string of 0 to 62000 hexadecimal digits\nenclosed with apostrophes followed by the characters XI1 for a BYTEINT, XI2 for a SMALLINT, XI4 for an INTEGER, or XI8 for a BIGINT.\nDecimal Literal A decimal literal declares literal strings of decimal numbers.\nSQL Fundamentals\nPage 43 of 99Type Description\nDecimal literals consist of the following components, reading from left-to-right: an optional sign, an optional sequence of up to 38 digits\n(mandatory only when no digits appear after the decimal point), an optional decimal point, an optional sequence of digits (mandatory only\nwhen no digits appear before the decimal point). The scale and precision of a decimal literal are determined by the total number of digits in the\nliteral and the number of digits to the right of the decimal point, respectively.\nFloating Point Literal A ﬂoating point literal declares literal strings of ﬂoating point numbers.\nFloating point literals consist of the following components, reading from left-to-right: an optional sign, an optional sequence of digits (mandatory\nonly when no digits appear after the decimal point) representing the whole number portion of the mantissa, an optional decimal point, an\noptional sequence of digits (mandatory only when no digits appear before the decimal point) representing the fractional portion of the\nmantissa, the literal character E, an optional sign, a sequence of digits representing the exponent.\nDateT ime Literals\nDate and time literals declare date, time, or timestamp values in a SQL expression, view or macro deﬁnition text, or CONSTRAINT deﬁnition text.\nDate and time literals are introduced by keywords. For example:\nDATE '1969-12-23'\nThere are three types of DateTime literals: DATE, TIME, and TIMESTAMP.\nType Description\nDATE Literal A date literal declares a date value in ANSI DATE format. ANSI DATE literal is the preferred format for DATE constants. All DATE operations accept\nthis format.\nTIME Literal A time literal declares a time value and an optional time zone offset.\nTIMESTAMP Literal A timestamp literal declares a timestamp value and an optional time zone offset.\nInterval Literals\nInterval literals provide a means for declaring spans of time.\nInterval literals are introduced and followed by keywords. For example:\nINTERVAL '200' HOUR\nThere are two mutually exclusive categories of interval literals: Year-Month and Day-Time.\nCategory Type Description\nYear-Month\nYEAR\nYEAR TO MONTH\nMONTHRepresent a time span that can include a number of years and months.\nDay-Time\nDAY\nDAY TO HOUR\nDAY TO MINUTE\nDAY TO SECOND\nHOUR\nHOUR TO MINUTE\nHOUR TO SECOND\nMINUTE\nMINUTE TO SECOND\nSECONDRepresent a time span that can include a number of days, hours, minutes, or seconds.\nCharacter Literals\nA character literal declares a character value in an expression, view or macro deﬁnition text, or CHECK constraint deﬁnition text.\nType Description\nCharacter literal Character literals consist of 0 to 31000bytes delimited by a matching pair of apostrophes. A zero-length character literal is represented by two\nconsecutive apostrophes ('').\nHexadecimal character literal A hexadecimal character literal speciﬁes a string of 0 to 62000 hexadecimal digits enclosed with apostrophes followed by the characters XCF\nfor a CHARACTER data type or XCV for a VARCHAR data type.\nUnicode character string literal Unicode character string literals consist of 0 to 31000 Unicode characters and are useful for inserting character strings containing characters\nthat cannot generally be entered directly from a keyboard.\nSQL Fundamentals\nPage 44 of 99Graphic Literals\nA graphic literal speciﬁes multibyte characters within the graphic repertoire.\nPeriod Literals\nA period literal speciﬁes a constant value of a Period data type. Period literals are introduced by the PERIOD keyword. For example:\nPERIOD '(2008-01-01, 2008-02-01)'\nThe element type of a period literal (DATE, TIME, or TIMESTAMP) is derived from the format of the DateTime values speciﬁed in the string literal.\nRelated Information\nFor more information about:\nData type conversions, see Teradata Vantage™ - Data Types and Literals, B035-1143.\nQuery expressions, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nNumeric, DateTime, interval, character, graphic, period, object name and hexadecimal literals, see Teradata Vantage™ - Data Types and Literals, B035-1143.\nCalendar functions, see Teradata Vantage™ - SQL Date and Time Functions and Expressions, B035-1211.\nDateTime functions and expressions, see Teradata Vantage™ - Data Types and Literals, B035-1143.\nSet operators, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nSee Teradata Vantage™ - SQL Functions, Expressions, and Predicates, B035-1145 for the following:\naggregate functions\narithmetic expressions\nbuilt-in functions\nbyte functions\nCASE expressions\ncomparison operators and functions\ninterval expressions\nlogical predicates\nordered analytical functions\nstring operators and functions\nuser-deﬁned functions\nFunctions\nThe following sections describe functions.\nScalar Functions\nScalar functions take input parameters and return a single value result. Some examples of standard SQL scalar functions are CHARACTER_LENGTH, POSITION, and SUBSTRING.\nAggregate Functions\nAggregate functions produce summary results. They differ from scalar functions in that they take grouped sets of relational data, make a pass over each group, and return one result\nfor the group. Some examples of standard SQL aggregate functions are AVG, SUM, MAX, and MIN.\nBuilt-In Functions\nThe built-in functions, or special register functions, which have no arguments, return various information about the system and can be used like other literals within SQL expressions. In\nan SQL query, the appropriate system value is substituted by the Parser after optimization but prior to executing a query using a cachable plan.\nAvailable built-in functions include all of the following:\nACCOUNT\nCURRENT_DATE\nCURRENT_ROLE\nCURRENT_TIME\nCURRENT_TIMESTAMP\nCURRENT_USER\nDATABASE\nDATE\nPROFILE\nROLE\nSESSION\nTIME\nUSER\nRelated Information\nFor more information about:\nSQL Fundamentals\nPage 45 of 99See Teradata Vantage™ - SQL Functions, Expressions, and Predicates, B035-1145 for the following.\nNames, parameters, return values, and other details of scalar and aggregate functions\nClauses and phrases\nOperators\nSQL operators express logical and arithmetic operations. Operators of the same precedence are evaluated from left to right.\nParentheses can be used to control the order of precedence. When parentheses are present, operations are performed from the innermost set of parentheses outward.\nThe following deﬁnitions apply to SQL operators.\nTerm Deﬁnition\nnumeric Any literal, data reference, or expression having a numeric value.\nstring Any character string or string expression.\nlogical A Boolean expression (resolves to TRUE, FALSE, or unknown).\nvalue Any numeric, character, or byte data item.\nset A collection of values returned by a subquery, or a list of values separated by commas and enclosed by parentheses.\nSQL Operations and Precedence\nSQL operations, and the order in which they are performed when no parentheses are present, appear in the following table. Operators of the same precedence are evaluated from left\nto right.\nPrecedence Result T ype Operation\nhighest numeric + numeric   (unary plus)\n- numeric    (unary minus)\nintermediatenumeric numeric ** numeric     (exponentiation)\nnumeric numeric * numeric     (multiplication)\nnumeric / numeric     (division)\nnumeric MOD numeric     (modulo operator)\nnumeric numeric + numeric    (addition)\nnumeric - numeric    (subtraction)\nstring concatenation operator\nlogical\nvalue EQ value\nvalue NE value\nvalue GT value\nvalue LE value\nvalue LT value\nvalue GE value\nvalue IN set\nvalue NOT IN set\nvalue BETWEEN value AND value\ncharacter value LIKE character value\nlogical NOT logical\nlogical logical AND logical\nlowest logical logical OR logical\nSeparators\nThe following sections describes separators.\nLexical Separators\nA lexical separator is a character string that can be placed between words, literals, and delimiters without changing the meaning of a statement.\nValid lexical separators are any of the following.\nComments\nPad characters (several pad characters are treated as a single pad character except in a string literal)\nSQL Fundamentals\nPage 46 of 99RETURN characters (X’0D’)\nStatement Separators\nThe SEMICOLON is a Teradata SQL statement separator.\nEach statement of a multistatement request must be separated from any subsequent statement with a semicolon.\nThe following multistatement request illustrates the semicolon as a statement separator.\nSHOW TABLE Payroll_Test ; INSERT INTO Payroll_Test\n(EmpNo, Name, DeptNo) VALUES ('10044', 'Jones M',\n'300') ; INSERT INTO ...\nFor statements entered using BTEQ, a request terminates with an input line-ending semicolon unless that line has a comment, beginning with two dashes (- -). Everything to the right of\nthe - - is a comment. In this case, the semicolon must be on the following line.\nThe SEMICOLON as a statement separator in a multistatement request is a Teradata extension to the ANSI/ISO SQL:2011 standard.\nRelated Information\nFor an explanation of comment lexical separators, see Comments.\nDelimiters\nDelimiters are special characters having meanings that depend on context.\nThe function of each delimiter appears in the following table.\nDelimiter Name Purpose\n(\n)LEFT PARENTHESIS\nRIGHT PARENTHESISGroup expressions and deﬁne the limits of various phrases.\n, COMMA Separates and distinguishes column names in the select list, or column names or parameters in an optional clause,\nor DateTime ﬁelds in a DateTime type.\n: COLON Preﬁxes reference parameters or client system variables.\nAlso separates DateTime ﬁelds in a DateTime type.\n. FULLSTOP\nSeparates database names from table, trigger, UDF, UDT, and stored procedure names, such as\npersonnel.employee.\nSeparates table names from a particular column name, such as employee.deptno).\nIn numeric constants, the period is the decimal point.\nSeparates DateTime ﬁelds in a DateTime type.\nSeparates a method name from a UDT expression in a method invocation.\n; SEMICOLON\nSeparates statements in multistatement requests.\nSeparates statements in a stored procedure body.\nSeparates SQL procedure statements in a triggered SQL statement in a trigger deﬁnition.\nTerminates requests submitted via utilities such as BTEQ.\nTerminates embedded SQL statements in C or PL/I applications.\n’ APOSTROPHE\nDeﬁnes the boundaries of character string constants.\nTo include an APOSTROPHE character or show possession in a title, double the APOSTROPHE characters.\nAlso separates DateTime ﬁelds in a DateTime type.\n“ QUOTATION MARKS Deﬁnes the boundaries of nonstandard names.\n/ SOLIDUS Separates DateTime ﬁelds in a DateTime type.\nB\nbUppercase B\nLowercase b\n- HYPHEN-MINUS\nExample: Using Delimiters\nIn the following statement submitted through BTEQ, the FULLSTOP separates the database name (Examp and Personnel) from the table name (Proﬁles and Employee), and, where\nreference is qualiﬁed to avoid ambiguity, it separates the table name (Proﬁles, Employee) from the column name (DeptNo).\nSQL Fundamentals\nPage 47 of 99UPDATE Examp.Profiles SET FinGrad = 'A'\nWHERE Name = 'Phan A' ; SELECT EdLev, FinGrad,JobTitle,\nYrsExp FROM Examp.Profiles, Personnel.Employee\nWHERE Profiles.DeptNo = Employee.DeptNo ;\nThe ﬁrst SEMICOLON separates the UPDATE statement from the SELECT statement. The second SEMICOLON terminates the entire multistatement request.\nThe semicolon is required in Teradata SQL to separate multiple statements in a request and to terminate a request submitted through BTEQ.\nComments\nYou can embed comments within an SQL request anywhere a pad character can occur.\nThe SQL Parser and the preprocessor recognize the following types of ANSI/ISO SQL:2011-compliant embedded comments:\nSimple\nBracketed\nSimple Comments\nThe simple form of a comment is delimited by two consecutive HYPHEN-MINUS (U+002D) characters (--) at the beginning of the comment and the newline character at the end of the\ncomment.\n-- comment_text  new_line_character\nThe newline character is implementation-speciﬁc, but is typed by pressing the Enter (non-3270 terminals) or Return (3270 terminals) key.\nSimple SQL comments cannot span multiple lines.\nExample: Using a Simple Comment at the End of a Line\nThe following examples illustrate the use of a simple comment at the end of a line, at the beginning of a line, and at the beginning of a statement:\nSELECT EmpNo, Name FROM Payroll_Test\nORDER BY Name -- Simple comment at the end of a line\n;\nSELECT EmpNo, Name FROM Payroll_Test\n-- Simple comment at the beginning of a line\nORDER BY Name;\n-- Simple comment at the beginning of a statement\nSELECT EmpNo, Name FROM Payroll_Test\nORDER BY Name;\nBracketed Comments\nA bracketed comment is a text string of unrestricted length that is delimited by the beginning comment characters SOLIDUS (U+002F) and ASTERISK (U+002A) /* and the end\ncomment characters ASTERISK and SOLIDUS */.\n/* comment_text  */\nBracketed comments can begin anywhere on an input line and can span multiple lines.\nExample: Using a Bracketed Comment at the Beginning, Middle and End of a Line\nThe following examples illustrate the use of a bracketed comment at the end of a line, in the middle of a line, at the beginning of a line, and at the beginning of a statement:\nSELECT EmpNo, Name FROM Payroll_Test /* This bracketed comment starts\n                                        at the end of a line\n                                        and spans multiple lines. */\nORDER BY Name;\nSELECT EmpNo, Name FROM Payroll_Test\n/* This bracketed comment starts\n   at the beginning of a line\n   and spans multiple lines. */\nORDER BY Name;\n/* This bracketed comment starts\n   at the beginning of a statement\n   and spans multiple lines. */\nSELECT EmpNo, Name FROM Payroll_Test\nORDER BY Name;\nSELECT EmpNo, Name\nFROM  /* This comment is in the middle of a line. */ Payroll_Test\nORDER BY Name;\nSELECT EmpNo, Name FROM Payroll_Test /* This bracketed\n   comment starts at the end of a line, spans multiple\nSQL Fundamentals\nPage 48 of 99   lines, and ends in the middle of a line. */ ORDER BY Name;\nSELECT EmpNo, Name FROM Payroll_Test\n/* This bracketed comment starts at the beginning of a line,\n   spans multiple lines, and ends in the\n   middle of a line. */ ORDER BY Name;\nComments W ith Multibyte Character Set Strings\nYou can include multibyte character set strings in both simple and bracketed comments.\nWhen using mixed mode in comments, you must have a properly formed mixed mode string, which means that a Shift-In (SI) must follow its associated Shift-Out (SO).\nIf an SI does not follow the multibyte string, the results are unpredictable.\nWhen using bracketed comments that span multiple lines, the SI must be on the same line as its associated SO. If the SI and SO are not on the same line, the results are unpredictable.\nYou must specify the bracketed comment delimiters (/* and */) as single byte characters.\nTerminators\nThe SEMICOLON is a Teradata SQL request terminator when it is the last nonblank character on an input line in BTEQ unless that line has a comment beginning with two dashes. In this\ncase, the SEMICOLON request terminator must be on the line following the comment line.\nA request is considered complete when either the “End of Text” character or the request terminator character is detected.\nANSI Compliance\nThe SEMICOLON as a request terminator is a Teradata extension to the ANSI/ISO SQL:2011 standard.\nExample: Request T ermination\nOn the following input line:\nSELECT *\nFROM Employee ;\nthe SEMICOLON terminates the single-statement request “SELECT * FROM Employee”.\nBTEQ uses SEMICOLONs to terminate multistatement requests.\nA request terminator is mandatory for request types that are:\nIn the body of a macro\nTriggered action statements in a trigger deﬁnition\nEntered using the BTEQ interface\nEntered using other interfaces that require BTEQ\nExample: Using a Request T erminator in the Body of a Macro\nThe following statement illustrates the use of a request terminator in the body of a macro.\nCREATE MACRO Test_Pay (number (INTEGER),\n                       name (VARCHAR(12)),\n                       dept (INTEGER) AS\n( INSERT INTO Payroll_Test (EmpNo, Name, DeptNo)\n  VALUES (:number, :name, :dept) ;\n  UPDATE DeptCount\n  SET EmpCount = EmpCount + 1 ;\n  SELECT *\n  FROM DeptCount ; )\nExample: BTEQ Request\nWhen entered through BTEQ, the entire CREATE MACRO statement must be terminated.\nCREATE MACRO Test_Pay\n(number (INTEGER),\n name   (VARCHAR(12)),\n dept   (INTEGER) AS\n(INSERT INTO Payroll_Test (EmpNo, Name, DeptNo)\n VALUES (:number, :name, :dept) ;\n UPDATE DeptCount\n SET EmpCount = EmpCount + 1 ;\n SELECT *\n FROM DeptCount ; ) ;\nSQL Fundamentals\nPage 49 of 99The Default Database\nThe default database is a Teradata extension to SQL that deﬁnes a database that is used to look for unqualiﬁed names, such as table, view, trigger, or macro names, in SQL\nstatements.\nThe default database is not the only database used to ﬁnd an unqualiﬁed name in an SQL statement. Vantage also looks for the name in:\nOther databases, if any, referenced by the SQL statement\nThe logon user database for a volatile table, if the unqualiﬁed object name is a table name\nThe SYSLIB database, if the unqualiﬁed object name is a C or C++ UDF that is not in the default database\nIf the unqualiﬁed object name exists in more than one of the databases in which Vantage looks, the SQL statement produces an ambiguous name error.\nEstablishing a Permanent Default Database\nYou can establish a permanent default database that is invoked each time you log on.\nGoal SQL Data Deﬁnition Statements to Use\nDeﬁne a permanent default database.\nCREATE USER, with a DEFAULT DATABASE clause.\nCREATE USER, with a PROFILE clause that speciﬁes a proﬁle that deﬁnes the default database.\nChange your permanent default database deﬁnition.\nMODIFY USER, with a DEFAULT DATABASE clause.\nMODIFY USER, with a PROFILE clause.\nMODIFY PROFILE, with a DEFAULT DATABASE clause.\nadd a default database when one had not been established previously\nMODIFY USER, with a DEFAULT DATABASE clause.\nMODIFY USER, with a PROFILE clause.\nMODIFY PROFILE, with a DEFAULT DATABASE clause.\nFor example, the following statement automatically establishes Personnel as the default database for Marks at the next logon:\nMODIFY USER marks AS\nDEFAULT DATABASE = personnel ;\nAfter you assign a default database, Vantage uses that database as one of the databases to look for all unqualiﬁed object references.\nTo obtain information from a table, view, trigger, or macro in another database, fully qualify the table reference by specifying the database name, a FULLSTOP character, and the table\nname.\nEstablishing a Default Database for a Session\nYou can establish a default database for the current session that Vantage uses to look for unqualiﬁed object names in SQL statements.\nTo establish a default database for a session, use the DATABASE statement.\nFor example, after entering the following SQL statement:\nDATABASE personnel ;\nYou can enter a SELECT statement:\nSELECT deptno (TITLE 'Org'), name\nFROM employee ;\nwhich has the same results as:\nSELECT deptno (TITLE 'Org'), name\nFROM personnel.employee;\nTo establish a default database, you must have a privilege on an object in that database. After deﬁnition, the default database remains in effect until the end of a session or until it is\nreplaced by a subsequent DATABASE statement.\nDefault Database for a Stored Procedure\nStored procedures can contain SQL statements with unqualiﬁed object references. The default database that Vantage uses for the unqualiﬁed object references depends on whether\nthe CREATE PROCEDURE statement includes the SQL SECURITY clause and, if it does, which option the SQL SECURITY clause speciﬁes.\nRelated Information\nFor more information about:\nSQL Fundamentals\nPage 50 of 99The DATABASE, CREATE USER, MODIFY USER, or using the CREATE PROFILE statement to deﬁne the default database for member users, see Teradata Vantage™ - SQL Data\nDeﬁnition Language Syntax and Examples, B035-1144.\nFully-qualiﬁed names, see Referencing Object Names in a Request or Object Names.\nUsing the creator, invoker, or owner of the stored procedure as the default database, see CREATE PROCEDURE in Teradata Vantage™ - SQL Data Deﬁnition Language Syntax\nand Examples, B035-1144.\nNull Statements\nA null statement has no content except for optional pad characters or SQL comments.\nExample: Using a Semicolon as a Null Statement\nThe semicolon in the following request is a null statement.\n/* This example shows a comment followed by\n   a semicolon used as a null statement */\n;  UPDATE Pay_Test SET ...\nExample: Using Semicolons as Null Statements and Statement Separators\nThe ﬁrst SEMICOLON in the following request is a null statement. The second SEMICOLON is taken as statement separator:\n/*    This example shows a semicolon used as a null\n   statement and as a statement separator */\n;  UPDATE Payroll_Test SET Name = 'Wedgewood A'\n   WHERE Name = 'Wedgewood A'\n;   SELECT ...\n-- This example shows the use of an ANSI component\n-- used as a null statement and statement separator ;\nExample: Using a Semicolon that Precedes the Statement\nA SEMICOLON that precedes the ﬁrst (or only) statement of a request is taken as a null statement.\n;DROP TABLE temp_payroll;\nNULL Keyword as a Literal\nThe keyword NULL represents null, and is sometimes available as a special construct similar to, but not identical with, a literal.\nA null represents either:\nAn empty column\nAn unknown value\nAn unknowable value\nNulls are neither values nor do they signify values; they represent the absence of value. A null is a place holder indicating that no value is present.\nANSI Compliance\nNULL is ANSI/ISO SQL:2011-compliant with extensions.\nUsing NULL  as a Literal\nUse NULL as a literal in the following ways:\nA CAST source operand, for example:\nSELECT CAST (NULL AS DATE);\nA CASE result, for example.\nSELECT CASE WHEN orders = 10 THEN NULL END FROM sales_tbl;\nAn insert item specifying a null is to be placed in a column position on INSERT.\nAn update item specifying a null is to be placed in a column position on UPDATE.\nA default column deﬁnition speciﬁcation, for example:\nCREATE TABLE European_Sales\n   (Region INTEGER DEFAULT 99\n   ,Sales Euro_Type DEFAULT NULL);\nAn explicit SELECT item, for example:\nSELECT NULL\nSQL Fundamentals\nPage 51 of 99This is a Teradata extension to ANSI.\nAn operand of a function, for example:\nSELECT TYPE(NULL)\nThis is a Teradata extension to ANSI.\nData T ype of NULL\nWhen you use NULL as an explicit SELECT item or as the operand of a function, its data type is INTEGER. In all other cases NULL has no data type because it has no value.\nFor example, if you perform SELECT TYPE(NULL), then INTEGER is returned as the data type of NULL.\nTo avoid type issues, cast NULL to the desired type.\nRelated Information\nFor information on the behavior of nulls and how to use them in data manipulation statements, see Manipulating Nulls.\nSQL Data Definition, Control, and Manipulation\nThis section describes the functional families of the SQL language.\nSQL Functional Families and Binding Styles\nThe SQL language can be characterized in several different ways. This section is organized around functional groupings of the components of the language with minor emphasis on\nbinding styles.\nFunctional Family\nSQL provides facilities for deﬁning database objects, for deﬁning user access to those objects, and for manipulating the data stored within them.\nThe following list describes the principal functional families of the SQL language.\nSQL Data Deﬁnition Language (DDL)\nSQL Data Control Language (DCL)\nSQL Data Manipulation Language (DML)\nQuery and Workload Analysis Statements\nHelp and Database Object Deﬁnition Tools\nSome classiﬁcations of SQL group the data control language statements with the data deﬁnition language statements.\nBinding Style\nThe ANSI/ISO SQL standards do not deﬁne the term binding style. The expression refers to a possible method by which an SQL statement can be invoked.\nThe database supports the following SQL binding styles:\nDirect, or interactive\nEmbedded SQL\nStored procedure\nSQL Call Level Interface (as ODBC)\nJDBC\nThe direct binding style is usually not qualiﬁed in this document set because it is the default style. Embedded SQL and stored procedure binding styles are always clearly speciﬁed,\neither explicitly or by context.\nRelated Information\nFor more information about:\nEmbedded SQL, see Teradata® Preprocessor2 for Embedded SQL Programmer Guide, B035-2446 or Teradata Vantage™ - SQL Stored Procedures and Embedded SQL,\nB035-1148.\nStored procedures, see Teradata Vantage™ - SQL Stored Procedures and Embedded SQL, B035-1148.\nODBC, see ODBC Driver for Teradata® User Guide, B035-2526.\nJDBC, see Teradata JDBC Driver Reference, available at https://teradata-docs.s3.amazonaws.com/doc/connectivity/jdbc/reference/current/frameset.html.\nData Definition Language\nThe SQL Data Deﬁnition Language (DDL) is a subset of the SQL language and consists of all SQL statements that support the deﬁnition of database objects.\nData deﬁnition language statements perform the following functions:\nCreate, drop, rename, alter, modify, and replace database objects\nComment on database objects\nSQL Fundamentals\nPage 52 of 99Collect statistics on a column set or index\nEstablish a default database\nSet a different collation sequence, account priority, DateForm, time zone, and database for the session\nSet roles\nSet the query band for a session or transaction\nBegin and end logging\nEnable and disable online archiving for all tables in a database or a speciﬁc set of tables\nRules on Entering DDL  Statements\nA DDL statement can be entered as:\nA single-statement request.\nThe solitary statement, or the last statement, in an explicit transaction (in Teradata mode, one or more requests enclosed by user-supplied BEGIN TRANSACTION and END\nTRANSACTION statement, or in ANSI mode, one or more requests ending with the COMMIT keyword).\nThe solitary statement in a macro.\nDDL statements cannot be entered as part of a multistatement request.\nSuccessful execution of a DDL statement automatically creates and updates entries in the Data Dictionary.\nRelated Information\nFor detailed information about the function, syntax, and usage of Teradata SQL Data Deﬁnition statements, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and\nExamples, B035-1144.\nAltering Table Structure and Definition\nYou may need to change the structure or deﬁnition of an existing table or temporary table. In many cases, you can use ALTER TABLE and RENAME to make the changes. Some\nchanges, however, may require you to use CREATE TABLE to recreate the table.\nYou cannot use ALTER TABLE on an error logging table.\nMaking Changes to a T able\nUse the RENAME TABLE statement to change the name of a table, temporary table, queue table, or error logging table.\nUse the ALTER TABLE statement to perform any of the following functions:\nAdd or drop columns on an existing table or temporary table\nAdd column default control, FORMAT, and TITLE attributes on an existing table or temporary table\nAdd or remove journaling options on an existing table or temporary table\nAdd or remove the FALLBACK option on an existing table or temporary table\nChange the DATABLOCKSIZE or percent FREESPACE on an existing table or temporary table\nAdd or drop column and table level constraints on an existing table or temporary table\nChange the LOG and ON COMMIT options for a global temporary table\nModify referential constraints\nChange the properties of the primary index for a table (some cases require an empty table)\nChange the partitioning properties of the primary index for a table, including modiﬁcations to the partitioning expression deﬁned for use by a partitioned primary index (some\ncases require an empty table)\nRegenerate table headers and optionally validate and correct the partitioning of PPI table rows\nDeﬁne, modify, or delete the COMPRESS attribute for an existing column\nAdd the BLOCKCOMPRESSION option used to modify the temperature-based Block Level Compression (BLC) for a table.\nChange column attributes (that do not affect stored data) on an existing table or temporary table\nRestrictions apply to many of the preceding modiﬁcations.\nTo perform any of the following functions, use CREATE TABLE to recreate the table:\nRedeﬁne the primary index or its partitioning for a non-empty table when not allowed for ALTER TABLE\nChange a data type attribute that affects existing data\nAdd a column that would exceed the maximum lifetime column count\nInteractively, the SHOW TABLE statement can call up the current table deﬁnition, which can then be modiﬁed and resubmitted to create a new table.\nIf the stored data is not affected by incompatible data type changes, an INSERT ... SELECT statement can be used to transfer data from the existing table to the new table.\nRelated Information\nFor a complete list of rules and restrictions on using ALTER TABLE to change the structure or deﬁnition of an existing table, see Teradata Vantage™ - SQL Data Deﬁnition Language\nSyntax and Examples, B035-1144.\nDropping and Renaming Objects\nTo drop an object, use the appropriate DDL statement.\nSQL Fundamentals\nPage 53 of 99Dropping Objects\nObject Use\nConstraint DROP CONSTRAINT\nError logging table DROP ERROR TABLE\nDROP TABLE\nHash index DROP HASH INDEX\nJoin index DROP JOIN INDEX\nMacro DROP MACRO\nProﬁle DROP PROFILE\nRole DROP ROLE\nSecondary index DROP INDEX\nStored procedure DROP PROCEDURE\nTable DROP TABLE\nGlobal temporary table or volatile table\nPrimary index\nTrigger DROP TRIGGER\nUser-deﬁned function DROP FUNCTION\nUser-deﬁned method ALTER TYPE\nUser-deﬁned type DROP TYPE\nView DROP VIEW\nRenaming Objects\nTeradata SQL provides RENAME statements that you can use to rename some objects. To rename objects that do not have associated RENAME statements, you must ﬁrst drop them\nand then recreate them with a new name, or, in the case of primary indexes, use ALTER TABLE.\nObject Use\nHash index DROP HASH INDEX and then CREATE HASH INDEX\nJoin index DROP JOIN INDEX and then CREATE JOIN INDEX\nMacro RENAME MACRO\nPrimary index ALTER TABLE\nProﬁle DROP PROFILE and then CREATE PROFILE\nRole DROP ROLE and then CREATE ROLE\nSecondary index DROP INDEX and then CREATE INDEX\nStored procedure RENAME PROCEDURE\nTable RENAME TABLE\nGlobal temporary table or volatile table\nQueue table\nError logging table\nTrigger RENAME TRIGGER\nUser-Deﬁned Function RENAME FUNCTION\nUser-Deﬁned Method ALTER TYPE and then CREATE METHOD\nUser-Deﬁned Type DROP TYPE and then CREATE TYPE\nView RENAME VIEW\nRelated Information\nFor more information on these statements, including rules that apply to usage, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nData Control Language\nSQL Fundamentals\nPage 54 of 99The SQL Data Control Language (DCL) is a subset of the SQL language and consists of all SQL statements that support the deﬁnition of security authorization for accessing database\nobjects.\nData control statements perform the following functions:\nGrant and revoke privileges\nGive ownership of a database to another user\nRules on Entering DCL  Statements\nA data control statement can be entered as:\nA single-statement request\nThe solitary statement, or as the last statement, in an “explicit transaction” (one or more requests enclosed by user-supplied BEGIN TRANSACTION and END TRANSACTION\nstatement in Teradata mode, or in ANSI mode, one or more requests ending with the COMMIT keyword).\nThe solitary statement in a macro\nA data control statement cannot be entered as part of a multistatement request.\nSuccessful execution of a data control statement automatically creates and updates entries in the Data Dictionary.\nTeradata SQL  DCL  Statements\nFor detailed information about the function, syntax, and usage of Teradata SQL Data Control statements, see Teradata Vantage™ - SQL Data Control Language, B035-1149.\nData Manipulation Language\nThe SQL Data Manipulation Language (DML) is a subset of the SQL language and consists of all SQL statements that support the manipulation or processing of database objects.\nSelecting Columns\nThe SELECT statement returns information from the tables in a relational database. SELECT speciﬁes the table columns from which to obtain the data, the corresponding database (if\nnot deﬁned by default), and the table (or tables) to be accessed within that database.\nFor example, to request the data from the name, salary, and jobtitle columns of the Employee table, type:\nSELECT name, salary, jobtitle FROM employee ;\nThe response might be something like the following results table.\nName Salary JobT itle\nNewman P28600.00 Test Tech\nChin M 38000.00 Controller\nAquilar J 45000.00 Manager\nRussell S 65000.00 President\nClements D38000.00 Salesperson\nThe left-to-right order of the columns in a result table is determined by the order in which the column names are entered in the SELECT statement. Columns in a relational table are not\nordered logically.\nAs long as a statement is otherwise constructed properly, the spacing between statement elements is not important as long as at least one pad character separates each element that\nis not otherwise separated from the next.\nFor example, the SELECT statement in the previous example could just as well be formulated like this:\nSELECT   name,   salary,jobtitle\nFROM employee;\nNotice that there are multiple pad characters between most of the elements and that a comma only (with no pad characters) separates column name salary from column name jobtitle.\nTo select all the data in the employee table, you could enter the following SELECT statement:\nSELECT * FROM employee ;\nThe asterisk speciﬁes that the data in all columns (except system-derived columns) of the table is to be returned.\nSelecting Rows\nThe SELECT statement retrieves stored data from a table. All rows, speciﬁed rows, or speciﬁc columns of all or speciﬁed rows can be retrieved. When used in a subquery, the SELECT\nstatement can also select rows from a derived table or view.\nThe FROM, WHERE, ORDER BY, DISTINCT, WITH, GROUP BY, HAVING, and TOP clauses provide for a ﬁne detail of selection criteria.\nTo obtain data from speciﬁc rows of a table, use the WHERE clause of the SELECT statement. That portion of the clause following the keyword WHERE causes a search for rows that\nsatisfy the condition speciﬁed.\nSQL Fundamentals\nPage 55 of 99For example, to get the name, salary, and title of each employee in Department 100, use the WHERE clause:\nSELECT name, salary, jobtitle FROM employee\n   WHERE deptno = 100 ;\nThe response appears in the following table.\nName Salary JobT itle\nChin M 38000.00 Controller\nGreene W 32500.00 Payroll Clerk\nMofﬁt H 35000.00 Recruiter\nPeterson J 25000.00 Payroll Clerk\nTo obtain data from a multirow result table in embedded SQL, declare a cursor for the SELECT statement and use it to fetch individual result rows for processing.\nTo obtain data from the row with the oldest timestamp value in a queue table, use the SELECT AND CONSUME statement, which also deletes the row from the queue table.\nZero-T able SELECT\nZero-table SELECT statements return data but do not access tables.\nFor example, the following SELECT statement speciﬁes an expression after the SELECT keyword that does not require a column reference or FROM clause:\nSELECT 40000.00 / 52.;\nThe response is one row:\n(40000.00/52.)\n-----------------\n           769.23\nHere is another example that speciﬁes an attribute function after the SELECT keyword:\nSELECT TYPE(sales_table.region);\nBecause the argument to the TYPE function is a column reference that speciﬁes the table name, a FROM clause is not required and the query does not access the table.\nThe response is one row that might be something like the following:\nType(region)\n---------------------------------------\nINTEGER\nAdding Rows\nTo add a new row to a table, use the INSERT statement. To perform a bulk insert of rows by retrieving the new row data from another table, use the INSERT ... SELECT form of the\nstatement.\nDefaults and constraints deﬁned by the CREATE TABLE statement affect an insert operation in the following ways.\nWHEN an INSERT statement … THEN the system …\nattempts to add a duplicate row\nfor any unique index\nto a table deﬁned as SET (not to allow duplicate rows)returns an error, with one exception. The system silently ignores duplicate rows that an\nINSERT ... SELECT would create when the:\ntable is deﬁned as SET\nmode is Teradata\nomits a value for a column for which a default value is deﬁned stores the default value for that column.\nomits a value for a column for which both of the following statements are true:\nNOT NULL is speciﬁed\nno default is speciﬁedrejects the operation and returns an error message.\nsupplies a value that does not satisfy the constraints speciﬁed for a column or violates\nsome deﬁned constraint on a column or columnsrejects the operation and returns an error message.\nIf you are performing a bulk insert of rows using INSERT ... SELECT, and you want Vantage to log errors that prevent normal completion of the operation, use the LOGGING ERRORS\noption. Database logs errors as error rows in an error logging table that you create with a CREATE ERROR TABLE statement.\nUpdating Rows\nTo modify data in one or more rows of a table, use the UPDATE statement. In the UPDATE statement, you specify the column name of the data to be modiﬁed along with the new value.\nYou can also use a WHERE clause to qualify the rows to change.\nSQL Fundamentals\nPage 56 of 99Attributes speciﬁed in the CREATE TABLE statement affect an update operation in the following ways:\nWhen an update supplies a value that violates some deﬁned constraint on a column or columns, the update operation is rejected and an error message is returned.\nWhen an update supplies the value NULL and a NULL is allowed, any existing data is removed from the column.\nIf the result of an UPDATE will violate uniqueness constraints or create a duplicate row in a table which does not allow duplicate rows, an error message is returned.\nTo update rows in a multirow result table in embedded SQL, declare a cursor for the SELECT statement and use it to fetch individual result rows for processing, then use a WHERE\nCURRENT OF clause in a positioned UPDATE statement to update the selected rows.\nTeradata supports a special form of UPDATE, called the upsert form, which is a single SQL statement that includes both UPDATE and INSERT functionality. The speciﬁed update\noperation performs ﬁrst, and if it fails to ﬁnd a row to update, then the speciﬁed insert operation performs automatically.\nDeleting Rows\nThe DELETE statement allows you to remove an entire row or rows from a table. A WHERE clause qualiﬁes the rows that are to be deleted.\nMerging Rows\nThe MERGE statement merges a source row set into a target table based on whether any target rows satisfy a speciﬁed matching condition with the source row. The MERGE statement\nis a single SQL statement that includes both UPDATE and INSERT functionality.\nIF the source and target rows … THEN the merge operation is an …\nsatisfy the matching condition update based on the speciﬁed WHEN MATCHED THEN UPDATE clause.\ndo not satisfy the matching condition insert based on the speciﬁed WHEN NOT MATCHED THEN INSERT clause.\nIf you are performing a bulk insert or update of rows using MERGE, and you want Vantage to log errors that prevent normal completion of the operation, use the LOGGING ERRORS\noption. Database logs errors as error rows in an error logging table that you create with a CREATE ERROR TABLE statement.\nRelated Information\nFor more information about:\nSelecting, adding, updating, deleting, and merging rows, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nThe DATABASE, CREATE USER, MODIFY USER, or using the CREATE PROFILE statement to deﬁne the default database for member users, see Teradata Vantage™ - SQL Data\nDeﬁnition Language Syntax and Examples, B035-1144.\nFor the way to delete rows in a multirow result table in embedded SQL, see Teradata Vantage™ - SQL Stored Procedures and Embedded SQL, B035-1148.\nSubqueries\nSubqueries are nested SELECT statements. They can be used to ask a series of questions to arrive at a single answer.\nExample: Three Level Subqueries\nThe following subqueries, nested to three levels, answer the question “Who manages the manager of Marston?”\nSELECT Name\nFROM Employee\nWHERE EmpNo IN\n   (SELECT MgrNo\n    FROM Department\n    WHERE DeptNo IN\n      (SELECT DeptNo\n       FROM Employee\n       WHERE Name = 'Marston A') ) ;\nThe subqueries that pose the questions leading to the ﬁnal answer are inverted:\nThe third subquery asks the Employee table for the number of Marston’s department.\nThe second subquery asks the Department table for the employee number (MgrNo) of the manager associated with this department number.\nThe ﬁrst subquery asks the Employee table for the name of the employee associated with this employee number (MgrNo).\nThe result table looks like the following:\nName\n--------\nWatson L\nThis result can be obtained using only two levels of subquery, as the following example shows.\nSELECT Name\nFROM Employee\nWHERE EmpNo IN\n (SELECT MgrNo\n  FROM Department, Employee\nSQL Fundamentals\nPage 57 of 99  WHERE Employee.Name = 'Marston A'\n  AND Department.DeptNo = Employee.DeptNo) ;\nIn this example, the second subquery deﬁnes a join of Employee and Department tables.\nThis result could also be obtained using a one-level query that uses correlation names, as the following example shows.\nSELECT M.Name\nFROM Employee M, Department D, Employee E\nWHERE M.EmpNo = D.MgrNo AND\n      E.Name = 'Marston A' AND\n      D.DeptNo = E.DeptNo;\nIn some cases, as in the preceding example, the choice is a style preference. In other cases, correct execution of the query may require a subquery.\nRelated Information\nFor more information, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nRecursive Queries\nA recursive query is a way to query hierarchies of data, such as an organizational structure, bill of materials, and document hierarchy.\nRecursion is typically characterized by three steps:\n1. Initialization\n2. Recursion, or repeated iteration of the logic through the hierarchy\n3. Termination\nSimilarly, a recursive query has three execution phases:\n1. Create an initial result set.\n2. Recursion based on the existing result set.\n3. Final query to return the ﬁnal result set.\nSpecifying a Recursive Query\nYou can specify a recursive query by:\nPreceding a query with the WITH RECURSIVE clause\nCreating a view using the RECURSIVE clause in a CREATE VIEW statement\nExample: Using the WITH RECURSIVE Clause\nConsider the following employee table:\nCREATE TABLE employee\n   (employee_number INTEGER\n   ,manager_employee_number INTEGER\n   ,last_name CHAR(20)\n   ,first_name VARCHAR(30));\nThe table represents an organizational structure containing a hierarchy of employee-manager data.\nThe following ﬁgure depicts what the employee table looks like hierarchically.\n\nSQL Fundamentals\nPage 58 of 99The following recursive query retrieves the employee numbers of all employees who directly or indirectly report to the manager with employee_number 801:\nWITH RECURSIVE temp_table (employee_number) AS\n( SELECT root.employee_number\n  FROM employee root\n  WHERE root.manager_employee_number = 801\nUNION ALL\n  SELECT indirect.employee_number\n  FROM temp_table direct, employee indirect\n  WHERE direct.employee_number = indirect.manager_employee_number\n)\nSELECT * FROM temp_table ORDER BY employee_number;\nIn the example, temp_table is a temporary named result set that can be referred to in the FROM clause of the recursive statement.\nThe initial result set is established in temp_table by the nonrecursive, or seed, statement and contains the employees that report directly to the manager with an employee_number of\n801:\nSELECT root.employee_number\nFROM employee root\nWHERE root.manager_employee_number = 801\nThe recursion takes place by joining each employee in temp_table with employees who report to the employees in temp_table. The UNION ALL adds the results to temp_table.\nSELECT indirect.employee_number\nFROM temp_table direct, employee indirect\nWHERE direct.employee_number = indirect.manager_employee_number\nRecursion stops when no new rows are added to temp_table.\nThe ﬁnal query is not part of the recursive WITH clause and extracts the employee information out of temp_table:\nSELECT * FROM temp_table ORDER BY employee_number;\nHere are the results of the recursive query:\nemployee_number\n---------------\n1001\n1002\n1003\n1004\n1006\n1008\n1010\n1011\n1012\n1014\n1015\n1016\n1019\nUsing the RECURSIVE Clause in a CREA TE VIEW Statement\nCreating a view using the RECURSIVE clause is similar to preceding a query with the WITH RECURSIVE clause.\nConsider the employee table that was presented in the previous example. The following statement creates a view named hierarchy_801 using a recursive query that retrieves the\nemployee numbers of all employees who directly or indirectly report to the manager with employee_number 801:\nCREATE RECURSIVE VIEW hierarchy_801 (employee_number) AS\n( SELECT root.employee_number\n  FROM employee root\n  WHERE root.manager_employee_number = 801\nUNION ALL\n  SELECT indirect.employee_number\n  FROM hierarchy_801 direct, employee indirect\n  WHERE direct.employee_number = indirect.manager_employee_number\n);\nThe seed statement and recursive statement in the view deﬁnition are the same as the seed statement and recursive statement in the previous recursive query that uses the WITH\nRECURSIVE clause, except that the hierarchy_801 view name is different from the temp_table temporary result name.\nTo extract the employee information, use the following SELECT statement on the hierarchy_801 view:\nSELECT * FROM hierarchy_801 ORDER BY employee_number;\nHere are the results:\nSQL Fundamentals\nPage 59 of 99employee_number\n---------------\n1001\n1002\n1003\n1004\n1006\n1008\n1010\n1011\n1012\n1014\n1015\n1016\n1019\nDepth Control to Avoid Inﬁnite Recursion\nIf the hierarchy is cyclic, or if the recursive statement speciﬁes a bad join condition, a recursive query can produce a runaway query that never completes with a ﬁnite result. The best\npractice is to control the depth of the recursion:\nSpecify a depth control column in the column list of the WITH RECURSIVE clause or recursive view\nInitialize the column value to 0 in the seed statements\nIncrement the column value by 1 in the recursive statements\nSpecify a limit for the value of the depth control column in the join condition of the recursive statements\nHere is an example that modiﬁes the previous recursive query that uses the WITH RECURSIVE clause of the employee table to limit the depth of the recursion to ﬁve cycles:\nWITH RECURSIVE temp_table (employee_number, depth) AS\n( SELECT root.employee_number, 0 AS depth\n  FROM employee root\n  WHERE root.manager_employee_number = 801\nUNION ALL\n  SELECT indirect.employee_number, direct.depth+1 AS newdepth\n  FROM temp_table direct, employee indirect\n  WHERE direct.employee_number = indirect.manager_employee_number\n  AND newdepth <= 5\n)\nSELECT * FROM temp_table ORDER BY employee_number;\nRelated Information\nFor more information about:\nRecursive queries, see the information about WITH RECURSIVE in Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nRecursive views, see the information about CREATE VIEW in Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nQuery and Workload Analysis Statements\nData Collection and Analysis\nTeradata provides the following SQL statements for collecting and analyzing query and data demographics and statistics:\nBEGIN QUERY LOGGING\nCOLLECT DEMOGRAPHICS\nCOLLECT STATISTICS\nDROP STATISTICS\nDUMP EXPLAIN\nEND QUERY LOGGING\nINITIATE INDEX ANALYSIS\nINITIATE PARTITION ANALYSIS\nINSERT EXPLAIN\nRESTART INDEX ANALYSIS\nSHOW QUERY LOGGING\nCollected data can be used in several ways, for example:\nBy the Optimizer, to produce the best query plans possible.\nTo populate user-deﬁned Query Capture Database (QCD) tables with data used by various utilities to analyze query workloads as part of the ongoing process to re-engineer the\ndatabase design process.\nIndex Analysis and T arget Level Emulation\nSQL Fundamentals\nPage 60 of 99Teradata also provides diagnostic statements that support the cost-based and sample-based components of the target level emulation facility used to emulate a production\nenvironment on a test system.\nSettings should be changed in the production environment only under the direction of Teradata Support Center personnel.\nRelated Information\nFor more information about:\nBEGIN QUERY LOGGING, END QUERY LOGGING, and SHOW QUERY LOGGING, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nOther query and workload analysis statements, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nHelp and Database Object Definition Tools\nTeradata SQL provides several powerful tools to get help about database object deﬁnitions and summaries of database object deﬁnition statement text.\nHELP  Statements\nVarious HELP statements return reports about the current column deﬁnitions for named database objects. The reports these statements return can be useful to database designers\nwho need to ﬁne tune index deﬁnitions, column deﬁnitions (for example, changing data typing to eliminate the necessity of ad hoc conversions), and so on.\nSHOW Statements\nMost SHOW statements return a CREATE statement indicating the last data deﬁnition statement performed against the named database object. Some SHOW statements, such as\nSHOW QUERY LOGGING, return other information. These statements are particularly useful for application developers who need to develop exact replicas of existing objects for\npurposes of testing new software.\nExample: Incompatible Characters in HELP  and SHOW Output\nConsider the following deﬁnition for a table named department:\nCREATE TABLE department, FALLBACK\n   (department_number SMALLINT\n   ,department_name CHAR(30) NOT NULL\n   ,budget_amount DECIMAL(10,2)\n   ,manager_employee_number INTEGER\n   )\n UNIQUE PRIMARY INDEX (department_number)\n,UNIQUE INDEX (department_name);\nTo get the attributes for the table, use the HELP TABLE statement:\nHELP TABLE department;\nThe HELP TABLE statement returns:\nColumn Name                    Type Comment\n------------------------------ ---- -------------------------\ndepartment_number              I2   ?\ndepartment_name                CF   ?\nbudget_amount                  D    ?\nmanager_employee_number        I    ?\nTo get the CREATE TABLE statement that deﬁnes the department table, use the SHOW TABLE statement:\nSHOW TABLE department;\nThe SHOW TABLE statement returns:\nCREATE SET TABLE TERADATA_EDUCATION.department, FALLBACK,\n   NO BEFORE JOURNAL,\n   NO AFTER JOURNAL,\n   CHECKSUM = DEFAULT\n   (department_number SMALLINT,\n    department_name CHAR(30) CHARACTER SET LATIN\n                         NOT CASESPECIFIC NOT NULL,\n    budget_amount DECIMAL(10,2),\n    manager_employee_number INTEGER)\nUNIQUE PRIMARY INDEX ( department_number )\nUNIQUE INDEX ( department_name );\nRelated Information\nFor more information about:\nSQL Fundamentals\nPage 61 of 99SQL HELP statements, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nSQL SHOW statements, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nSQL Data Handling\nThis section describes the fundamentals of data handling in Vantage.\nInvoking SQL Statements\nSQL provides several ways to invoke an executable SQL statement:\nInteractively from a terminal\nEmbedded within an application program\nDynamically performed from within an embedded application\nEmbedded within a stored procedure or external stored procedure\nExecutable SQL  Statements\nAn executable SQL statement performs an action. The action can be on data or on a transaction or some other entity at a higher level than raw data.\nSome examples of executable SQL statements include:\nSELECT\nCREATE TABLE\nCOMMIT\nCONNECT\nPREPARE\nMost executable SQL statements can be performed interactively from a terminal using an SQL query manager like BTEQ.\nThe following types of executable SQL commands cannot be performed interactively:\nCursor control and declaration statements\nDynamic SQL control statements\nStored procedure control statements and condition handlers\nConnection control statements\nSpecial forms of SQL statements such as SELECT ... INTO\nThese statements can only be used within an embedded SQL or stored procedure application.\nNonexecutable SQL  Statements\nA nonexecutable SQL statement is one that declares an SQL statement, object, or host or local variable to the preprocessor or stored procedure compiler. Nonexecutable SQL\nstatements are not processed during program execution.\nExamples of nonexecutable SQL statements for embedded SQL applications include:\nDECLARE CURSOR\nBEGIN DECLARE SECTION\nEND DECLARE SECTION\nEXEC SQL\nExamples of nonexecutable SQL statements for stored procedures include:\nDECLARE CURSOR\nDECLARE\nSQL Requests\nA request to Vantage can span any number of input lines. Vantage can receive and perform SQL statements that are:\nEmbedded in a client application program that is written in a procedural language.\nEmbedded in a stored procedure.\nEntered interactively through BTEQ interfaces.\nSubmitted in a BTEQ script as a batch job.\nSubmitted through other supported methods (such as CLIv2, ODBC, and JDBC).\nSubmitted from a C or C++ external stored procedure using CLIv2 or a Java external stored procedure using JDBC.\nTransactions\nA transaction is a logical unit of work where the statements nested within the transaction either execute successfully as a group or do not execute.\nTransaction Processing Mode\nYou can perform transaction processing in either of the following session modes:\nSQL Fundamentals\nPage 62 of 99ANSI\nIn ANSI session mode, transaction processing adheres to the rules deﬁned by the ANSI/ISO SQL speciﬁcation.\nTeradata\nIn Teradata session mode, transaction processing follows the rules deﬁned by Teradata prior to the emergence of the ANSI/ISO SQL standard.\nTo set the transaction processing mode, use the:\nSessionMode ﬁeld of the DBS Control Record\nBTEQ command SET SESSION TRANSACTION\nPreprocessor2 TRANSACT() option\nODBC SessionMode option in the .odbc.ini ﬁle\nJDBC TeraDataSource.setTransactMode() method\nRelated Information\nFor detailed information on statement and transaction processing, see Teradata Vantage™ - SQL Request and Transaction Processing, B035-1142.\nTransaction Processing in Teradata Session Mode\nA Teradata SQL transaction can be a single Teradata SQL statement, or a sequence of Teradata SQL statements, treated as a single unit of work.\nEach request is processed as one of the following transaction types:\nImplicit\nExplicit\nTwo-phase commit (2PC)\nImplicit T ransactions\nAn implicit transaction is a request that does not include the BEGIN TRANSACTION and END TRANSACTION statements. The implicit transaction starts and completes all within the\nSQL request: it is self-contained.\nAn implicit transaction can be a:\nSingle DML statement that affects one or more rows of one or more tables.\nMacro or trigger containing one or more statements.\nRequest containing multiple statements separated by SEMICOLON characters. SEMICOLON characters can appear anywhere in the input line. The Parser interprets a\nSEMICOLON at the end of an input line as the request terminator.\nDDL statements are not valid in a multistatement request and are therefore not valid in an implicit multistatement transaction.\nExplicit T ransactions\nIn Teradata session mode, an explicit transaction contains one or more statements enclosed by BEGIN TRANSACTION and END TRANSACTION statements. The ﬁrst BEGIN\nTRANSACTION initiates a transaction and the last END TRANSACTION terminates the transaction.\nWhen multiple statements are included in an explicit transaction, you can only specify a DDL statement if it is the last statement in the series.\n2PC Rules\n2PCprotocol is supported in Teradata session mode:\nA 2PC transaction contains one or more DML statements that affect multiple databases and are coordinated externally using the 2PC protocol.\nA DDL statement is not valid in a two-phase commit transaction.\nTransaction Processing in ANSI Session Mode\nIn ANSI mode, transactions are always implicitly started and explicitly closed.\nA transaction initiates when one of the following happens:\nThe ﬁrst SQL statement in a session executes.\nThe ﬁrst statement following the close of a transaction executes.\nThe COMMIT or ROLLBACK/ABORT statements close a transaction.\nIf a transaction includes a DDL statement, it must be the last statement in the transaction. DATABASE and SET SESSION are DDL statements.\nIf a session terminates with an open transaction, any effects of that transaction are rolled back.\nTwo-Phase Commit\nSessions in ANSI session mode do not support Two-Phase Commit (2PC). If an attempt is made to use the 2PC protocol in ANSI session mode, the Logon process aborts and an error\nreturns to the requestor.\nSQL Fundamentals\nPage 63 of 99Multistatement Requests\nAn atomic request containing more than one SQL statement, each terminated by a SEMICOLON character.\nANSI Compliance\nMultistatement requests are non-ANSI/ISO SQL:2011 standard.\nSyntax\nstatement  [;...] [;]\nRules and Restrictions\nThe database imposes restrictions on the use of multistatement requests:\nOnly one USING modiﬁer is permitted per request, so only one USING modiﬁer can be used per multistatement request.\nThis rule applies to interactive SQL only. Embedded SQL and stored procedures do not permit the USING modiﬁer.\nA multistatement request cannot include a DDL statement.\nHowever, a multistatement request can include one SET QUERY_BAND FOR TRANSACTION statement if it is the ﬁrst statement in the request.\nA multistatement request cannot include a CALL statement.\nThe keywords BEGIN REQUEST and END REQUEST must delimit a multistatement request in a stored procedure.\nPower of Multistatement Requests\nThe multistatement request is application-independent. It improves performance for a variety of applications that can package more than one SQL statement at a time. BTEQ, CLI, and\nthe SQL preprocessor all support multistatement requests.\nMultistatement requests improve system performance by reducing processing overhead. By performing a series of statements as one request, performance for the client, the Parser,\nand the Database Manager are all enhanced.\nBecause of this reduced overhead, using multistatement requests also decreases response time. A multistatement request that contains 10 SQL statements could be as much as 10\ntimes more efﬁcient than the 10 statements entered separately (depending on the types of statements submitted).\nImplicit Multistatement T ransaction\nIn Teradata session mode, a multistatement request is one form of implicit transaction. As such, the outcome of an implicit multistatement transaction is typically all-or-nothing. If one\nstatement in the request fails, the entire implicit transaction fails and the system rolls it back.\nStatement Independence for Simple INSERT s\nWhen a multistatement request includes only simple INSERT statements, a failure of one or more INSERTs does not cause the entire request to be rolled back. In these cases, errors\nare reported for the INSERT statements that failed, so those statements can be resubmitted. INSERT statements that completed successfully are not rolled back.\nThis behavior is limited to requests submitted directly to the database using an SQL INSERT multistatement request or submitted using a JDBC application request.\nParallel Step Processing\nThe database can perform some requests in parallel. This capability applies both to implicit transactions, such as macros and multistatement requests, and to Teradata-style\ntransactions explicitly deﬁned by BEGIN/END TRANSACTION statements.\nStatements in a multistatement request are broken down by the Parser into one or more steps that direct the execution performed by the AMPs. It is these steps, not the actual\nstatements, that are executed in parallel.\nA handshaking protocol between the PE and the AMP allows the AMP to determine when the PE can dispatch the next parallel step.\nUp to twenty parallel steps can be processed per request if channels are not required, such as a request with an equality constraint based on a primary index value. Up to ten\nchannels can be used for parallel processing when a request is not constrained to a primary index value.\nFor example, if an INSERT step and a DELETE step are allowed to run in parallel, the AMP informs the PE that the DELETE step has progressed to the point where the INSERT step will\nnot impact it adversely. This handshaking protocol also reduces the chance of a deadlock.\nParallel steps illustrates the following process:\n1. The statements in a multistatement request are broken down into a series of steps.\n2. The Optimizer determines which steps in the series can be executed in parallel.\n3. The steps are processed.\nEach step undergoes some preliminary processing before it is executed, such as placing locks on the objects involved. These preliminary processes are not performed in parallel with\nthe steps.\nSQL Fundamentals\nPage 64 of 99Parallel Steps\nRelated Information\nFor more information about multistatement requests or multistatement request processing, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146 or Teradata\nVantage™ - SQL Request and Transaction Processing, B035-1142.\nIterated Requests\nUsage\nIterated requests do not directly impact the syntax of SQL statements. They provide an efﬁcient way to execute the same single-statement DML operation on multiple data records, like\nthe way that ODBC applications execute parameterized statements for arrays of parameter values, for example.\nSeveral Teradata Tools and Utilities client tools and interfaces provide facilities to pack multiple data records in a single buffer with a single DML statement.\nFor example, suppose you use BTEQ to import rows of data into table ptable using the following INSERT statement and USING modiﬁer:\nUSING (pid INTEGER, pname CHAR(12))\nINSERT INTO ptable VALUES(:pid, :pname);\nTo repeat the request as many times as necessary to read up to 200 data records and pack a maximum of 100 data records with each request, precede the INSERT statement with the\nfollowing BTEQ command:\n.REPEAT RECS 200 PACK 100\nThe PACK option is ignored if the database being used does not support iterated requests or if the request that follows the REPEAT command is not a DML statement supported by\niterated requests.\nThe following tools and interfaces provide facilities that you can use to execute iterated requests.\nTool/Interface Facility\nCLIv2 for workstation-attached systems using_data_count ﬁeld in the DBCAREA data area\nCLIv2 for mainframe systems using-data-count ﬁeld in the DBCAREA data area\nODBC Parameter arrays\nJDBC type 4 driver Batch operations\nMicrosoft OLE DB Provider for ODBC and Teradata ODBC DriverParameter sets\nBTEQ\n.REPEAT command\n.SET PACK command\nRules\nThe iterated request must consist of a single DML statement from the following list:\nABORT\nDELETE (excluding the positioned form of DELETE)\nEXECUTE macro_name\nSQL Fundamentals\nPage 65 of 99The fully-expanded macro must be equivalent to a single DML statement that is qualiﬁed to be in an iterated request.\nINSERT\nMERGE\nROLLBACK\nSELECT\nUPDATE (including atomic UPSERT, but excluding the positioned form of UPDATE)\nThe DML statement may not be a CALL statement.\nThe DML statement must reference user-supplied input data, either as named ﬁelds in a USING modiﬁer or as '?' parameter markers in a parameterized request.\nIterated requests do not support the USING modiﬁer with the TOP n operator.\nAll the data records in a given request must use the same record layout. This restriction applies by necessity to requests where the record layout is given by a single USING modiﬁer in\nthe request text itself; but the restriction also applies to parameterized requests, where the request text has no USING modiﬁer and does not fully specify the input record.\nThe server processes the iterated request as if it were a single multistatement request, with each iteration and its response associated with a corresponding statement number.\nStatement Independence for Simple INSERT s\nWhen an iterated request includes only simple INSERT statements, a failure of one or more INSERTs does not cause the entire request to be rolled back. In these cases, errors are\nreported for the INSERT statements that failed, so those statements can be resubmitted. INSERT statements that completed successfully are not rolled back.\nThis behavior is limited to requests submitted directly to the database using an SQL INSERT multistatement request or submitted using a JDBC application request.\nRelated Information\nFor more information about:\nThe PACK option, see Usage.\nIterated request processing, see Teradata Vantage™ - SQL Request and Transaction Processing, B035-1142.\nWhich DML statements can be speciﬁed in an iterated request, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nCLIv2, see Teradata® Call-Level Interface Version 2 Reference for Workstation-Attached Systems, B035-2418.\nODBC parameter arrays, see ODBC Driver for Teradata® User Guide, B035-2526.\nJDBC driver batch operations, see Teradata JDBC Driver Reference, available at https://teradata-\ndocs.s3.amazonaws.com/doc/connectivity/jdbc/reference/current/frameset.html.\nMicrosoft OLE DB Provider for ODBC and ODBC Driver for Teradata, see ODBC Driver for Teradata® User Guide, B035-2526.\nBTEQ PACK command, see Basic Teradata® Query Reference, B035-2414.\nAborting SQL Requests\nIf an error is found in a request, that request is aborted. Normally, the entire transaction is not aborted. However, some failures will abort the entire transaction.\nA single statement or multistatement request that does not include the BEGIN TRANSACTION and END TRANSACTION statements is treated as an implicit transaction. If an error is\nfound in any statement in this type of request, then the entire transaction is aborted.\nAbort Processing\nFollowing are the steps for abort processing:\n1. Back out any changes made to the database as a result of any preceding statements in the transaction.\n2. Delete any associated spooled output.\n3. Release any associated locks.\n4. Bypass any remaining statements in the transaction.\nCompleted Requests\nA request is considered to have completed when either an End of Text character or the request terminator is encountered. The request terminator is a SEMICOLON character. It is the\nlast nonpad character on an input line.\nA request terminator is optional except when the request is embedded in an SQL macro or trigger or when it is entered through BTEQ.\nDynamic and Static SQL\nDynamic SQL is a method of invoking an SQL statement by compiling and performing it at runtime from within an embedded SQL application program or a stored procedure. The\nspeciﬁcation of data to be manipulated by the statement is also determined at runtime. Static SQL is, by default, any method of invoking an SQL statement that is not dynamic.\nANSI Compliance\nDynamic SQL is ANSI/ISO SQL:2011-compliant. The ANSI/ISO SQL standard does not deﬁne the expression static SQL, but relational database management commonly uses it to\ncontrast with the ANSI-deﬁned expression dynamic SQL.\nAd Hoc and Hard-Coded Invocation of SQL  Statements\nSQL Fundamentals\nPage 66 of 99Perhaps the best way to think of dynamic SQL is to contrast it with ad hoc SQL statements created and executed from a terminal and with preprogrammed SQL statements created by\nan application programmer and executed by an application program.\nIn the case of the ad hoc query, everything legal is available to the requester: choice of SQL statements and clauses, variables and their names, databases, tables, and columns to\nmanipulate, and literals.\nIn the case of the application programmer, the choices are made in advance and hard-coded into the source code of the application. Once the program is compiled, nothing can be\nchanged short of editing and recompiling the application.\nDynamic Invocation of SQL  Statements\nDynamic SQL offers a compromise between the extremes of ad hoc and hard-coded queries. By choosing to code dynamic SQL statements in the application, the programmer has the\nﬂexibility to allow an end user to select not only the variables to be manipulated at run time, but also the SQL statement to be executed.\nAs you might expect, the ﬂexibility that dynamic SQL offers a user is offset by more work and increased attention to detail on the part of the application programmer, who needs to set\nup additional dynamic SQL statements and manipulate information in the SQLDA to ensure a correct result.\nThis is done by ﬁrst preparing, or compiling, an SQL text string containing placeholder tokens at run time and then executing the prepared statement, allowing the application to\nprompt the user for values to be substituted for the placeholders.\nSQL Statements to Set Up and Invoke Dynamic SQL\nThe embedded SQL statements for preparing and executing an SQL statement dynamically are:\nPREPARE\nEXECUTE\nEXECUTE IMMEDIATE\nEXECUTE IMMEDIATE is a special form that combines PREPARE and EXECUTE into one statement. EXECUTE IMMEDIATE can only be used in the case where there are no input host\nvariables.\nRelated Information\nFor more information about:\nExamples of dynamic SQL code in C, COBOL, and PL/I, see Teradata® Preprocessor2 for Embedded SQL Programmer Guide, B035-2446.\nEmbedded SQL statements, see Teradata Vantage™ - SQL Stored Procedures and Embedded SQL, B035-1148.\nEXECUTE IMMEDIATE, see Teradata Vantage™ - SQL Stored Procedures and Embedded SQL, B035-1148.\nDynamic SQL in Stored Procedures\nStored procedures support of dynamic SQL statements is different from embedded SQL support.\nUse the following statement to set up and invoke dynamic SQL in a stored procedure:\nCALL DBC.SysExecSQL( string_expression )\nwhere string_expression is any valid string expression that builds an SQL statement.\nThe string expression consists of string literals, status variables, local variables, input (IN and INOUT) parameters, and for-loop aliases. Dynamic SQL statements are not validated at\ncompile time.\nThe resulting SQL statement cannot have status variables, local variables, parameters, for-loop aliases, or a USING or EXPLAIN modiﬁer.\nExample: Using Dynamic SQL  Within Stored Procedure Source T ext\nThe following example uses dynamic SQL within stored procedure source text:\nCREATE PROCEDURE new_sales_table( my_table VARCHAR(30),\n                                  my_database VARCHAR(30))\nBEGIN\n  DECLARE sales_columns VARCHAR(128)\n    DEFAULT '(item INTEGER, price DECIMAL(8,2), sold INTEGER)';\n  CALL DBC.SysExecSQL('CREATE TABLE ' || my_database ||\n                '.' || my_table || sales_columns);\nEND;\nA stored procedure can make any number of calls to SysExecSQL. The request text in the string expression can specify a multistatement request, but the call to SysExecSQL must be\ndelimited by BEGIN REQUEST and END REQUEST keywords.\nBecause the request text of dynamic SQL statements can vary from execution to execution, dynamic SQL provides more usability and conciseness to the stored procedure deﬁnition.\nRestrictions\nWhether the creator, owner, or invoker of the stored procedure must have appropriate privileges on the objects that the stored procedure accesses depends on whether the CREATE\nPROCEDURE statement includes the SQL SECURITY clause and which option the SQL SECURITY clause speciﬁes.\nSQL Fundamentals\nPage 67 of 99The following SQL statements cannot be speciﬁed as dynamic SQL in stored procedures:\nALTER PROCEDURE\nCALL\nCREATE PROCEDURE\nDATABASE\nEXPLAIN modiﬁer\nHELP\nOPEN\nPREPARE\nREPLACE PROCEDURE\nSELECT\nSET ROLE\nSET SESSION ACCOUNT\nSET SESSION COLLATION\nSET SESSION DATEFORM\nSET TIME ZONE\nSHOW\nCursor statements, including:\nCLOSE\nFETCH\nOPEN\nRelated Information\nFor rules and usage examples of dynamic SQL statements in stored procedures, see Teradata Vantage™ - SQL Stored Procedures and Embedded SQL, B035-1148.\nUsing SELECT With Dynamic SQL\nUnlike other executable SQL statements, SELECT returns information beyond statement responses and return codes to the requester.\nDESCRIBE Statement\nBecause the requesting application needs to know how much (if any) data will be returned by a dynamically prepared SELECT, you must use an additional SQL statement, DESCRIBE,\nto make the application aware of the demographics of the data to be returned by the SELECT statement.\nDESCRIBE writes this information to the SQLDA declared for the SELECT statement as follows.\nTHIS information … IS written to this ﬁeld of SQLDA  …\nnumber of values to be returned SQLN\ncolumn name or label of nth valueSQLVAR\n(nth row in the SQLVAR(n) array) column data type of nth value\ncolumn length of nth value\nGeneral Procedure\nAn application must use the following general procedure to set up, execute, and retrieve the results of a SELECT statement invoked as dynamic SQL.\n1. Declare a dynamic cursor for the SELECT in the form:\nDECLARE  cursor_name   CURSOR FOR  sql_statement_name\n2. Declare the SQLDA, preferably using an INCLUDE SQLDA statement.\n3. Build and PREPARE the SELECT statement.\n4. Issue a DESCRIBE statement in the form:\nDESCRIBE  sql_statement_name   INTO SQLDA \nDESCRIBE performs the following actions:\na. Interrogate the database for the demographics of the expected results.\nb. Write the addresses of the target variables to receive those results to the SQLDA.\nThis step is bypassed if any of the following occurs:\nThe request does not return any data.\nAn INTO clause was present in the PREPARE statement.\nThe statement returns known columns and the INTO clause is used on the corresponding FETCH statement.\nThe application code deﬁnes the SQLDA.\n5. Allocate storage for target variables to receive the returned data based on the demographics reported by DESCRIBE.\n6. Retrieve the result rows using the following SQL cursor control statements:\nOPEN cursor_name\nFETCH cursor_name USING DESCRIPTOR SQLDA\nCLOSE cursor_name\nSQL Fundamentals\nPage 68 of 99In this step, results tables are examined one row at a time using the selection cursor. This is because client programming languages do not support data in terms of\nsets, but only as individual records.\nRelated Information\nFor more information about the DESCRIBE statement, see Teradata Vantage™ - SQL Stored Procedures and Embedded SQL, B035-1148.\nEvent Processing Using Queue Tables\nVantage provides queue tables that you can use for event processing. Queue tables are base tables with ﬁrst-in-ﬁrst-out (FIFO) queue properties.\nWhen you create a queue table, you deﬁne a timestamp column. You can query the queue table to retrieve data from the row with the oldest timestamp.\nUsage\nAn application can perform peek, FIFO push, and FIFO pop operations on queue tables.\nTo perform a … Use the …\nFIFO push INSERT statement.\nFIFO pop SELECT AND CONSUME statement.\npeek SELECT statement.\nHere is an example of how an application can process events using queue tables:\nDeﬁne a trigger on a base table to insert a row into the queue table when the trigger ﬁres.\nFrom the application, submit a SELECT AND CONSUME statement that waits for data in the queue table.\nWhen data arrives in the queue table, the waiting SELECT AND CONSUME statement returns a result to the application, which processes the event. Additionally, the row is\ndeleted from the queue table.\nRelated Information\nFor more information about:\nCreating queue tables, see the CREATE/REPLACE TABLE statement in Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nSELECT AND CONSUME, see Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nManipulating Nulls\nNulls are neither values nor do they signify values. They represent the absence of value. A null is a place holder indicating that no value is present.\nYou cannot solve for the value of a null because, by deﬁnition, it has no value. For example, the expression NULL = NULL has no meaning and therefore can never be true. A query\nthat speciﬁes the predicate WHERE NULL = NULL is not valid because it can never be true. The meaning of the comparison it speciﬁes is not only unknown, but unknowable.\nThese properties make the use and interpretation of nulls in SQL problematic. The following sections outline the behavior of nulls for various SQL operations to help you to understand\nhow to use them in data manipulation statements and to interpret the results those statements affect.\nNulls and DateT ime and Interval Data\nA DateTime or Interval value is either atomically null or it is not null. For example, you cannot have an interval of YEAR TO MONTH in which YEAR is null and MONTH is not.\nRules for the Result of Expressions That Contain Nulls\nWhen any component of a value expression is null, then the result is null.\nThe result of a conditional expression that has a null component is unknown.\nIf an operand of any arithmetic operator (such as + or -) or function (such as ABS or SQRT) is null, then the result of the operation or function is null with the exception of\nZEROIFNULL. If the argument to ZEROIFNULL is NULL, then the result is 0.\nCOALESCE, a special shorthand variant of the CASE expression, returns NULL if all its arguments evaluate to null. Otherwise, COALESCE returns the value of the ﬁrst argument\nthat has a value that is not null.\nNulls and Comparison Operators\nIf either operand of a comparison operator is null, then the result is unknown. If either operand is the keyword NULL, an error is returned that recommends using IS NULL or IS NOT\nNULL instead. The following examples indicate this behavior.\n5 = NULL\n5 <> NULL\nNULL = NULL\nNULL <> NULL\n5 = NULL + 5\nIf the argument of the NOT operator is unknown, the result is also unknown. This translates to FALSE as a ﬁnal boolean result.\nInstead of using comparison operators, use the IS NULL operator to search for ﬁelds that contain nulls and the IS NOT NULL operator to search for ﬁelds that do not contain nulls.\nSQL Fundamentals\nPage 69 of 99Using IS NULL is different from using the comparison operator =. When you use an operator like =, you specify a comparison between values or value expressions, whereas when you\nuse the IS NULL operator, you specify an existence condition.\nRules for Nulls and CASE Expressions\nCASE and its related expressions COALESCE and NULLIF can return a null.\nNULL and null expressions are valid as the CASE test expression in a valued CASE expression.\nWhen testing for NULL, it is best to use a searched CASE expression using the IS NULL or IS NOT NULL operators in the WHEN clause.\nNULL and null expressions are valid as THEN clause conditions.\nExcluding Nulls\nTo exclude nulls from the results of a query, use the operator IS NOT NULL.\nFor example, to search for the names of all employees with a value other than null in the jobtitle column, enter the statement.\nSELECT name\nFROM employee\nWHERE jobtitle IS NOT NULL ;\nSearching for Nulls\nTo search for columns that contain nulls, use the operator IS NULL.\nThe IS NULL operator tests row data for the presence of nulls.\nFor example, to search for the names of all employees who have a null in the deptno column, you could enter the statement:\nSELECT name\nFROM employee\nWHERE deptno IS NULL ;\nThis query produces the names of all employees with a null in the deptno ﬁeld.\nSearching for Nulls and V alues that Are Not Null T ogether\nTo search for nulls and values that are not null in the same statement, the search condition for nulls must be separate from any other search conditions.\nFor example, to select the names of all employees with the job title of Vice Pres, Manager, or null, enter the following SELECT statement.\nSELECT name, jobtitle\nFROM employee\nWHERE jobtitle IN ('Manager', 'Vice Pres') OR jobtitle IS NULL ;\nIncluding NULL in the IN list has no effect because NULL never equals NULL or any value.\nNull Sorts as the Lowest or Highest V alue in a Collation\nWhen you use an ORDER BY clause to sort records, Vantage sorts null as the lowest or highest value.\nIf any row has a null in the column being grouped, then all rows having a null are placed into one group.\nNULL  and Unique Indexes\nFor unique indexes, Vantage treats nulls as if they are equal rather than unknown (and therefore false).\nFor single-column unique indexes, only one row may have null for the index value; otherwise a uniqueness violation error occurs.\nFor multicolumn unique indexes, no two rows can have nulls in the same columns of the index and also have values that are not null and that are equal in the other columns of the\nindex.\nFor example, consider a two-column index. Rows can occur with the following index values:\nValue of First Column in Index Value of Second Column in Index\n1 null\nnull 1\nnull null\nAn attempt to insert a row that matches any of these rows will result in a uniqueness violation.\nReplacing Nulls W ith V alues on Return to Client in Record Mode\nWhen Vantage returns information to a client system in record mode, nulls must be replaced with some value for the underlying column because client system languages do not\nrecognize nulls.\nSQL Fundamentals\nPage 70 of 99The following table shows the values returned for various column data types.\nData T ype Substitute V alue Returned for Null\nCHARACTER(n)\nDATE\nTIME\nTIMESTAMP\nINTERVALPad character (or n pad characters for CHARACTER(n), where n > 1)\nPERIOD(DATE) 8 binary zero bytes\nPERIOD(TIME [(n)]) 12 binary zero bytes\nPERIOD(TIME [(n)] WITH TIME ZONE) 16 binary zero bytes\nPERIOD(TIMESTAMP [(n)] [WITH TIME ZONE]) 0-length byte string\nBYTE[(n)] Binary zero byte if n omitted else n binary zero bytes\nVARBYTE(n) 0-length byte string\nVARCHARACTER(n) 0-length character string\nBIGINT\nINTEGER\nSMALLINT\nBYTEINT\nFLOAT\nDECIMAL\nREAL\nDOUBLE PRECISION\nNUMERIC0\nThe substitute values returned for nulls are not, by themselves, distinguishable from valid values that are not null. Data from CLI is normally accessed in IndicData mode, in which\nadditional identifying information that ﬂags nulls is returned to the client.\nBTEQ uses the identifying information, for example, to determine whether the values it receives are values or just aliases for nulls so it can properly report the results. BTEQ displays\nnulls as >?, which are not by themselves distinguishable from a CHAR or VARCHAR value of '?'.\nNulls and Aggregate Functions\nWith the important exception of COUNT(*), aggregate functions ignore nulls in their arguments. This treatment of nulls is very different from the way arithmetic operators and functions\ntreat them.\nThis behavior can result in apparent nontransitive anomalies. For example, if there are nulls in either column A or column B (or both), then the following expression is virtually always\ntrue.\nSUM(A) + (SUM B) <> SUM (A+B)\nIn other words, for the case of SUM, the result is never a simple iterated addition if there are nulls in the data being summed.\nThe only exception to this is the case in which the values for columns A and B are both null in the same rows, because in those cases the entire row is disregarded in the aggregation.\nThis is a trivial case that does not violate the general rule.\nThe same is true, the necessary changes being made, for all the aggregate functions except COUNT(*).\nIf this property of nulls presents a problem, you can always do either of the following workarounds, each of which produces the desired result of the aggregate computation SUM(A) +\nSUM(B) = SUM(A+B).\nAlways deﬁne NUMERIC columns as NOT NULL DEFAULT 0.\nUse the ZEROIFNULL function within the aggregate function to convert any nulls to zeros for the computation, for example\nSUM(ZEROIFNULL(x) + ZEROIFNULL(y))\nwhich produces the same result as this:\nSUM(ZEROIFNULL(x) + ZEROIFNULL(y)).\nCOUNT(*) includes nulls in its result.\nRANGE_N and CASE_N Functions\nNulls have special considerations in the RANGE_N and CASE_N functions.\nRelated Information\nSQL Fundamentals\nPage 71 of 99For more information about:\nHow to use the NULL keyword as a literal, see NULL Keyword as a Literal.\nRules on the result of expressions containing nulls, see Teradata Vantage™ - SQL Functions, Expressions, and Predicates, B035-1145.\nRules for nulls in CASE, NULLIF, and COALESCE expressions, see Teradata Vantage™ - SQL Functions, Expressions, and Predicates, B035-1145.\nCOUNT(*) including nulls in its result, see Teradata Vantage™ - SQL Functions, Expressions, and Predicates, B035-1145.\nNulls that have special considerations in the RANGE_N and CASE_N functions, see Teradata Vantage™ - SQL Functions, Expressions, and Predicates, B035-1145.\nSession Parameters\nThe following session parameters can be controlled with keywords or predeﬁned system variables.\nParameter Valid Keywords or System V ariables\nSQL Flagger ON\nOFF\nTransaction Mode ANSI (COMMIT)\nTeradata (BTET)\nSession Collation ASCII\nEBCDIC\nMULTINATIONAL\nHOST\nCHARSET_COLL\nJIS_COLL\nAccount and Priority Account and reprioritization. See Teradata® Viewpoint User Guide, B035-2206 and Teradata Vantage™ - Workload Management User Guide,\nB035-1197.\nDate Form ANSIDATE\nINTEGERDATE\nCharacter Set Indicates the character set being used by the client.\nYou can view site-installed client character sets from DBC.CharSetsV or DBC.CharTranslationsV.\nThe following client character sets are permanently enabled:\nASCII\nEBCDIC\nUTF-8\nUTF-16\nSQL Flagger\nWhen enabled, the SQL Flagger assists SQL programmers by notifying them of the use of non-ANSI and non-entry level ANSI/ISO SQL syntax.\nEnabling the SQL Flagger can be done regardless of whether you are in ANSI or Teradata session mode.\nTo set the SQL Flagger on or off for BTEQ, use the .SET SESSION command.\nTo set this level of ﬂagging … Set the ﬂag variable to this value …\nNone SQLFLAG NONE\nEntry level SQLFLAG ENTRY\nIntermediate level SQLFLAG INTERMEDIATE\nTo set the SQL Flagger on or off for embedded SQL, use the SQLCHECK or -sc and SQLFLAGGER or -sf options when you invoke the preprocessor.\nIf you are using SQL in other application programs, see the reference manual for that application for instructions on enabling the SQL Flagger.\nTransaction Mode\nYou can run transactions in either Teradata or ANSI session modes and these modes can be set or changed.\nTo set the transaction mode, use the .SET SESSION command in BTEQ.\nTo run transactions in this mode … Set the variable to this value …\nTeradata TRANSACTION BTET\nANSI TRANSACTION ANSI\nSQL Fundamentals\nPage 72 of 99If you are using SQL in other application programs, see the reference manual for that application for instructions on setting or changing the transaction mode.\nSession Collation\nCollation of character data is an important and complex option. Teradata provides several named collations. The MULTINATIONAL and CHARSET_COLL collations allow the system\nadministrator to provide collation sequences tailored to the needs of the site.\nThe collation for the session is determined at logon from the deﬁned default collation for the user. You can change your collation any number of times during the session using the SET\nSESSION COLLATION statement, but you cannot change your default logon in this way.\nYour default collation is assigned via the COLLATION option of the CREATE USER or MODIFY USER statement. This has no effect on any current session, only new logons.\nEach named collation can be CASESPECIFIC or NOT CASESPECIFIC. NOT CASESPECIFIC collates lowercase data as if it were converted to uppercase before the named collation is\napplied.\nCollation Name Description\nASCII Character data is collated in the order it would appear if converted for an ASCII session, and a binary sort performed.\nEBCDIC Character data is collated in the order it would appear if converted for an EBCDIC session, and a binary sort performed.\nMULTINATIONAL The default MULTINATIONAL collation is a two-level collation based on the Unicode collation standard.\nYour system administrator can redeﬁne this collation to any two-level collation of characters in the LATIN repertoire.\nFor backward compatibility, the following are true:\nMULTINATIONAL collation of KANJI1 data is single level.\nThe system administrator can redeﬁne single byte character collation.\nThis deﬁnition is not compatible with MULTINATIONAL collation of non-KANJI1 data. CHARSET_COLL collation is usually a better solution\nfor KANJI1 data.\nHOST The default. HOST collation defaults are:\nEBCDIC collation for mainframe systems.\nASCII collation for all others.\nCHARSET_COLL Character data is collated in the order it would appear if converted to the current client character set and then sorted in binary order.\nCHARSET_COLL collation is a system administrator-deﬁned collation.\nJIS_COLL Character data is collated based on the Japanese Industrial Standards (JIS).\nJIS characters collate in the following order:\n1. JIS X 0201-deﬁned characters in standard order\n2. JIS X 0208-deﬁned characters in standard order\n3. JIS X 0212-deﬁned characters in standard order\n4. KanjiEBCDIC-deﬁned characters not deﬁned in JIS X 0201, JIS X 0208, or JIS X 0212 in standard order\n5. All remaining characters in Unicode standard order\nAccount and Priority\nYou can dynamically downgrade or upgrade the priority for your account.\nPriorities can be downgraded or upgraded at either the session or the request level. See Teradata® Viewpoint User Guide, B035-2206 and Teradata Vantage™ - Workload\nManagement User Guide, B035-1197.\nDate Form\nYou can change the format in which DATE data is imported or exported in your current session.\nDATE data can be set to be treated either using the ANSI date format (DATEFORM=ANSIDATE) or using the Teradata date format (DATEFORM=INTEGERDATE).\nSetting the Client Character Set\nTo set the client character set, use one of the following:\nFrom BTEQ, use the BTEQ [.] SET SESSION CHARSET 'name' command.\nIn a CLIv2 application, call CHARSET name.\nIn the URL for selecting a Teradata JDBC driver connection to a database, use the CHARSET=name database connection parameter.\nwhere the ‘name’ or name value is ASCII, EBCDIC, UTF-8, UTF-16, or a name assigned to the translation codes that deﬁne an available character set.\nIf not explicitly requested, the session default is the character set associated with the logon client. This is either the standard client default, or the character set assigned to the client by\nthe database administrator.\nSQL Fundamentals\nPage 73 of 99HELP  SESSION\nThe HELP SESSION statement identiﬁes attributes in effect for the current session, including:\nTransaction mode\nCharacter set\nCollation sequence\nDate form\nQueryband\nRelated Information\nUsing the SQL Flagger, see Using the SQL Flagger.\nCharacter sets, see International Character Set Support, B035-1125.\nTransaction semantics, see Teradata Vantage™ - SQL Request and Transaction Processing, B035-1142.\nMULTINATIONAL, see the information about the ORDER BY clause in Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nInformation on setting up the MULTINATIONAL collation sequence, see International Character Set Support, B035-1125.\nFor examples of the following statements, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nSET SESSION COLLATION\nSET SESSION ACCOUNT\nSET SESSION DATEFORM\nHELP SESSION\nSession Management\nEach session is logged on and off via calls to CLIv2 routines or through ODBC or JDBC, which offer a one-step logon-connect function.\nSessions are internally managed by dividing the session control functions into a series of single small steps that are executed in sequence to implement multithreaded tasking. This\nprovides concurrent processing of multiple logon and logoff events, which can be any combination of individual users, and one or more concurrent sessions established by one or\nmore users and applications.\nOnce connected and active, a session can be viewed as a work stream consisting of a series of requests between the client and server.\nSession Pools\nYou can establish session pools, in which an application logs on to Vantage and establishes a pooled connection, so users can use application functions that access the database\nwithout the need to log on individually. This capability is particularly advantageous for transaction processing in which interaction with the database consists of many single, short\ntransactions.\nThe database identiﬁes a session with a session number, the username of the initiating (application) user, and the logical host identiﬁcation number of the connection TDP.\nSession Reserve\nOn a mainframe client, use the ENABLE SESSION RESERVE command from Teradata Director Program to reserve session capacity in the event of a PE failure. To release reserved\nsession capacity, use the DISABLE SESSION RESERVE command.\nSession Control\nThe major functions of session control are session logon and logoff.\nUpon receiving a session request, the logon function veriﬁes authorization and returns a yes or no response to the client.\nThe logoff function terminates any ongoing activity and deletes the session context.\nTrusted Sessions\nApplications that use connection pooling can be conﬁgured to use trusted sessions, asserting individual end user identities and roles to manage privileges and audit access.\nRequests and Responses\nRequests are sent to a server to initiate an action. Responses are sent by a server to reﬂect the results of that action. Both requests and responses are associated with an established\nsession.\nA request consists of the following components:\nOne or more Teradata SQL statements\nControl information\nOptional USING data\nIf any operation speciﬁed by an initiating request fails, the request is backed out, along with any change that was made to the database. In this case, a failure response is returned to\nthe application.\nRelated Information\nSQL Fundamentals\nPage 74 of 99For more information about:\nSESSION RESERVE, see Teradata® Director Program Reference, B035-2416.\nTrusted sessions, see Teradata Vantage™ - Analytics Database Security Administration, B035-1100.\nReturn Codes\nSQL return codes provide information about the status of a completed executable SQL DML statement.\nStatus V ariables for Receiving SQL  Return Codes\nANSI/ISO SQL deﬁnes two status variables for receiving return codes:\nSQLSTATE\nSQLCODE\nSQLCODE is not ANSI/ISO SQL-compliant. The ANSI/ISO SQL-92 standard explicitly deprecates SQLCODE, and the ANSI/ISO SQL-99 standard does not deﬁne SQLCODE. The\nANSI/ISO SQL committee recommends that new applications use SQLSTATE in place of SQLCODE.\nThe database deﬁnes a third status variable for receiving the number of rows affected by an SQL statement in a stored procedure:\nACTIVITY_COUNT\nTeradata SQL deﬁnes a non-ANSI/ISO SQL Communications Area (SQLCA) that also has a ﬁeld named SQLCODE for receiving return codes.\nException and Completion Conditions\nANSI/ISO SQL deﬁnes two categories of conditions that issue return codes:\nException conditions\nCompletion conditions\nException Conditions\nAn exception condition indicates a statement failure.\nA statement that raises an exception condition does nothing more than return that exception condition to the application.\nThere are as many exception condition return codes as there are speciﬁc exception conditions.\nCompletion Conditions\nA completion condition indicates statement success.\nThere are three categories of completion conditions:\nSuccessful completion\nWarnings\nNo data found\nA statement that raises a completion condition can take further action such as querying the database and returning results to the requesting application, updating the database,\ninitiating an SQL transaction, and so on.\nCompletion Condition SQLST ATE Return Code SQLCODE Return Code\nSuccess '00000' 0\nWarning '01901' 901\n'01800' to '01841' 901\n'01004' 902\nNo data found '02000' 100\nReturn Codes for Stored Procedures\nThe return code values are different in the case of SQL control statements in stored procedures.\nThe return codes for stored procedures appear in the following table.\nCompletion Condition SQLST ATE Return Code SQLCODE Return Code\nSuccessful completion '00000' 0\nWarning SQLSTATE value corresponding to the warning code.Database warning code.\nNo data found or any other Exception SQLSTATE value corresponding to the error code. Database error code.\nSQL Fundamentals\nPage 75 of 99How an Application Uses SQL  Return Codes\nAn application program or stored procedure tests the status of a completed executable SQL statement to determine its status.\nCondition Action\nSuccessful completion None.\nWarning The statement execution continues.\nIf a warning condition handler is deﬁned in the application, the handler executes.\nNo data found or any other exception Whatever appropriate action is required by the exception.\nIf an EXIT handler is deﬁned for the exception, the statement execution terminates.\nIf a CONTINUE handler is deﬁned, execution continues after the remedial action.\nRelated Information\nFor more information about:\nException conditions, see Failure Response and Error Response (ANSI Session Mode Only).\nCompletion conditions, see Statement Responses, Success Response and Warning Response.\nSQLSTATE, SQLCODE, or ACTIVITY_COUNT, see the information about result code variables in Teradata Vantage™ - SQL Stored Procedures and Embedded SQL, B035-1148.\nSQLCA, see Teradata Vantage™ - SQL Stored Procedures and Embedded SQL, B035-1148.\nStatement Responses\nResponse T ypes\nThe database responds to an SQL request with one of the following condition responses:\nSuccess response, with optional warning\nFailure response\nError response (ANSI session mode only)\nDepending on the type of statement, the database also responds with one or more rows of data.\nMultistatement Responses\nA response to a request that contains more than one statement, such as a macro, is not returned to the client until all statements in the request are successfully executed.\nReturning a Response\nThe manner in which the response is returned depends on the interface that is being used.\nFor example, if an application is using a language preprocessor, then the activity count, warning code, error code, and ﬁelds from a selected row are returned directly to the program\nthrough its appropriately declared variables. If the application is a stored procedure, then the activity count is returned directly in the ACTIVITY_COUNT status variable.\nIf you are using BTEQ, then a success, error, or failure response is displayed automatically.\nResponse Condition Codes\nSQL statements also return condition codes that are useful for handling errors and warnings in embedded SQL and stored procedure applications.\nFor information about SQL response condition codes, see the following in Teradata Vantage™ - SQL Stored Procedures and Embedded SQL, B035-1148:\nSQLSTATE\nSQLCODE\nACTIVITY_COUNT\nSuccess Response\nA success response contains an activity count that indicates the total number of rows involved in the result.\nFor example, the activity count for a SELECT statement is the total number of rows selected for the response. For a SELECT, CALL (when the stored procedure or external stored\nprocedure creates result sets), COMMENT, or ECHO statement, the activity count is followed by the data that completes the response.\nAn activity count is meaningful for statements that return a result set, for example:\nSELECT\nINSERT\nUPDATE\nDELETE\nSQL Fundamentals\nPage 76 of 99HELP\nMERGE\nSHOW\nEXPLAIN\nCREATE PROCEDURE\nREPLACE PROCEDURE\nCALL (when the stored procedure or external stored procedure creates result sets)\nFor other SQL statements, activity count is meaningless.\nExample: Using an Interactive SELECT Statement\nThe following interactive SELECT statement returns the successful response message.\nSELECT AVG(f1)\nFROM Inventory;\nResult:\n*** Query completed. One row found. One column returned.\n*** Total elapsed time was 1 second.\nAverage(f1)\n-----------\n         14\nWarning Response\nA success or OK response with a warning indicates either that an anomaly has occurred or informs the user about the anomaly and indicates how it can be important to the\ninterpretation of the results returned.\nExample: Returning Message about Nulls\nAssume the current session is running in ANSI session mode.\nIf nulls are included in the data for column f1, then the following interactive query returns the successful response message with a warning about the nulls.\nSELECT AVG(f1) FROM Inventory;\nResult:\n *** Query completed. One row found. One column returned.\n *** Warning: 2892 Null value eliminated in set function.\n *** Total elapsed time was 1 second.\nAverage(f1)\n-----------\n         14\nThis warning response is not generated if the session is running in Teradata session mode.\nError Response (ANSI Session Mode Only)\nAn error response occurs when a query anomaly is severe enough to prevent the correct processing of the request.\nIn ANSI session mode, an error for a request causes the request to rollback, and not the entire transaction.\nExample: Returning an Error Message\nThe following command returns the error message immediately following.\n.SET SESSION TRANS ANSI;\nResult:\n *** Error: You must not be logged on .logoff to change the SQLFLAG\nor TRANSACTION settings.\nExample: Returning Error Messages in ANSI Mode\nAssume that the session is running in ANSI session mode, and the following table is deﬁned:\nCREATE MULTISET TABLE inv, FALLBACK,\n   NO BEFORE JOURNAL,\n   NO AFTER JOURNAL\n   (\n   item INTEGER CHECK ((item >=10) AND (item <= 20) ))\nPRIMARY INDEX (item);\nSQL Fundamentals\nPage 77 of 99You insert a value of 12 into the item column of the inv table.\nThis is valid because the deﬁned integer check speciﬁes that any integer between 10 and 20 (inclusive) is valid.\nINSERT INTO inv (12);\nThe following results message returns.\n*** Insert completed. One row added....\nYou insert a value of 9 into the item column of the inv table.\nThis is not valid because the deﬁned integer check speciﬁes that any integer with a value less than 10 is not valid.\nINSERT INTO inv (9);\nThe following error response returns:\n*** Error 5317 Check constraint violation: Check error in field\ninv.item.\nYou commit the current transaction:\nCOMMIT;\nThe following results message returns:\n*** COMMIT done. ...\nYou select all rows from the inv table:\nSELECT * FROM inv;\nThe following results message returns:\n*** Query completed. One row found. One column returned.\n   item\n-------\n     12\nFailure Response\nA failure response is a severe error. The response includes a statement number, an error code, and an associated text string describing the cause of the failure.\nTeradata Session Mode\nIn Teradata session mode, a failure causes the system to roll back the entire transaction.\nIf one statement in a macro fails, a single failure response is returned to the client, and the results of any previous statements in the transaction are backed out.\nANSI Session Mode\nIn ANSI session mode, a failure causes the system to roll back the entire transaction, for example, when the current request:\nResults in a deadlock\nPerforms a DDL statement that aborts\nExecutes an explicit ROLLBACK or ABORT statement\nExample: Returning Error Messages When Using a SELECT Statement\nThe following SELECT statement\nSELECT * FROM Inventory:;\nin BTEQ, returns the failure response message:\n*** Failure 3706 Syntax error: expected something between the word\n 'Inventory' and ':'.\n                Statement# 1, Info =20\n*** Total elapsed time was 1 second.\nExample: Returning Error Messages in ANSI Mode\nAssume that the session is running in ANSI session mode, and the following table is deﬁned:\nCREATE MULTISET TABLE inv, FALLBACK,\n   NO BEFORE JOURNAL,\n   NO AFTER JOURNAL\n   (\nSQL Fundamentals\nPage 78 of 99   item INTEGER CHECK ((item >=10) AND (item <= 20) ))\nPRIMARY INDEX (item);\nYou insert a value of 12 into the item column of the inv table.\nThis is valid because the deﬁned integer check speciﬁes that any integer between 10 and 20 (inclusive) is valid.\nINSERT INTO inv (12);\nThe following results message returns.\n*** Insert completed. One row added....\nYou commit the current transaction:\nCOMMIT;\nThe following results message returns:\n*** COMMIT done. ...\nYou insert a valid value of 15 into the item column of the inv table:\nINSERT INTO inv (15);\nThe following results message returns.\n*** Insert completed. One row added....\nYou can use the ABORT statement to cause the system to roll back the transaction:\nABORT;\nThe following failure message returns:\n*** Failure 3514 User-generated transaction ABORT.\n                Statement# 1, Info =0\nYou select all rows from the inv table:\nSELECT * FROM inv;\nThe following results message returns:\n*** Query completed. One row found. One column returned.\n   item\n-------\n     12\nQuery Processing\nQuery Processing.\nQuery Processing\nThis section discusses query processing, including single AMP requests and all AMP requests, and table access methods available to the Optimizer.\nQueries and AMPs\nAn SQL query, which includes DELETE, INSERT, MERGE, and UPDATE as well as SELECT, can affect one AMP, several AMPs, or all AMPs in the conﬁguration.\nIF a query … THEN …\n(involving a single table) uses a unique primary index (UPI) the row hash can be used to identify a single AMP.\nAt most one row can be returned.\n(involving a single table) uses a nonunique primary index (NUPI) the row hash can be used to identify a single AMP.\nAny number of rows can be returned.\nuses a unique secondary index (USI) one or two AMPs are affected (one AMP if the subtable and base table are on the same AMP).\nAt most one row can be returned.\nuses a nonunique secondary index (NUSI) if the table has a partitioned primary index (PPI) and the NUSI is the same column set as a NUPI, the query\naffects one AMP.\nOtherwise, all AMPs take part in the operation and any number of rows can be returned.\nSQL Fundamentals\nPage 79 of 99The SELECT statements in subsequent examples reference the following table data.\nEmployee  \nEmployee NumberManager Employee\nNumberDept. NumberJob Code Last Name First Name Hire Date Birth Date Salary Amount\nPK/UPI FK FK FK\n1006 1019 301 312101 Stein John 961005 631015 2945000\n1008 1019 301 312102 Kanieski Carol 970201 680517 2925000\n1005 0801 403 431100 Ryan Loretta 1061015 650910 3120000\n1004 1003 401 412101 Johnson Darlene 1061015 760423 3630000\n1007 1005 403 432101 Villegas Arnando 1050102 770131 4970000\n1003 0801 401 411100 Trader James 960731 670619 3755000\n1016 0801 302 321100 Rogers Nora 980310 690904 5650000\n1012 1005 403 432101 Hopkins Paulene 970315 720218 3790000\n1019 0801 301 311100 Kubic Ron 980801 721211 5770000\n1023 1017 501 512101 Rabbit Peter 1040301 621029 2650000\n1083 0801 619 414221 Kimble George 1010312 810330 3620000\n1017 0801 501 511100 Runyon Irene 980501 611110 6600000\n1001 1003 401 412101 Hoover William 1010818 700114 2552500\nThe meanings of the abbreviations are as follows.\nAbbreviation Meaning\nPK Primary Key\nFK Foreign Key\nUPI Unique Primary Index\nSingle AMP  Request\nAssume that a PE receives the following SELECT statement:\nSELECT last_name\nFROM Employee\nWHERE employee_number = 1008;\nBecause a unique primary index value is used as the search condition (the column employee_number is the primary index for the Employee table), PE1 generates a single AMP step\nrequesting the row for employee 1008. The AMP step, along with the PE identiﬁcation, is put into a message, and sent via the BYNET to the relevant AMP (processor).\nFlow Diagram of a Single AMP  Request\nOnly one BYNET is shown to simplify the illustration.\nSQL Fundamentals\nPage 80 of 99\nAssuming that AMP2 has the row, it accepts the message. As shown in the next diagram, AMP2 retrieves the row from disk, includes the row and the PE identiﬁcation in a return\nmessage, and sends the message back to PE1 via the BYNET. PE1 accepts the message and returns the response row to the requesting application.\nFlow Diagram of a Single AMP  Response to Requesting PE\nThe following diagram illustrates a single AMP request with partition elimination.\nAll AMP  Request\nAssume PE1 receives a SELECT statement that speciﬁes a range of primary index values as a search condition as shown in the following example:\nSELECT last_name, employee_number\nFROM employee\nWHERE employee_number BETWEEEN 1001 AND 1010\nORDER BY last_name;\nIn this case, each value hashes differently, and all AMPs must search for the qualifying rows.\nPE1 ﬁrst parses the request and creates the following AMP steps:\nRetrieve rows between 1001 and 1010\nSort ascending on last_name\nMerge the sorted rows to form the answer set\nPE1 then builds a message for each AMP step and puts that message onto the BYNET. Typically, each AMP step is completed before the next one begins; note, however, that some\nqueries can generate parallel steps.\nWhen PE1 puts the message for the ﬁrst AMP step on the BYNET, that message is broadcast to all processors as illustrated in the following diagram.\nSQL Fundamentals\nPage 81 of 99\nThe process is:\n1. All AMPs accept the message, but the PEs do not.\n2. Each AMP checks for qualifying rows on its disk storage units.\n3. If any qualifying rows are found, the data in the requested columns is converted to the client format and copied to a spool ﬁle.\n4. Each AMP completes the step, whether rows were found or not, and puts a completion message on the BYNET.\nThe completion messages ﬂow across the BYNET to PE1.\n5. When all AMPs have returned a completion message, PE1 transmits a message containing AMP Step 2 to the BYNET.\nUpon receipt of Step 2, the AMPs sort their individual answer sets into ascending sequence by last_name, as illustrated in the following diagram.\nIf partitioned on employee_number, the scan may be limited to a few partitions based on partition elimination.\n6. Each AMP sorts its answer set, then puts a completion message on the BYNET.\n7. When PE1 has received all completion messages for Step 2, it sends a message containing AMP Step 3.\n8. Upon receipt of Step 3, each AMP copies the ﬁrst block from its sorted spool to the BYNET.\nBecause there can be multiple AMPs on a single node, each node might be required to handle sort spools from multiple AMPs (see the following diagram).\nSQL Fundamentals\nPage 82 of 99\n9. Nodes that contain multiple AMPs must ﬁrst perform an intermediate sort of the spools generated by each of the local AMPs.\nWhen the local sort is complete on each node, the lowest sorting row from each node is sent over the BYNET to PE1. From this point on, PE1 acts as the Merge coordinator\namong all the participating nodes.\n10. The Merge continues with PE1 building a globally sorted buffer.\nWhen this buffer ﬁlls, PE1 forwards it to the application and begins building subsequent buffers.\n11. When a participant node has exhausted its sort spool, it sends a Done message to PE1.\nThis causes PE1 to prune this node from the set of Merge participants.\nWhen there are no remaining Merge participants, PE1 sends the ﬁnal buffer to the application along with an End Of File message.\nPartition Elimination\nA PPI can increase query efﬁciency through partition elimination, where partitions can automatically be skipped because they cannot contain qualifying rows.\nThe database supports several types of partition elimination.\nType Description\nStatic Based on constant conditions such as equality or inequality on the partitioning\ncolumns.\nDynamic The partitions to eliminate cannot be determined until the query is executed and the\ndata is scanned.\nDelayed Occurs with conditions comparing a partitioning column to a USING variable or built-in\nfunction such as CURRENT_DATE, where the Optimizer builds a somewhat\ngeneralized plan for the query but delays partition elimination until speciﬁc values of\nUSING variables and built-in functions are known.\nThe degree of partition elimination depends on the:\nPartitioning expressions for the primary index of the table\nConditions in the query\nAbility of the Optimizer to detect partition elimination\nIt is not always required that all values of the partitioning columns be speciﬁed in a query to have partition elimination occur.\nIF a query … THEN …\nspeciﬁes values for all the primary index columns the AMP where the rows reside can be determined and only a single AMP is accessed.\nIf conditions are not speciﬁed on the partitioning columns, each partition can be probed to ﬁnd the\nrows based on the hash value.\nSQL Fundamentals\nPage 83 of 99IF a query … THEN …\nIf conditions are also speciﬁed on the partitioning columns, partition elimination may reduce the\nnumber of partitions to be probed on that AMP.\ndoes not specify the values for all the primary index columns an all-AMP full ﬁle scan is required for a table with an NPPI.\nHowever, with a PPI, if conditions are speciﬁed on the partitioning columns, partition elimination may reduce\nan all-AMP full ﬁle scan to an all-AMP scan of only the non-eliminated partitions.\nSingle AMP  Request W ith Partition Elimination\nIf a SELECT speciﬁes values for all the primary index columns, the AMP where the rows reside can be determined and only a single AMP is accessed.\nIf conditions are also speciﬁed on the partitioning columns, partition elimination may reduce the number of partitions to be probed on that AMP.\nSuppose the Employee table is deﬁned with a single-level PPI where the partitioning column is dept_number.\nAssume that a PE receives the following SELECT statement:\nSELECT last_name\nFROM Employee\nWHERE employee_number = 1023\nAND dept_number = 501;\nThe following ﬂow diagram illustrates this process.\nThe AMP Step includes the list of partitions (in this case, P3) to access. Partition elimination (in this case, static partition elimination) reduces access to the partitions that satisfy the\nquery requirements. In each partition in the list (in this case, only P3), look for rows with a given row hash value of the PI.\nPartition elimination is similar for the Employee table with a multilevel PPI where one partitioning expression uses the dept_number column and another partitioning expression uses the\nhire_date column.\nAssume that a PE receives the following SELECT statement:\nSELECT last_name\nFROM Employee\nWHERE employee_number = 1023\nAND dept_number = 501\nAND hire_date BETWEEN DATE '2006-01-01' AND DATE '2006-12-31'\nThe following ﬂow diagram illustrates this process.\nSQL Fundamentals\nPage 84 of 99\nNo one was hired in department number 401 in 2006, so partition P4 is empty.\nRelated Information\nFor more information on partition elimination, see Teradata Vantage™ - Database Design, B035-1094.\nTable Access\nVantage uses indexes and partitions to access the rows of a table. If indexed or partitioned access is not suitable for a query, or if a query accesses a NoPI table that does not have an\nindex deﬁned on it, the result is a full-table scan.\nAccess Methods\nThe following table access methods are available to the Optimizer:\nUnique Primary Index\nUnique Partitioned Primary Index\nNonunique Primary Index\nNonunique Partitioned Primary Index\nUnique Secondary Index\nNonunique Secondary Index\nJoin Index\nHash Index\nFull-Table Scan\nPartition Scan\nEffects of Conditions in WHERE Clause\nFor a query on a table that has an index deﬁned on it, the predicates or conditions that appear in the WHERE clause of the query determine whether the system can use row hashing,\nor do a table scan with partition elimination, or whether it must do a full-table scan.\nSQL Fundamentals\nPage 85 of 99The following functions are applied to rows identiﬁed by the WHERE clause, and have no effect on the selection of rows from the base table:\nGROUP BY\nHAVING\nINTERSECT\nMINUS/EXCEPT\nORDER BY\nQUALIFY\nSAMPLE\nUNION\nWITH ... BY\nWITH\nStatements that specify any of the following WHERE clause conditions result in full-table scans (FTS). If the table has a PPI, partition elimination might reduce the FTS access to only the\naffected partitions.\nnonequality comparisons\ncolumn_name IS NOT NULL\ncolumn_name NOT IN (explicit list of values)\ncolumn_name NOT IN (subquery)\ncolumn_name BETWEEN ... AND\ncondition_1 OR condition_2\nNOT condition_1\ncolumn_name LIKE\ncolumn_1 || column_2 = value\ntable1.column_x = table1.column_y\ntable1.column_x [computation] = value\ntable1.column_x [computation] - table1.column_y\nINDEX (column_name)\nSUBSTR (column_name)\nSUM\nMIN\nMAX\nAVG\nDISTINCT\nCOUNT\nANY\nALL\nmissing WHERE clause\nThe type of table access that the system uses when statements specify any of the following WHERE clause conditions depends on whether the column or columns are indexed, the\ntype of index, and its selectivity:\ncolumn_name = value or constant expression\ncolumn_name IS NULL\ncolumn_name IN (explicit list of values)\ncolumn_name IN (subquery)\ncondition_1 AND condition_2\ndifferent data types\ntable1.column_x = table2.column_x\nIn summary, a query inﬂuences processing choices:\nA full-table scan (possibly with partition elimination if the table has a PPI) is required if the query includes an implicit range of values, such as in the following WHERE examples.\nWhen a small BETWEEN range is speciﬁed, the Optimizer can use row hashing rather than a full-table scan.\n... WHERE column_name [BETWEEN <, >, <>, <=, >=]\n... WHERE column_name [NOT] IN (SELECT...)\n... WHERE column_name NOT IN (val1, val2 [,val3])\nRow hashing can be used if the query includes an explicit value, as shown in the following WHERE examples:\n... WHERE column_name = val\n... WHERE column_name IN (val1, val2, [,val3])\nRelated Information\nFor more information about:\nThe efﬁciency, number of AMPs used, and the number of rows accessed by all table access methods, see Teradata Vantage™ - Database Design, B035-1094.\nStrengths and weaknesses of table access methods, see Teradata Vantage™ - Database Introduction, B035-1091.\nFull-table scans, see Full-Table Scans.\nFull-Table Scans\nSQL Fundamentals\nPage 86 of 99A full-table scan is a retrieval mechanism that touches all rows in a table.\nThe database always uses a full-table scan to access the data of a table if a query:\nAccesses a NoPI table that does not have an index deﬁned on it\nDoes not specify a WHERE clause\nEven when results are qualiﬁed using a WHERE clause, indexed or partitioned access may not be suitable for a query, and a full-table scan may result.\nA full-table scan is always an all-AMP operation, and should be avoided when possible. Full-table scans may generate spool ﬁles that can have as many rows as the base table.\nFull-table scans are not something to fear, however. The architecture used to make a full-table scan an efﬁcient procedure, and optimization is scalable based on the number of AMPs\ndeﬁned for the system. The sorts of unplanned, ad hoc queries that characterize the data warehouse process, and that often are not supported by indexes, perform very effectively for\nthe database using full-table scans.\nAccessing Rows in a Full-T able Scan\nBecause full-table scans necessarily touch every row on every AMP, they do not use the following mechanisms for locating rows:\nHashing algorithm and hash map\nPrimary indexes\nSecondary indexes or their subtables\nPartitioning\nInstead, a full-table scan uses the Master Index and Cylinder Index ﬁle system tables to locate each data block. Each row within a data block is located by a forward scan.\nBecause rows from different tables are never mixed within the same data block and because rows never span blocks, an AMP can scan up to 128K bytes of the table on each block\nread, making a full-table scan a very efﬁcient operation. Data block read-ahead and cylinder reads can also increase efﬁciency.\nRelated Information\nFor more information about:\nFull-table scans, see Teradata Vantage™ - Database Design, B035-1094.\nCylinder reads, see Teradata Vantage™ - Database Administration, B035-1093.\nEnabling data block read-ahead operations, see DBS Control Utility in Teradata Vantage™ - Database Utilities, B035-1102.\nCollecting Statistics\nThe COLLECT STATISTICS (Optimizer form) statement collects demographic data for one or more columns of a base table, hash index, or join index, computes a statistical proﬁle of\nthe collected data, and stores the synopsis in the Data Dictionary.\nThe Optimizer uses the synopsis data when it generates its table access and join plans.\nUsage\nYou should collect statistics on newly created, empty data tables. An empty collection deﬁnes the columns, indexes, and synoptic data structure for loaded collections. You can easily\ncollect statistics again after the table is populated for prototyping, and again when it is in production.\nYou can collect statistics on the following.\nA unique index, which can be:\nPrimary or secondary\nSingle or multiple column\nPartitioned or nonpartitioned\nA nonunique index, which can be:\nPrimary or secondary\nSingle or multiple column\nPartitioned or nonpartitioned\nWith or without COMPRESS ﬁelds\nA non-indexed column or set of columns, which can be:\nPartitioned or nonpartitioned\nWith or without COMPRESS ﬁelds\nJoin index\nHash index\nNoPI table\nA temporary table\nIf you specify the TEMPORARY keyword but a materialized table does not exist, the system ﬁrst materializes an instance based on the column names and indexes you\nspecify. This means that after a true instance is created, you can update (re-collect) statistics on the columns by entering COLLECT STATISTICS and the TEMPORARY\nkeyword without having to specify the desired columns and index.\nIf you omit the TEMPORARY keyword but the table is a temporary table, statistics are collected for an empty base table rather than the materialized instance.\nSample (system-selected percentage) of the rows of a data table or index, to detect data skew and dynamically increase the sample size when found.\nThe system does not store both sampled and deﬁned statistics for the same index or column set. Once sampled statistics have been collected, implicit re-collection hits\nthe same columns and indexes, and operates in the same mode. To change this, specify any keywords or options and name the columns or indexes.\nSQL Fundamentals\nPage 87 of 99Related Information\nFor more information about:\nUsing the COLLECT STATISTICS statement, see Teradata Vantage™ - SQL Data Deﬁnition Language Syntax and Examples, B035-1144.\nCollecting statistics on a join index, collecting statistics on a hash index, or when to collect statistics on base table columns instead of hash index columns, see Teradata\nVantage™ - Database Design, B035-1094.\nDatabase administration and collecting statistics, see Teradata Vantage™ - Database Administration, B035-1093.\nNotation Conventions\nHow to Read Syntax\nThis document uses the following syntax conventions.\nSyntax Convention Meaning\nKEYWORDKeyword. Spell exactly as shown.\nMany environments are case-insensitive. Syntax shows keywords in uppercase unless\noperating system restrictions require them to be lowercase or mixed-case.\nvariableVariable. Replace with actual value.\nnumberString of one or more digits. Do not use commas in numbers with more than three digits.\nExample: 10045\n[ x ]x is optional.\n[ x | y ]You can specify x, y, or nothing.\n{ x | y }You must specify either x or y.\nx [...]You can repeat x, separating occurrences with spaces.\nExample: x x x\nSee note after table.\nx [,...]You can repeat x, separating occurrences with commas.\nExample: x, x, x\nSee note after table.\nx [delimiter ...]You can repeat x, separating occurrences with speciﬁed delimiter.\nExamples:\nIf delimiter is semicolon:\nx; x; x\nIf delimiter is {,|OR}, you can do either of the following:\nx, x, x\nx OR x OR x\nSee note after table.\n( [ x [ [,] y ] ] )Depends on context. See descriptions of syntax elements that use parentheses.\nYou can repeat only the immediately preceding item. For example, if the syntax is:\nKEYWORD x [...]\nYou can repeat x. Do not repeat KEYWORD.\nIf there is no white space between x and the delimiter, the repeatable item is x and the delimiter. For example, if the syntax is:\n[ x, [...] ] y\nYou can omit x: y\nSQL Fundamentals\nPage 88 of 99You can specify x once: x, y\nYou can repeat x and the delimiter: x, x, x, y\nCharacter Shorthand Notation Used in This Document\nThis document uses the Unicode naming convention for characters. For example, the lowercase character ‘a’ is more formally speciﬁed as either LATIN CAPITAL LETTER A or U+0041.\nThe U+xxxx notation refers to a particular code point in the Unicode standard, where xxxx stands for the hexadecimal representation of the 16-bit value deﬁned in the standard.\nThis document may use a symbol to represent a special character, or a particular class of characters, especially when discussing the following Japanese character encodings:\nKanjiEBCDIC\nKanjiEUC\nKanjiShift-JIS\nThese encodings are further deﬁned in International Character Set Support, B035-1125.\nCharacter Symbols\nThe following table deﬁnes the symbols and their associated character sets.\nSymbol Encoding Meaning\na-z\nA-Z\n0-9Any Any single byte Latin letter or digit.\na-z\nA-Z\n0-9Any Any fullwidth Latin letter or digit.\n< KanjiEBCDIC Shift Out [SO] (0x0E).\nIndicates transition from single to multibyte character in KanjiEBCDIC.\n> KanjiEBCDIC Shift In [SI] (0x0F).\nIndicates transition from multibyte to single byte KanjiEBCDIC.\nT Any Any multibyte character.\nThe encoding depends on the current character set.\nFor KanjiEUC, code set 3 characters are always preceded by ss3.\nI Any Any single byte Hankaku Katakana character.\nIn KanjiEUC, it must be preceded by ss2, forming an individual multibyte character.\nΔ Any Represents the graphic pad character.\nΔ Any Represents a single or multibyte pad character, depending on context.\nss 2 KanjiEUC Represents the EUC code set 2 introducer (0x8E).\nss 3 KanjiEUC Represents the EUC code set 3 introducer (0x8F).\nFor example, string \"TEST\", where each letter is intended to be a fullwidth character, is written as TEST. When encoding is important, hexadecimal representation is used.\nFor example, the following mixed single byte/multibyte character data in KanjiEBCDIC character set:\nLMN<TEST>QRS\nis represented as:\nD3 D4 D5 0E 42E3 42C5 42E2 42E3 0F D8 D9 E2\nPad Characters\nThe following table lists the pad characters for the character data types.\nServer Character Set Pad Character Name Pad Character V alue\nLATIN SPACE 0x20\nUNICODE SPACE U+0020\nGRAPHIC IDEOGRAPHIC SPACEU+3000\nKANJISJIS ASCII SPACE 0x20\nSQL Fundamentals\nPage 89 of 99Server Character Set Pad Character Name Pad Character V alue\nKANJI1 ASCII SPACE 0x20\nRestricted Words\nTeradata and ANSI/ISO SQL standards restrict the use of certain words as identiﬁers because those words may be incorrectly interpreted as SQL keywords. Restricted words should\nnot be used as database object names or as parameters in application programs that interface with the database. The categories of restricted words are:\nCategory Description\nReserved words These words are used as keywords by Teradata or ANSI/ISO SQL. They cannot be used as\nidentiﬁers to name database objects, such as databases, tables, columns, or stored\nprocedures. They also must not be used as macro or stored procedure parameters or local\nvariables, host variables, or correlation names. Reserved words can be reserved by\nTeradata, by the ANSI/ISO SQL standard, or by both.\nFuture reserved words These words are likely to be used as Teradata keywords in the future. Like reserved words,\nfuture reserved words cannot be used as identiﬁers.\nNonreserved words\nThese words may become keywords in the future.\nTeradata does not recommend using nonreserved words as identiﬁers if these words are\nreserved words in the ANSI standard. These words may become reserved words in the\nfuture.\nTeradata Parallel Transporter (Teradata PT) has a different set of restricted words.\nYou can use the SQLRestrictedWords view and SQLRestrictedWords_TBF function to see or query the restricted words in the current or previous database releases.\nRelated Information\nFor a list of Teradata PT restricted words, see Teradata® Parallel Transporter Reference, B035-2436.\nSQLRestrictedWords View\nThe SQLRestrictedWords view lists all restricted words for the current database release.\nSQLRestrictedWords is created in the SYSLIB database by the DIPDEM script, which is run automatically by the DIP utility when Analytics Database is installed.\nThe view contains these columns:\nColumn Name Description\nrestricted_word The restricted word.\ncategory These words are likely to be used as database keywords in the future. Like reserved words,\nfuture reserved words cannot be used as identiﬁers.\nANSI_restricted One of the following characters that represents the category of the Teradata-restricted\nword:\nR: a Teradata-reserved word\nF: a Teradata future reserved word\nN: a Teradata-nonreserved word\nUsage Notes\nQueries of SQLRestrictedWords are case-speciﬁc.\nExample: Getting the Restricted W ords for the Current Release\nThis query returns the restricted words for the current database release. Because it returns all columns from the view, it includes the Teradata and ANSI categories for the words.\nA portion of the output is shown as follows.\nSELECT * FROM SYSLIB.SQLRestrictedWords;\nResult:\nrestricted_word                 category  ANSI_restricted\n------------------------------  --------  ---------------\nABORT                           R         T\nABORTSESSION                    R         T\nABS                             R         R\nSQL Fundamentals\nPage 90 of 99ACCESS_LOCK                     R         T\n...                             ...       ...\nExample: Getting Nonreserved W ords that are ANSI Reserved W ords\nThe following query returns the nonreserved words for the current database release that are ANSI-reserved words. A portion of the output is shown as follows.\nThis result is useful because Teradata does not recommend using nonreserved words as identiﬁers if these words are reserved words in the ANSI standard. These words may become\nreserved words in the future.\nSELECT restricted_word\nFROM SYSLIB.SQLRestrictedWords\nWHERE category=’N’ AND ANSI_restricted=’R’;\nResult:\nrestricted_word\n------------------------------\nALLOCATE\nCALLED\nCONDITION\nGLOBAL\nMEMBER\n...\nRelated Information\nTo retrieve or query Teradata SQL restricted words from database releases other than the current release, see SQLRestrictedWords_TBF Function.\nSQLRestrictedWords_TBF Function\nSQLRestrictedWords_TBF is a table function that can be used to query the restricted words for the current or previous database releases. This is useful for migration or upgrade\nplanning, or in cases when you want to back down to the previous database release.\nFor each word, the table indicates the category of restriction (reserved, nonreserved, or future reserved), the database release when the word was introduced or dropped as a\nrestricted word, and whether the word is reserved, nonreserved, or neither in the current ANSI/ISO SQL standard.\nSQLRestrictedWords_TBF is created in the SYSLIB database by the DIPDEM script, which is run automatically by the DIP utility when Analytics Database is installed.\nANSI Compliance\nThis is a Teradata extension to the ANSI SQL:2011 standard.\nSyntax\n[ SYSLIB. ] SQLRestrictedWords_TBF()\nResult\nThe output table includes these columns:\nColumn Name Description\nrestricted_word The restricted word.\nrelease_introduced The release when the restricted word was introduced.\nThe release number is presented in this format:\nMM.mm\nwhere MM are 2 digits representing a major release number and mm are 2 digits\nrepresenting a minor release number. For example, '06.02', '12.00', or '13.10'.\nThe value of the release_introduced ﬁeld for restricted words that were introduced in\nTeradata Database 6.2 or in earlier releases is '06.02'.\nrelease_dropped The release when the restricted word was dropped. The release number is in the same\nformat as for the release_introduced column.\nA NULL value indicates that the restricted word has not been dropped, and is currently a\nrestricted word.\ncategory One of the following characters that represents the category of the Teradata-restricted\nword:\nR: a Teradata-reserved word\nSQL Fundamentals\nPage 91 of 99Column Name Description\nF: a Teradata future reserved word\nN: a Teradata-nonreserved word\nANSI_restricted One of the following characters that represents the ANSI category of the restricted word:\nR: an ANSI-eserved word\nN: an ANSI-nonreserved word\nT: a word that is neither reserved nor nonreserved by current ANSI standards, but\nthat is restricted by Teradata\nUsage Notes\nTable functions can be used only in the FROM clause of an SQL SELECT statement.\nQueries using the SQLRestrictedWords_TBF function are case-speciﬁc.\nExample: Getting the Restricted W ords for a Speciﬁc Release\nThe following query returns the restricted words for Teradata Database 12.0. Note that the query includes words that were restricted in all releases up to and including 12.0, and\nexcludes words that were dropped as restricted words prior to release 12.0. A portion of the output is included below. Note also that the reference to release 12.0 must be entered in\nthe query as ‘12.00’.\nSELECT * FROM TABLE (SYSLIB.SQLRestrictedWords_TBF()) AS t1\nWHERE release_introduced <= '12.00' \n  AND (release_dropped > '12.00' OR release_dropped IS NULL);\nResult:\nrestricted_word                release_introduced release_dropped category\n------------------------------ ------------------ --------------- --------\n...                            ...                ...             ... \nRESTRICTWORDS                  12.00              ?               N\nRETAIN                         06.02              ?               N\nREUSE                          06.02              ?               N\nRU                             06.02              ?               N\nSAMPLES                        06.02              ?               N\nSEARCHSPACE                    06.02              ?               N\nSECURITY                       06.02              ?               N\nSEED                           06.02              ?               N\nSELF                           06.02              ?               N\nSERIALIZABLE                   06.02              ?               N\nSHARE                          06.02              ?               N\nSOURCE                         06.02              ?               N\nSPECCHAR                       06.02              ?               N\nSPL                            06.02              ?               N\nSQLDATA                        12.00              13.10           N\n...                            ...                ...             ...\nEven though SQLDATA was dropped as a restricted word for release 13.10, it was a restricted word for release 12.0, so it is returned by the query.\nExample: Double Entries in the Restricted W ords T able\nIt is possible to have two rows returned for a speciﬁc restricted word if the word was dropped and later reintroduced, or if the restriction category was changed for the word. In this\nexample, the results show that NUMBER was changed from a nonreserved word to a reserved word in release 14.00.\nSELECT * FROM TABLE (SYSLIB.SQLRestrictedWords_TBF()) AS t1 \nWHERE restricted_word = 'NUMBER';\nResult:\nrestricted_word                release_introduced release_dropped category\n------------------------------ ------------------ --------------- --------\nNUMBER                         14.00              ?               R\nNUMBER                         13.00              14.00           N\nRelated Information\nFor more information on:\nTable functions, see the discussion of Table UDFs in Teradata Vantage™ - SQL External Routine Programming, B035-1147.\nThe use of table functions in queries, see the TABLE option of the FROM clause in Teradata Vantage™ - SQL Data Manipulation Language, B035-1146.\nSQL Fundamentals\nPage 92 of 99ANSI/ISO SQL Compliance\nTeradata conforms closely to the ANSI/ISO SQL standard while supporting extensions that enable users to take full advantage of the efﬁciency beneﬁts of parallelism. Teradata\ndevelops new features that conform to existing ANSI/ISO standards and handles differences between Teradata SQL and ANSI/ISO SQL as follows:\nWHEN ... THEN ...\nthe difference between the Teradata SQL dialect and the ANSI/ISO SQL standard for a\nlanguage feature is slightthe ANSI/ISO SQL is added to Teradata features as an option.\nthe difference between the Teradata SQL dialect and the ANSI/ISO SQL standard for a\nlanguage feature is signiﬁcantboth syntaxes are offered and the user has the choice of operating in either Teradata or\nANSI mode or of turning off SQL Flagger. The mode can be deﬁned:\nPersistently\nUse the SessionMode ﬁeld of the DBS Control Record to deﬁne session mode\ncharacteristics.\nFor a session\nUse the BTEQ .SET SESSION TRANSACTION command to control transaction\nsemantics.\nUse the BTEQ .SET SESSION SQLFLAG command to control use of the SQL\nFlagger.\nUse the SQL statement SET SESSION DATEFORM to control how data typed as\nDATE is handled.\nTerminology Differences Between ANSI/ISO SQL and Teradata SQL\nThe ANSI/ISO SQL standard and Teradata SQL occasionally use different terminology. The following table lists the more important variances.\nANSI/ISO SQL Teradata SQL\nBase table Table\nIn the ANSI/ISO SQL standard, the term table means:\nA base table\nA viewed table (view)\nA derived table\nBinding style Not deﬁned, but implicitly includes the following:\nInteractive SQL\nEmbedded SQL\nODBC\nCLIv2\nAuthorization ID User ID\nCatalog Dictionary\nCLI ODBC\nANSI CLI is not exactly equivalent to ODBC, but the ANSI standard is heavily based on the ODBC deﬁnition.\nDirect SQL Interactive SQL\nDomain Not deﬁned\nExternal routine function User-deﬁned function (UDF)\nModule Not deﬁned\nPersistent stored module Stored procedure\nSchema User\nDatabase\nSQL database Relational database\nViewed table View\nNot deﬁned Explicit transaction\nANSI transactions are always implicit, beginning with an executable SQL statement and ending with either a COMMIT or a\nROLLBACK statement.\nSQL Fundamentals\nPage 93 of 99ANSI/ISO SQL Teradata SQL\nNot deﬁned CLIv2\nTeradata CLIv2 is an implementation-deﬁned binding style.\nNot deﬁned Macro\nThe function of database macros is similar to that of ANSI persistent stored modules without having the loop and branch\ncapabilities stored modules offer.\nUsing the SQL Flagger\nTeradata provides the SQL Flagger to help users identify non-standard SQL. SQL Flagger always permits statements ﬂagged as non-entry-level or non-compliant ANSI/ISO SQL to\nexecute. Its task is to return a warning message to the requestor noting the noncompliance.\nFlagging is enabled by a client application before a session is logged on and generally is used only to assist in checking for ANSI compliance in code that must be portable across\nmultiple vendor environments.\nThe SQL Flagger is disabled by default. You can enable or disable it using any of the following procedures, depending on your application.\nFor this software … Use these commands or options … To turn the SQL  Flagger …\nBTEQ .[SET] SESSION SQLFLAG ENTRY to entry-level ANSI\n.[SET] SESSION SQLFLAG NONE off\nFor details on using BTEQ commands, see Basic Teradata® Query Reference, B035-2414.\nPreprocessor2 SQLFLAGGER(ENTRY) to entry-level ANSI\nSQLFLAGGER(NONE) off\nFor details on setting Preprocessor options, see Teradata® Preprocessor2 for Embedded SQL Programmer Guide, B035-2446.\nCLI set lang_conformance = ‘2’\nset lang_conformance to ‘2’to entry-level ANSI\nset lang_conformance = ‘N’ off\nFor details on setting the conformance ﬁeld, see Teradata® Call-Level Interface Version 2 Reference for Mainframe-Attached Systems,\nB035-2417 and Teradata® Call-Level Interface Version 2 Reference for Workstation-Attached Systems, B035-2418.\nSQL Statements and SQL Requests\nIn Teradata SQL, a statement, a syntactic concept, has three components:\nA statement keyword, such as SELECT, CREATE TABLE, or ALTER PROCEDURE\nZero or more expressions, functions, keywords, clauses, or phrases, such as WHERE, PRIMARY KEY, or UNIQUE PRIMARY INDEX\nA semicolon\nIn Teradata SQL, a request deﬁnes a unit of work that is transmitted from a client application to the database in a single message. A request is a semantic concept that has several\ncomponents, including:\nOne or more SQL statements\nRequest-level CLIv2 options parcel\nMetadata about the request data\nAll individual SQL statements, called single statement requests, are also individual SQL requests, but not all SQL requests are also SQL statements because one request can contain\nan unlimited number of SQL statements. This special case is called a multistatement request, and it is the primary superﬁcial characteristic that distinguishes a statement from a\nrequest in the Teradata world.\nRelated Information\nFor more information about multistatement requests, see Multistatement Requests.\nPerformance Considerations\nThis section provides suggestions for improving database query performance.\nUsing the 2PC Protocol\nTwo-Phase Commit (2PC) is an IMS and CICS protocol for committing update transactions processed by multiple systems that do not share the same locking and recovery mechanism.\nPerformance Impact\nSQL Fundamentals\nPage 94 of 99Consider the following disadvantages of using the 2PC protocol:\nPerformance may decrease because, at the point of synchronization, up to two additional messages are exchanged between the coordinator and participant, in addition to the\nnormal messages that update the database.\nIf your original SQL request took longer to complete than your other requests, the performance impact due to the 2PC overhead will be less noticeable.\nIf the database restarts, and a session using the 2PC protocol ends up in an IN-DOUBT state, the database holds data locks indeﬁnitely until you resolve the IN-DOUBT\nsession. During this time, other work could be blocked if it accesses the same data for which the database holds those locks.\nTo resolve this situation, perform the following :\n1. Use the COMMIT/ROLLBACK command to resolve manually the IN-DOUBT sessions.\n2. Use the RELEASE LOCKS command.\n3. Use the RESTART command to restart your system.\n2PC causes no system overhead when it is disabled.\nRelated Information\nFor more information on 2PC, see Teradata® Director Program Reference, B035-2416.\nSystem Validated Object Names\nThe system determines the validity of object names based on the rules presented in the topics beginning with Object Names and according to the settings of the related DBS Control\nﬁelds.\nThe entries in the following topics deﬁne whether various speciﬁcations in common SQL DDL statements are subject to object naming rules. The listed names follow the same object\nnaming rules when speciﬁed in DCL and DML statements.\nNames Subject to Object Naming Rules\nName Example Usage\nACCOUNT (acc)\nCREATE USER u1 as perm=10e6, password = pass1,\naccount=’acc2’, default journal table=u1jnl;\nMODIFY USER u1 AS ACCOUNT=’acc1’;\nBEGIN QUERY LOGGING WITH SQL ON ALL ACCOUNT =’acc1’;\nCREATE PROFILE prof5 AS PERM=10e5 PASSWORD=pass5 ACCOUNT= ‘acc3’;\nATTRIBUTE (att)\nCREATE TYPE udt2 AS (att1 INT, att2 DATE) NOT FINAL;\nALTER TYPE udt2 ADD ATTRIBUTE att3 FLOAT;\nAUTHORIZATION\n(auth)CREATE AUTHORIZATION db1.auth1 as INVOKER USER 'bdUsr' PASSWORD 'bdPsswd';\nCOLUMN (col)\nCREATE FUNCTION fn1 (NumRows INTEGER)\nRETURNS TABLE\n(col1  INTEGER,\n  col2  INTEGER)\nLANGUAGE C\nNO SQL\nPARAMETER STYLE SQL\nEXTERNAL NAME ‘SS!easy!easy.c’;\nCREATE TABLE t1 ( col1  int);\nCONSTRAINT (con)\nCREATE TABLE t2 (col1 int CONSTRAINT  con2   CHECK (i=1));\nCREATE TABLE t3 (col3 int CONSTRAINT  con3   REFERENCES t1(c1));\nALTER TABLE t1 ADD CONSTRAINT  con1  UNIQUE (c1);\nCONSTRAINT (con)\nCREATE CONSTRAINT  conA  data_type,[NULL|NOT NULL],\nVALUES value_name:integer_code ...\nSQL Fundamentals\nPage 95 of 99Name Example Usage\n(row level security\nconstraint)[, value_name:integer_code],\nInsert SYSLIB.insert_udf_name ,\nUpdate SYSLIB.update_udf_name ,\nDelete SYSLIB.delete_udf_name ,\nSelect SYSLIB.select_udf_name ;\nDATABASE (db)\nCREATE DATABASE  db1  as perm=10e6;\nFUNCTION (fn)\nCREATE FUNCTION  fn2  (integer, float)\nRETURNS FLOAT\nLANGUAGE C\nNO SQL\nEXTERNAL NAME   ‘sp:/Teradata/tdbs_udf/usr/second1.so’;\nRENAME FUNCTION fn1 AS  fn2;\nALTER SPECIFIC FUNCTION  fn1  EXECUTE PROTECTED;\nCREATE CAST (udt1 as udt2) WITH FUNCTION sysudtlib. fn1(udt1);\nCREATE ORDERING FOR udt1\nORDER FULL BY MAP WITH FUNCTION SYSUDTLIB. fn3;\nCREATE TRANSFORM FOR abov_strInt\nabov_strInt_IO (TO SQL WITH SPECIFIC FUNCTION SYSUDTLIB.abov_StrIntToSQL,\nFROM SQL WITH SPECIFIC FUNCTION SYSUDTLIB. fn4);\nGLOP Set (glop)\nCREATE GLOP SET db. glop1;\nINDEX (idx)\nCREATE INDEX idx2  (c1) on t1;\nCOLLECT STAT USING SAMPLE ON db1. idx1 \nCOLUMN col1 FROM db2.t1;\nCREATE TABLE t1 (col1 int, col2 int)\nINDEX  idx1  (c2);\nCOLLECT STAT INDEX  idx1  ON tab1;\nMACRO\nCREATE MACRO m1 AS (SELECT ‘abc’; );\nRENAME MACRO m1 to m2;\nMETHOD (mth)\nCREATE CONSTRUCTOR METHOD  mth1 \n(P1 INTEGER)\nFOR abov_strInt\nEXTERNAL NAME 'SS!udt_strintcons!udt_strintcons.c!F!strintcons';\nCREATE TYPE udt1 AS (P1 INTEGER)\nNOT FINAL CONSTRUCTOR METHOD  mth2  (P1 INTEGER)\nRETURNS udt1\nSELF AS RESULT\nLANGUAGE C\nDETERMINISTIC\nNO SQL;\nSQL Fundamentals\nPage 96 of 99Name Example Usage\nALTER SPECIFIC METHOD  mth2  FOR UDT_name EXECUTE NOT PROTECTED;\nPARAMETER (pm)\nCREATE MACRO m1 ( pm1  int) AS (SEL : pm1; );\nCREATE FUNCTION Find_Text ( pm1  VARCHAR (500), pm2  VARCHAR (500))\nRETURNS CHAR\nLANGUAGE C\nNO SQL\nPARAMETER STYLE TD_GENERAL\nEXTERNAL NAME\n'SS:pattern2:/home/i18n/ca3/v2r5/UDF/tests/SrcNI/pattern2/pattern2.c:SI:pattern2:/home/i18n/ca3/v2r5/UDF/tests/SrcNI/pa\nCREATE PROCEDURE xsp_cr003(IN  pm1  varchar(20), OUT  pm2  VARCHAR(20))\nLANGUAGE C\nPARAMETER STYLE SQL\nEXTERNAL NAME 'CS!xsp_cr003!xsp_cr003.c';\nCREATE TYPE abov_strlnt AS ( pm2  INTEGER)\nNOT FINAL CONSTRUCTOR METHOD  abov_strlnt   (P1 INTEGER)\nRETURNS  abov_strlnt  \nSELF AS RESULT\nLANGUAGE C\nDETERMINISTIC\nNO SQL;\nREPLACE PROCEDURE sp3(out  pm1  integer)\nBEGIN\n  DECLARE var1 INTEGER DEFAULT 10;\n  SET p1 = var1;\nEND;\nPASSWORD (pass)\nCREATE USER u1 as perm=10e6,\npassword =  pass1  , account=’acc1’;\nMODIFY USER u1 AS PASSWORD= pass1;\nPLAN_DIRECTIVE\n(pd)INSERT PLAN_DIRECTIVE  pd1  IN  pd1cat \n(sel   'pd_dip002_TestID1_1_pd',\n      t100k_b.i1, pd_db1.t100k_b.i2, pd_db1.t100k_b.i5\nfrom  t100k_b, pd_db1.t200_a\nwhere\n      t100k_b.i2 = pd_db1.t200_a.i2\n;\n)\nWITH '1: NESTED JOIN(DUPED( SCAN( PD_DB1.T100K_B)),'\n'INDEXED WITH NO ROWIDLIST( INDEX(I2) PD_DB1.T200_A))'\nCOMMENT 'PD for pd_dip002_TestID1_1_pd'\nPROCEDURE (sp)\nCREATE PROCEDURE  sp2  ( a varchar(20), OUT result1 VARCHAR(20))\nLANGUAGE C\nPARAMETER STYLE SQL\nEXTERNAL NAME 'CS!xsp_cr003!xsp_cr003.c';\nRENAME PROCEDURE  sp2  TO sp3;\nALTER PROCEDURE  sp3  LANGUAGE C COMPILE ONLY;\nPROFILE (prof)\nCREATE PROFILE prof5 AS\n   PERM=10e5\nSQL Fundamentals\nPage 97 of 99Name Example Usage\n   PASSWORD=pass5\n   ACCOUNT= ‘acc5’;\nPROXYUSER\n(pxyuser)\nPROXYROLE (pxyrole)SET QUERY_BAND='PROXYUSER=pxyuser1;\nPROXYROLE=pxyrole1;' FOR SESSION;\nBEGIN TRANSACTION;\nSET QUERY_BAND='PROXYROLE=pxyrole2;' FOR TRANSACTION;\nSELECT *\nFROM table1;\nThe reserved query band names PROXYUSER and PROXYROLE are not permitted in the proﬁle query band.\nQUERY (qry)\nWITH qry1(a) AS (SELECT col1 FROM tab1)\nSELECT a FROM qry1;\nROLE (role)\nCREATE ROLE role1;\nEXTERNAL ROLE\n(extrole)CREATE EXTERNAL ROLE extrole2;\nTABLE (t)\nCREATE TABLE  t1  (col1 int);\nCREATE ERROR TABLE  t2  FOR t1;\nRENAME TABLE t1 AS  t3;\nMODIFY USER u1 AS DEFAULT JOURNAL TABLE= t1;\nCREATE TRIGGER trig5 AFTER UPDATE ON t3\nREFERENCING OLD_TABLE AS t1 NEW_TABLE AS  t2 FOR EACH Statement\n  When (35 <= (sel X.price from t2 X INNER JOIN t1 Y ON\n                         X.pubyear= Y.pubyear))\n(\n  INSERT t4 SELECT t2.titles, t2.price FROM t2;\n);\nBEGIN LOADING db1.t3\nERRORFILES db1.t1, db1.t2\nINSERT INTO db1.t3.*;\nBEGIN DELETE MLOAD TABLES db1.t1\nWITH db1.t2\nERRORTABLES db1. lt3;\nBEGIN IMPORT MLOAD TABLES db1.t1\nWORKTABLES db1.t2\nERRORTABLES db1.t3 db1.t4 ;\nCREATE USER u1 as perm=10e6,\npassword = pass1, default journal table= t1;\nTRANSFORM_GROUP\n(tg)CREATE TRANSFORM FOR abov_strInt \ntg1  (TO SQL WITH SPECIFIC FUNCTION\nSYSUDTLIB.abov_StrIntToSQL,\nFROM SQL WITH SPECIFIC FUNCTION\nSYSUDTLIB.abov_StrIntFromSQL);\nSQL Fundamentals\nPage 98 of 99Name Example Usage\nDROP TRANSFORM tg2 FOR arsv_point_t ;\nTRIGGER (trig)\nCREATE TRIGGER trig1 AFTER  UPDATE OF (i)\nON t1 REFERENCING OLD AS OLDROW\nNEW AS NEWROW FOR EACH ROW\nWHEN (i > 10)\n(INSERT INTO t1log VALUES (OLDROW.i, NEWROW.i); );\nRENAME TRIGGER trig1 TO trig2 ;\nUDT (udt)\nCREATE TYPE  udt15  AS VARCHAR(15) FINAL;\nCREATE CAST ( udt1  as  udt2) WITH FUNCTION\nsysudtlib.abov_DistFloattoDistInt( udt1)\nCREATE ORDERING FOR  udt3 \nORDER FULL BY MAP WITH FUNCTION\nSYSUDTLIB.abov_StrIntOrdering;\nCREATE TRANSFORM FOR  udt3 \nabov_strInt_IO (TO SQL WITH SPECIFIC\nFUNCTION SYSUDTLIB.abov_StrIntToSQL,\nFROM SQL WITH SPECIFIC FUNCTION\nSYSUDTLIB.abov_StrIntFromSQL);\nDROP TRANSFORM transform_io FOR  udt4  ;\nUSER_NAME (u)\nCREATE USER  u1  as perm=10e6,\npassword =  pass1, account=’u1account’ ;\nUSING VARIABLE\nNAME (usingvar).import data file = c:\\temp\\fil1.data\nusing (usingvar1   int,  usingvar2   char(10))\ninsert db1.tab1(: usingvar1 , :usingvar2 );\nVIEW (v)\nCREATE VIEW v1 AS SELECT ‘abc’ col1 ;\nREPLACE VIEW v1 AS SELECT ‘abc’ col1 ;\nRENAME VIEW v1 AS v2 ;\nNames Not Subject to Object Naming Rules\nThe following items are not subject to object naming rules.\nName Example Usage\nCHECKPOINT (chkpt)\nCREATE USER u3 AS PERM=1E6,\nPASSWORD=u3, DEFAULT JOURNAL TABLE = jnl1;\nCHECKPOINT jnl1, NAME  chkpt1  ;\nCOLLATION SEQUENCE (COLL)\nSET SESSION COLLATION  ASCII\nJAR (jar)\nCALL SQLJ.INSTALL_JAR\n('CJ:tempJar_a.jar',' jar1',0);\nSQL Fundamentals\nPage 99 of 99Name Example Usage\nTRANSLATION (trans)\nTRANSLATE(c1 USING  trans1)\nAdditional Information\nTeradata Links\nLink Description\nhttps://docs.teradata.com/ Search Teradata Documentation, customize content to your needs, and download PDFs.\nCustomers: Log in to access Orange Books.\nhttps://support.teradata.comHelpful resources in one place:\nSupport requests\nAccount management and software downloads\nKnowledge base, community, and support policies\nProduct documentation\nLearning resources, including Teradata University\nhttps://www.teradata.com/University/Overview Teradata education network\nhttps://support.teradata.com/community Link to Teradata community",
  "metadata": {
    "document_id": "80982fdb-019d-4c66-b8f2-535d80264d7a",
    "collection_id": 5,
    "repository_type": "knowledge",
    "filename": "TeradataSQLFundamentals.pdf",
    "document_type": "pdf",
    "source": "upload",
    "title": "TeradataSQLFundamentals.pdf",
    "author": "",
    "tags": "",
    "category": "",
    "created_at": "2025-12-07T20:12:07.886549+00:00",
    "file_size": 2620977,
    "page_count": 0
  },
  "created_at": "2025-12-07T20:12:12.321761+00:00"
}